{"searchDocs":[{"title":"Clarity Challenge pre-announcement","type":0,"sectionRef":"#","url":"/blog/Clarity Challenge pre-announcement","content":"","keywords":"","version":null},{"title":"The Task‚Äã","type":1,"pageTitle":"Clarity Challenge pre-announcement","url":"/blog/Clarity Challenge pre-announcement#the-task","content":" You will be provided with simulated scenes, each including a target speaker and interfering noise. For each scene, there will be signals that simulate those captured by a behind-the-ear hearing aid with three channels at each ear and those captured at the eardrum without a hearing aid present. The target speech will be a short sentence and the interfering noise will be either speech or domestic appliance noise.  The task will be to deliver a hearing aid signal processing algorithm that can improve the intelligibility of the target speaker for a specified hearing-impaired listener. Initially, entries will be evaluated using an objective speech intelligibility measure we will provide. Subsequently, up to twenty of the most promising systems will be evaluated by a panel of listeners.  We will provide a baseline system so that teams can choose to focus on individual components or to develop their own complete pipelines.  ","version":null,"tagName":"h2"},{"title":"What will be provided‚Äã","type":1,"pageTitle":"Clarity Challenge pre-announcement","url":"/blog/Clarity Challenge pre-announcement#what-will-be-provided","content":" Evaluation of the best entries by a panel of hearing-impaired listeners.Speech + interferer scenes for training and evaluation.An entirely new database of 10,000 spoken sentencesListener characterisations including audiograms and speech-in-noise testing.Software including tools for generating training data, a baseline hearing aid algorithm, a baseline model of hearing impairment, and a binaural objective intelligibility measure.  ","version":null,"tagName":"h2"},{"title":"Important Dates‚Äã","type":1,"pageTitle":"Clarity Challenge pre-announcement","url":"/blog/Clarity Challenge pre-announcement#important-dates","content":" January 2021 ‚Äì Challenge launch and release of software and dataApril 2021 ‚Äì Evaluation data releasedMay 2021 ‚Äì Submission deadlineJune-August 2021 ‚Äì Listening test evaluation periodSeptember 2021 ‚Äì Results announced at a Clarity Challenge Workshop in conjunction with Interspeech 2021  Challenge and workshop participants will be invited to contribute to a journal Special Issue on the topic of Machine Learning for Hearing Aid Processing that will be announced next year.  ","version":null,"tagName":"h2"},{"title":"Further information‚Äã","type":1,"pageTitle":"Clarity Challenge pre-announcement","url":"/blog/Clarity Challenge pre-announcement#further-information","content":" If you are interested in participating and wish to receive further information, please sign up.  If you have questions, contact us directly at contact@claritychallenge.org  ","version":null,"tagName":"h2"},{"title":"Organisers‚Äã","type":1,"pageTitle":"Clarity Challenge pre-announcement","url":"/blog/Clarity Challenge pre-announcement#organisers","content":" Prof. Jon P. Barker, Department of Computer Science, University of SheffieldProf. Michael A. Akeroyd, Hearing Sciences, School of Medicine, University of NottinghamProf. Trevor J. Cox, Acoustics Research Centre, University of SalfordProf. John F. Culling, School of Psychology, Cardiff UniversityProf. Graham Naylor, Hearing Sciences, School of Medicine, University of NottinghamDr Simone Graetzer, Acoustics Research Centre, University of SalfordDr Rhoddy Viveros Mu√±oz, School of Psychology, Cardiff UniversityEszter Porter, Hearing Sciences, School of Medicine, University of Nottingham  Funded by the Engineering and Physical Sciences Research Council (EPSRC), UK.  Supported by RNID (formerly Action on Hearing Loss), Hearing Industry Research Consortium, Amazon TTS Research, Honda Research Institute Europe.  ","version":null,"tagName":"h2"},{"title":"Acknowledgement‚Äã","type":1,"pageTitle":"Clarity Challenge pre-announcement","url":"/blog/Clarity Challenge pre-announcement#acknowledgement","content":" The image copyright is owned by the University of Nottingham. ","version":null,"tagName":"h2"},{"title":"CEC1 submissions received","type":0,"sectionRef":"#","url":"/blog/CEC1 submissions received","content":"The CEC1 submission deadline has now passed. Thank you to all the teams who sent us signals. Please remember to submit your finalised system descriptions by June 22nd to the Clarity workshop following the instructions provided on the workshop website. We are currently busy evaluating the submissions using the MBSTOI metric. We will be contacting teams on the 22nd with details of how to prepare signals for the listening panel evaluation. If you have been working on the challenge but missed the submission deadline then please do get in contact. We will still be happy to receive your signals and system descriptions. Although late entries will not be eligible for the official challenge ranking, we will be happy to compute the eval set MBSTOI score for you and may even be able to arrange listening test evaluation through our panel. For any questions please contact us at claritychallengecontact@gmail.com or by posting to the Clarity challenge google group.","keywords":"","version":null},{"title":"Announcement of ICASSP 2023 Grand Challenge","type":0,"sectionRef":"#","url":"/blog/Announcement of ICASSP 2023 Grand Challenge","content":"We are pleased to announce that registration for the ICASSP 2023 Clarity Grand Challenge is now open. To register please complete the simple Google form found on the registration page. The remaining important dates for the challenge are as follows: 28th Nov 2022: Challenge launch: Release training/dev data; tools; baseline; rules &amp; documentation.2nd Feb 2023: Release of evaluation data.10th Feb 2023: Teams submit processed signals and technical reports.14th Feb 2023: Results released. Top 5 ranked teams invited to submit papers to ICASSP-202320th Feb 2023: Invited papers submitted to ICASSP-20234-9th June 2023: Overview paper and invited papers presented at dedicated ICASSP session The challenge training, dev data and initial tools are now fully from the Github repository. If you have any questions please do not hesitate to contact us at claritychallengecontact@gmail.com.","keywords":"","version":null},{"title":"CEC1 eval data released","type":0,"sectionRef":"#","url":"/blog/CEC1 eval data released","content":"The evaluation dataset is now available to download from the myairbridge download site. The evaluation data filename is clarity_CEC1_data.scenes_eval.v1_1.tgz. Full details of how to prepare your submission are now available on this site. Please read them carefully. Registration: Teams must register via the Google form on the How To Submit page of this site. (Please complete this even if you have already completed a pre-registration form). Only one person from each team should register. Only those who have registered will be eligible to proceed to the evaluation. Once you have registered you will receive a confirmation email, a team ID and a link to a Google Drive to which you can upload your signals. Submission deadline: The deadline for submission is the 15th June. The submission consists of two components: i) a technical document of up to 2 pages describing the system/model and any external data and pre-existing tools, software and models used. This should be prepared as a Clarity-2021 workshop abstract and submitted to the workshop. ii) the set of processed signals that we will evaluate using the MBSTOI metric. Details of how to name and package your signals for upload can be found on the How To Submit page. Listening Tests: Teams that do well in the MBSTOI evaluation will be notified on 22nd June and invited to submit further signals for the second stage Listening Test evaluation. For any questions please contact us at claritychallengecontact@gmail.com or by posting to the Clarity challenge google group.","keywords":"","version":null},{"title":"CEC2 registration open","type":0,"sectionRef":"#","url":"/blog/CEC2 registration open","content":"We are pleased to announce that registration for the 2nd Clarity Enhancement Challenge (CEC2) is now open. To register please complete the simple Google form found on the registration page. The remaining important dates for the challenge are as follows: 25th July 2022: Evaluation data released1st Sept 2022: 1st round submission deadline for evaluation by objective measure15th Sept 2022: 2nd round submission deadline for listening testsSept-Nov 2022: Listening test evaluation period.2nd Dec 2022: Results announced at a Clarity Challenge Workshop; prizes awarded. The challenge training, dev data and initial tools are now fully from the Github repository. If you have any questions please do not hesitate to contact us at claritychallengecontact@gmail.com.","keywords":"","version":null},{"title":"Announcing the 2nd Clarity Prediction Challenge (CPC2)","type":0,"sectionRef":"#","url":"/blog/Announcing CPC2","content":"","keywords":"","version":null},{"title":"Register now to take part‚Äã","type":1,"pageTitle":"Announcing the 2nd Clarity Prediction Challenge (CPC2)","url":"/blog/Announcing CPC2#register-now-to-take-part","content":" If you are interested in participating please register now via the online registration form.  ","version":null,"tagName":"h3"},{"title":"Important Dates‚Äã","type":1,"pageTitle":"Announcing the 2nd Clarity Prediction Challenge (CPC2)","url":"/blog/Announcing CPC2#important-dates","content":" March - Launch of challenge, release of training data + baseline system.1st July - Release of evaluation data and opening of submission window.31st July - Submission deadline.19th August - ISCA Clarity 2023 workshop @ Interspeech19th September - Deadline for submission of finalised Workshop papers  ","version":null,"tagName":"h3"},{"title":"What will be provided‚Äã","type":1,"pageTitle":"Announcing the 2nd Clarity Prediction Challenge (CPC2)","url":"/blog/Announcing CPC2#what-will-be-provided","content":" Audio produced by a variety of (simulated) hearing aids for speech-in-noise;The corresponding clean reference signals (the original speech);Characteristics of the listeners (pure tone audiograms, etc);The measured speech intelligibility scores from listening tests, where hearing-impaired listeners were asked to say what they heard after listening to the hearing aid processed signals.Software tools including a baseline system based on HASPI scores.  ","version":null,"tagName":"h3"},{"title":"For further information‚Äã","type":1,"pageTitle":"Announcing the 2nd Clarity Prediction Challenge (CPC2)","url":"/blog/Announcing CPC2#for-further-information","content":" To be kept up to date please join our Clarity Challenge Google group. If you have questions, please contact us directly using the contact details found here. ","version":null,"tagName":"h3"},{"title":"Baseline speech intelligibility model in round one","type":0,"sectionRef":"#","url":"/blog/baseline","content":"","keywords":"","version":null},{"title":"Some comments on signal alignment and level-insensitivity‚Äã","type":1,"pageTitle":"Baseline speech intelligibility model in round one","url":"/blog/baseline#some-comments-on-signal-alignment-and-level-insensitivity","content":" Our baseline binaural speech intelligibility measure in round one is the Modified Binaural Short-Time Objective Intelligibility measure, or MBSTOI. This short post outlines the importance of correcting for delays that your hearing aid processing algorithm introduces into the audio signals to allow MBSTOI to estimate the speech intelligibility accurately. It also discusses the importance of considering the audibility of signals before evaluation with MBSTOI.  ","version":null,"tagName":"h3"},{"title":"Evaluation‚Äã","type":1,"pageTitle":"Baseline speech intelligibility model in round one","url":"/blog/baseline#evaluation","content":" In stage one, entries will be ranked according to the average MBSTOI score across all samples in the evaluation test set. In the second stage, entries will be evaluated by the listening panel. There will be prizes for both stages. See this page for more information.    ","version":null,"tagName":"h2"},{"title":"Signal alignment in time and frequency‚Äã","type":1,"pageTitle":"Baseline speech intelligibility model in round one","url":"/blog/baseline#signal-alignment-in-time-and-frequency","content":" If the signal processed by the hearing aid introduces a significant delay, you should correct for this delay before submitting your entry. This is necessary because MBSTOI requires alignment of the clean speech ‚Äúreference‚Äù with the processed signal in time and frequency. This needs to be done for both ear signals.  MBSTOI downsamples signals to 10 kHz, uses a Discrete Fourier Transform to decompose the signal into one-third octave bands, and performs envelope extraction and short-time segmentation into 386 ms regions. Each region consists of 30 frames. These approaches are motivated by what is know about which frequencies and modulation frequencies are most important for intelligibility. For each frequency band and frame (over the region of which it is the last frame), an intermediate correlation coefficient is calculated between the clean reference and processed power envelopes for each ear. These are averaged to obtain the MBSTOI index. Thus is usually between 0 and 1, and rises monotonically with measured intelligibility scores, such that higher values indicate greater speech intelligibility. Alignment is therefore required at the level of the one-third octave bands and short-time regions.  Our baseline corrects for broadband delay per ear due to the hearing loss model. (The delay is measured by running a kronnecker delta function through the model for each ear.) However, the baseline software will not correct for delays created by your hearing aid processing.  Consequently, when submitting your hearing aid output signals, you are responsible for correcting for any delays introduced by your hearing aid. Note that this must be done blindly; the clean reference signals will not be supplied for the test/evaluation set.  ","version":null,"tagName":"h2"},{"title":"Level insensitivity‚Äã","type":1,"pageTitle":"Baseline speech intelligibility model in round one","url":"/blog/baseline#level-insensitivity","content":" MBSTOI is level-independent, i.e., MBSTOI is broadly insensitive to the level of the processed signal because it is calculated using a cross-correlation method. This could be a problem because sounds that are below the auditory thresholds of the hearing impaired listener may appear to MBSTOI to be highly intelligible.  To overcome this, the baseline experimental code mbstoi_beta, in conjunction with the baseline hearing loss model, can be used to approximate hearing-impaired auditory thresholds. Specifically, mbstoi_beta adds internal noise that can be used to approximate normal hearing auditory thresholds. This noise, in combination with the attenuation of signals by the hearing loss model to simulate raised auditory thresholds, makes MBSTOI level-sensitive.  The noise is created by filtering white noise using pure tone threshold filter coefficients with one-third octave weighting, approximating the shape of a typical auditory filter (from Moore 2012, based on Patterson‚Äôs method, 1976). This noise is added to the processed signal. Note, the standard MBSTOI in the equalisation-cancellation stage adds internal noise to parameters, but this is an independent process.  ","version":null,"tagName":"h2"},{"title":"MBSTOI‚Äã","type":1,"pageTitle":"Baseline speech intelligibility model in round one","url":"/blog/baseline#mbstoi","content":" The method was developed by Asger Heidemann Andersen, Jan Mark de Haan, Zheng-Hua Tan and Jesper Jensen (Andersen et al., 2018). It builds on the Short-Time Objective Intelligibility (STOI) metric created by Cees H. Taal, Richard C. Hendriks, Richard Heusdens, and Jesper Jensen (Taal et al., 2011). MBSTOI includes a better ear stage and an equalisation-cancellation stage. For simplicity, the latter stage is not discussed here; see Andersen et al. (2018) for details.  ","version":null,"tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Baseline speech intelligibility model in round one","url":"/blog/baseline#references","content":" Andersen, A. H., de Haan, J. M., Tan, Z. H., &amp; Jensen, J. (2018). Refinement and validation of the binaural short time objective intelligibility measure for spatially diverse conditions. Speech Communication, 102, 1-13.Moore, B. C. (2012). An introduction to the psychology of hearing. Brill.Patterson, R. D. (1976). Auditory filter shapes derived with noise stimuli. The Journal of the Acoustical Society of America, 59(3), 640-654.Taal, C. H., Hendriks, R. C., Heusdens, R., &amp; Jensen, J. (2011). An algorithm for intelligibility prediction of time‚Äìfrequency weighted noisy speech. IEEE Transactions on Audio, Speech, and Language Processing, 19(7), 2125-2136. ","version":null,"tagName":"h2"},{"title":"CPC1 results and prizes","type":0,"sectionRef":"#","url":"/blog/CPC1 results and prizes","content":"The 1st Clarity Prediction Challenge is now complete. Thank you to all who took part! The full results can be found on the Clarity-2022 workshop website where you will also find links to system papers and the overview presentation. Many of the systems have led to successful Interspeech 2022 papers and will be contributing to the Interspeech 2022 special session on Speech Intelligibility Prediction for Hearing-Impaired Listeners. We hope to see many of you in Korea! In the meantime, please be sure to check out the onging 2nd Clarity Enhancement Challenge. The deadline for submitting enhanced signals is 1st September 2022, so there is still time to participate. To register a team please use the form here.","keywords":"","version":null},{"title":"CPC2 eval data released","type":0,"sectionRef":"#","url":"/blog/CPC2 eval data released","content":"The CPC2 evaluation data has now been released. The data is available for download as a single 478 MB file, clarity_CPC2_data.test.v1_0.tgz. The evaluation data should be untarred into the same root as the training data. Further details can be found on the challenge website. The data consists of the hearing aid algorithm output signals, clean reference signals, listener audiograms, and head rotation information. Listener responses are not provided for the evaluation data but will be made available after the submission window has closed. For details on how to prepare your submission please see the instructions on the website. If you have any questions please feel free to post them on this forum. The submission window will close on the 31st of July. Good luck!","keywords":"","version":null},{"title":"Hearing loss simulation","type":0,"sectionRef":"#","url":"/blog/Hearing loss simulation","content":"","keywords":"","version":null},{"title":"Audio examples of hearing loss‚Äã","type":1,"pageTitle":"Hearing loss simulation","url":"/blog/Hearing loss simulation#audio-examples-of-hearing-loss","content":" Here are two samples of speech in noise processed through the simulator. In each audio example there are three versions of the same sentence:  Unimpaired hearingMild hearing impairmentModerate to severe hearing impairment  0 dB signal to noise ratio  Your browser does not support the audio element.  And here is an example where the noise is louder:  Your browser does not support the audio element.  Noisier: -10dB signal to noise ratio  ","version":null,"tagName":"h2"},{"title":"Acknowledgements‚Äã","type":1,"pageTitle":"Hearing loss simulation","url":"/blog/Hearing loss simulation#acknowledgements","content":" The hearing loss model we‚Äôre using was generously supplied by Michael Stone at the University of Manchester as MATLAB code and translated by us into Python. The original code was written by members of the Auditory Perception Group at the University of Cambridge, ca. 1991-2013, including Michael Stone, Brian Moore, Brian Glasberg and Thomas Baer. Information about the model can be found primarily in Nejime and Moore (1997), but also in Nejime and Moore (1998), Baer and Moore (1993 and 1994), and Moore and Glasberg (1993).  The original speech recordings come from the ARU corpus, University of Liverpool (Hopkins et al. 2019). This corpus is freely available at the link in the reference below.  ","version":null,"tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Hearing loss simulation","url":"/blog/Hearing loss simulation#references","content":" Baer, T., &amp; Moore, B. C. (1993). Effects of spectral smearing on the intelligibility of sentences in noise. The Journal of the Acoustical Society of America, 94(3), 1229-1241.Baer, T., &amp; Moore, B. C. (1994). Effects of spectral smearing on the intelligibility of sentences in the presence of interfering speech. The Journal of the Acoustical Society of America, 95(4), 2277-2280.Hopkins, C., Graetzer, S., &amp; Seiffert, G. (2019). ARU adult British English speaker corpus of IEEE sentences (ARU speech corpus) version 1.0 [data collection]. Acoustics Research Unit, School of Architecture, University of Liverpool, United Kingdom. DOI: 10.17638/datacat.liverpool.ac.uk/681. Retrieved from http://datacat.liverpool.ac.uk/681/.Moore, B. C., &amp; Glasberg, B. R. (1993). Simulation of the effects of loudness recruitment and threshold elevation on the intelligibility of speech in quiet and in a background of speech. The Journal of the Acoustical Society of America, 94(4), 2050-2062.Moore, B. C., Glasberg, B. R., &amp; Vickers, D. A. (1996). Factors influencing loudness perception in people with cochlear hearing loss. B. Kollmeier, World Scientific, Singapore, 7-18.Nejime, Y., &amp; Moore, B. C. (1997). Simulation of the effect of threshold elevation and loudness recruitment combined with reduced frequency selectivity on the intelligibility of speech in noise. The Journal of the Acoustical Society of America, 102(1), 603-615.Nejime, Y., &amp; Moore, B. C. (1998). Evaluation of the effect of speech-rate slowing on speech intelligibility in noise using a simulation of cochlear hearing loss. The Journal of the Acoustical Society of America, 103(1), 572-576. ","version":null,"tagName":"h2"},{"title":"Clarity-2023 Workshop @ Interspeech, Dublin","type":0,"sectionRef":"#","url":"/blog/Clarity-2023 Workshop @ Interspeech, Dublin","content":"We are pleased to announce the 4th ISCA Clarity Workshop on Machine Learning Challenges for Hearing Aids (Clarity-2023). The event will be a one-day workshop held as an ISCA satellite event to Interspeech 2023 in Dublin, Ireland. For registration and programme details please visit the workshop website https://claritychallenge.github.io/clarity2023-workshop/ IMPORTANT DATES 2nd June 2023 - Workshop Submission Deadline (Regular Papers)31st July 2023 - Workshop Submission Deadline (Clarity Challenge Papers)5th August 2023 - Registration closes19th August - Workshop / Clarity Challenge results announced About One of the biggest challenges for hearing-impaired listeners is understanding speech in the presence of background noise. Everyday social noise levels can have a devastating impact on speech intelligibility. The inability to communicate effectively can lead to social withdrawal and isolation. Disabling hearing impairment affects 360 million people worldwide, with that number increasing because of the ageing population. Unfortunately, current hearing aid technology is often ineffective in noisy situations. Although amplification can restore audibility, it does not compensate fully for the effects of hearing loss. The Clarity workshops are designed to stimulate a two-way conversation between the speech research community and hearing aid developers. Hearing aid developers, who are not typically represented at Interspeech, will have an opportunity to present the challenges of their industry to the speech community; the speech community will be able to present and discuss potentially transformative approaches to speech in noise processing in the presence of hearing researchers and industry experts. Topics Any work related to the challenges of hearing aid signal processing will be considered relevant topics include, Binaural technology for speech enhancement and source separationMulti-microphone processing technologyReal-time approaches to speech enhancementStatistical model-driven approaches to hearing aid processingAudio quality &amp; intelligibility assessment hearing aid and cochlear implant usersEfficient and effective integration of psychoacoustic testing in machine learningMachine learning for diverse target listenersMachine learning models of hearing impairment The 2nd Clarity Prediction Challenge The Clarity-2023 will also host the 2nd Clarity Prediction Challenge, that is addressing the problem of developing new intrusive and non-intrusive approaches to hearing-aid speech intelligibility prediction. The Challenge will be launching on 1st March, is you may be interested in participating please sign up to our Google group for further announcements. Keynote Talks Prof Fei Chen, SUSTech, China,Prof DeLiang Wang, Ohio State University, US Organisers Michael Akeroyd, University of NottinghamJon Barker, University of SheffieldTrevor Cox, University of SalfordFei Chen, Southern University of Science and Technology, ChinaJohn Culling, University of CardiffSimone Graetzer, University of SalfordAndrew Hines, University College Dublin For further information To be kept up to date please join our Clarity Challenge Google group. If you have questions, please contact us directly using the contact details found here. Funded by the Engineering and Physical Sciences Research Council (EPSRC), UK Supported by RNID (formerly Action on Hearing Loss), Hearing Industry Research Consortium, Amazon TTS Research","keywords":"","version":null},{"title":"Launch of CEC2","type":0,"sectionRef":"#","url":"/blog/launch of CEC2","content":"We are pleased to announce the launch of the 2nd Clarity Enhancement Challenge (CEC2). The website has been fully updated to provide you with all the information you will need to participate in the challenge. The schedule for the challenge is as follows: 13th April 2022: Release of training and development data; initial tools.30th April 2022: Release of full toolset and baseline system.1st May 2022: Registration for challenge entrants opens.25th July 2022: Evaluation data released1st Sept 2022: 1st round submission deadline for evaluation by objective measure15th Sept 2022: 2nd round submission deadline for listening testsSept-Nov 2022: Listening test evaluation period.2nd Dec 2022: Results announced at a Clarity Challenge Workshop; prizes awarded. The challenge training, dev data and initial tools will be available from 13th April. In the meantime, please visit the CEC2 Intro page to learn more about the task. If you have any questions please do not hesitate to contact us at claritychallengecontact@gmail.com.","keywords":"","version":null},{"title":"ICASSP 2023 evaluation data released","type":0,"sectionRef":"#","url":"/blog/ICASSP 2023 evaluation data released","content":"We are pleased to announce that the evaluation dataset for the ICASSP Clarity Challenge is now available for download. https://www.myairbridge.com/en/#!/folder/EkthOZZeBW33aaDBWSDadTgpOkbgaFxO For instructions on preparing your submission please visit: https://claritychallenge.org/docs/icassp2023/taking_part/icassp2023_submission If you have not yet registered it is not too late to do so. Please use the form at the link below and we will then send you a Team ID and a personalised upload link for your submission. https://claritychallenge.org/docs/icassp2023/taking_part/icassp2023_registration Note, we have extended the deadline for submission until Friday 10th February so that teams have a full week to process the signals. The remaining schedule is as follows, 2nd Feb 2023: Release of evaluation data.10th Feb 2023: Teams submit processed signals and technical reports.14th Feb 2023: Results released. Top 5 ranked teams invited to submit papers to ICASSP-202320th Feb 2023: Invited papers submitted to ICASSP-20234-9th June 2023: Overview paper and invited papers presented at dedicated ICASSP session","keywords":"","version":null},{"title":"Release of CEC2 baseline","type":0,"sectionRef":"#","url":"/blog/release of CEC2 baseline","content":"We are pleased to announce the release of the 2nd Clarity Enhancement Challenge (CEC2) baseline system code. The baseline code has been released in the latest commit to the Clarity GitHub repository. The baseline system perform NAL-R amplification according to the audiogram of the target listener, followed by a simple gain control and output of the signals to 16-bit stereo wav format. The system has been kept deliberately simple with no microphone array processing or attempt at noise cancellation. HASPI scores for the dev set have been measured. The scores are as follows. System\tHASPIUnprocessed\t0.1615 NAL-R baseline\t0.2493 See here for further details. If you have any problems using the baseline code please do not hesitate to contact us at claritychallengecontact@gmail.com, or post questions on the Google group.","keywords":"","version":null},{"title":"Live events in January","type":0,"sectionRef":"#","url":"/blog/Jan-2-live-events","content":"","keywords":"","version":null},{"title":"Webinar - Challenge Overview‚Äã","type":1,"pageTitle":"Live events in January","url":"/blog/Jan-2-live-events#webinar---challenge-overview","content":" ","version":null,"tagName":"h2"},{"title":"Friday 14th January‚Äã","type":1,"pageTitle":"Live events in January","url":"/blog/Jan-2-live-events#friday-14th-january","content":" 9:00 GMT | 17:00 CST (GMT+8)  ","version":null,"tagName":"h3"},{"title":"Click here to join the webinar‚Äã","type":1,"pageTitle":"Live events in January","url":"/blog/Jan-2-live-events#click-here-to-join-the-webinar","content":" An introduction to the aims of the challenge and some background to the problem of speech intelligibility prediction for hearing aids:  Welcome, introduction to Clarity.Speech intelligibility models: Overview and why are they needed.Hearing impairment speech intelligibility prediction.The prediction challenge - details and how you can sign up to participate.Audience questions / discussion.  The presentations will be recorded and made available online shortly after the event. The Q&amp;A discussion will not be recorded.  You are welcome to join slightly later if you are only interested in joining for the Q&amp;A section (presentations should finish around 9:40 GMT).  ","version":null,"tagName":"h3"},{"title":"Live Q&A session‚Äã","type":1,"pageTitle":"Live events in January","url":"/blog/Jan-2-live-events#live-qa-session","content":" ","version":null,"tagName":"h2"},{"title":"Monday 17th January‚Äã","type":1,"pageTitle":"Live events in January","url":"/blog/Jan-2-live-events#monday-17th-january","content":" 17:00 GMT | 12:00 EST (GMT-5) | 9:00 PST (GMT-8)  ","version":null,"tagName":"h3"},{"title":"Click here to join the Q&A‚Äã","type":1,"pageTitle":"Live events in January","url":"/blog/Jan-2-live-events#click-here-to-join-the-qa","content":" A chance to ask the team questions about the Clarity Prediction Challenge - for anyone that could not attend the webinar on Friday 14th due to time zone differences.  Please note there will be no presentations in this session. The talks from Friday‚Äôs webinar will be uploaded to the Clarity project YouTube channel later in the day so you are invited to watch those before joining this live Q&amp;A. ","version":null,"tagName":"h3"},{"title":"Launch of CEC3","type":0,"sectionRef":"#","url":"/blog/launch of CEC3","content":"We are pleased to announce the launch of the 3rd Clarity Enhancement Challenge (CEC3). The challenge follows on from the success of the 2nd Clarity Enhancement Challenge (CEC2) and is about improving the performance of hearing aids for speech-in-noise. The challenge extends CEC2 is three separate directions which have been presented as three different tasks. Task 1: Real ambisonic room impulse responses (üî• LIVE üî•)Task 2: Real hearing aid signals (üî• LIVE üî•)Task 3: Real dynamic backgrounds (launching 1st May) Participants are welcome to submit to one or more tasks. We are particularly interested in systems that handle all three cases with little or no redesign/retraining. The website has been fully updated to provide you with all the information you will need to participate. The necessary data and software are available for download. The schedule for the challenge is as follows: 2nd April 2024: Launch of Task 1 and Task 2 with training and development data; initial tools.1st May 2024: Launch of Task 3.25th July 2024: Evaluation data released2nd Sept 2024: 1st round submission for evaluation by objective measure15th Sept 2024: 2nd round submission deadline for listening tests (Task 2 and 3)Sept-Nov 2024: Listening test evaluation period.Dec 2024: Results announced at a Clarity Challenge Workshop (Details TBD); prizes awarded. If you have any questions please do not hesitate to contact us at claritychallengecontact@gmail.com. If you wish to be kept informed, please sign up to our Google group. If you are considering participating, please complete the registration form on the registration page. Registration is free and carries no obligation to participate, but will help us to keep you informed of any changes to the challenge.","keywords":"","version":null},{"title":"Latency, computation time and real-time operation","type":0,"sectionRef":"#","url":"/blog/Latency, computation time and real-time operation","content":"","keywords":"","version":null},{"title":"The 1st Clarity Enhancement Challenge‚Äã","type":1,"pageTitle":"Latency, computation time and real-time operation","url":"/blog/Latency, computation time and real-time operation#the-1st-clarity-enhancement-challenge","content":" For a hearing aid to work well for users, the processing needs to be quick. The output of the hearing aid should be produced with a delay of less than about 10 ms. Many audio processing techniques are non-causal, i.e., the output of the system depends on samples from the future. Such processing is useless for hearing aids and therefore our rules include a restriction on the use of future samples.  The rules state the following:  Systems must be causal; the output at time t must not use any information from input samples more than 5 ms into the future (i.e., no information from input samples &gt;t+5ms).There is no limit on computational cost.    Mathematically this is:  yn=f(xm , xm+1 ... xn+N-1 , xn+N , L )  where yn is the output from your hearing aid for sample nnn.xxx is the audio input signal from a hearing aid microphone.N=0.005fsN = 0.005 fsN=0.005fs where fsfsfs is the sampling frequency.mmm is a sample number where m‚â§nm \\le nm‚â§n.LLL is the listener characteristics.f()f()f() is the hearing aid function. There is no limitation on how long this takes to compute.You can use multiple microphones; only a single input signal xxx is shown here just for simplicity.  Here it is illustrated as a diagram.    Figure. Example of how the limit of 5 ms is applied to a hearing aid input and output signal. We have a chosen a limit of 5 ms because in a real hearing aid there will be other sources of delay (e.g., analogue-to-digital, digital-to-analogue conversion).  ","version":null,"tagName":"h2"},{"title":"Why is there no limitation of how long f() takes to compute?‚Äã","type":1,"pageTitle":"Latency, computation time and real-time operation","url":"/blog/Latency, computation time and real-time operation#why-is-there-no-limitation-of-how-long-f-takes-to-compute","content":" We‚Äôre trying to foster new approaches to hearing aid processing and decided that at this stage we will drive more innovation if we don‚Äôt restrict computation time for round one. Such restrictions will be considered in future rounds.  ","version":null,"tagName":"h2"},{"title":"Why haven‚Äôt you talked about latency?‚Äã","type":1,"pageTitle":"Latency, computation time and real-time operation","url":"/blog/Latency, computation time and real-time operation#why-havent-you-talked-about-latency","content":" In discussions, it is apparent that this term is used in different ways by different people, so to avoid confusion we‚Äôre not using it!  ","version":null,"tagName":"h2"},{"title":"Do algorithms have to be real-time?‚Äã","type":1,"pageTitle":"Latency, computation time and real-time operation","url":"/blog/Latency, computation time and real-time operation#do-algorithms-have-to-be-real-time","content":" The above limitations mean that the algorithms could in theory be made real-time if a powerful enough computer was available, but your entry can take as long as it needs to process the signals. ","version":null,"tagName":"h2"},{"title":"One approach to our enhancement challenge","type":0,"sectionRef":"#","url":"/blog/One approach to our enhancement challenge","content":"","keywords":"","version":null},{"title":"References‚Äã","type":1,"pageTitle":"One approach to our enhancement challenge","url":"/blog/One approach to our enhancement challenge#references","content":" [1] Andersen, A.H., Haan, J.M.D., Tan, Z.H. and Jensen, J., 2015. A binaural short time objective intelligibility measure for noisy and enhanced speech. In the Sixteenth Annual Conference of the International Speech Communication Association.[2] Li, H., Fu, S.W., Tsao, Y. and Yamagishi, J., 2020. iMetricGAN: Intelligibility Enhancement for Speech-in-Noise using Generative Adversarial Network-based Metric Learning. arXiv preprint arXiv:2004.00932.[3] Gillhofer, M., Ramsauer, H., Brandstetter, J., Sch√§fl, B. and Hochreiter, S., 2019. A GAN based solver of black-box inverse problems. Proceedings of the NeurIPS 2019 Workshop.[4] Kawanaka, M., Koizumi, Y., Miyazaki, R. and Yatabe, K., 2020, May. Stable training of DNN for speech enhancement based on perceptually-motivated black-box cost function. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 7524-7528). IEEE. ","version":null,"tagName":"h2"},{"title":"Sounds for round one","type":0,"sectionRef":"#","url":"/blog/Sounds","content":"","keywords":"","version":null},{"title":"Everyday background noises that interfere with understanding of speech‚Äã","type":1,"pageTitle":"Sounds for round one","url":"/blog/Sounds#everyday-background-noises-that-interfere-with-understanding-of-speech","content":" A long and varied list of sounds cause problems. These lists are in no particular order.  Living room or spaceClocks tickingCrisp packets rustlingTaps runningKettles boilingDishwasherMicrowaveWashing machineTV, music, radioPhone ringing (or receiving texts ‚Äì unknown beeps/tones)Newspapers rustlingAir-conditioning and oven extractor fansVacuum cleanerDoorbell ringingDog barkingRain on window  ","version":null,"tagName":"h2"},{"title":"Family and friends‚Äã","type":1,"pageTitle":"Sounds for round one","url":"/blog/Sounds#family-and-friends","content":" Cutlery/crockery banging/clangingDoors opening/closing (to rooms and cupboards)MusicPeople walking around the roomChildren playing with toysLaughingPeople talking from another roomSpeakers from a different conversation in close proximity (i.e. beside you) when you are trying to converseTraffic outsideChewing/chompingSteam pipes/ coffee machinesChairs being moved  ","version":null,"tagName":"h2"},{"title":"Outside‚Äã","type":1,"pageTitle":"Sounds for round one","url":"/blog/Sounds#outside","content":" Church bellsMarket noiseFootsteps on different types of ground, i.e. heels on hard floors but also wellingtons in mudClothes rustling (such as waterproof coats or hat on hearing aid)Wind (even with HA on ‚Äòwind setting‚Äô)Pigeons/birdsSirensTraffic noise (especially at junctions)MusicLaughterPhones ringingTillsChildren playing outside or running around (in shops, on the street and at parks)Beeping signal at crossingsGarden centres ‚Äì high glass ceilings, open plan, trolleysRoad/ tyre and traffic noise when in a car or on the busAlso mentioned how people you speak to in the car may be in front or behind youTrains and the tubeAeroplanes and airports (suitcases rolling)Tannoys  ","version":null,"tagName":"h2"},{"title":"Characteristics of processed speech to consider‚Äã","type":1,"pageTitle":"Sounds for round one","url":"/blog/Sounds#characteristics-of-processed-speech-to-consider","content":" Clarity (clearness) or qualityRhythm of speech‚ÄòInflection‚Äô (intonation)Similarity to original speakerAgreed that in situations where the voice would not be processed clearly, i.e. outside with many noise sources, not sounding like the original - speaker is fine.  ","version":null,"tagName":"h2"},{"title":"Other comments‚Äã","type":1,"pageTitle":"Sounds for round one","url":"/blog/Sounds#other-comments","content":" Speed of speech; it was suggested that we have sentences read at different speeds as faster talkers are often harder to understand.Stated that emphasis on key words is useful for following conversation; perhaps key words in the sentence when marked should be given higher value.Lots of comments on room acoustics, i.e., ceiling heights, furnishings, floorings, windows etc., which has a big impact on how difficult it is to have a conversation with background noise.Different accents of talkers can make conversation more difficult; including speakers with different accents in the background.We‚Äôre now working out what sounds to use. But are there other sounds we should consider?  ","version":null,"tagName":"h2"},{"title":"Credits‚Äã","type":1,"pageTitle":"Sounds for round one","url":"/blog/Sounds#credits","content":" Thank you to the patient and public involvement representatives who participated.Clarity Organiser: Eszter Porter .Facilitators: Adele Horobin, Erin Dawe-Lane.This discussion group was supported by the National Institute for Health Research Nottingham Biomedical Research Centre. ","version":null,"tagName":"h2"},{"title":"Welcome","type":0,"sectionRef":"#","url":"/blog/welcome","content":"Welcome to the new Clarity blog. We will be using this blog to post regular updates about our Challenges and Workshop, as well as posts discussing the tools and techniques that we are using in our baseline systems.","keywords":"","version":null},{"title":"Introduction Webinar - Recording Available","type":0,"sectionRef":"#","url":"/blog/webinar-1-link","content":"The Clarity team recently hosted a webinar to introduce the Prediction Challenge. The recording is now available to view online: The slides are available to download: 1 Welcome and Overview 2 Speech Intelligibility Models 3 Hearing Impariment and SI Prediction 4 Clarity Prediction Challenge Details Note that we did not record the Q&amp;A session at the end, but if you have questions about taking part in the challenge you can contact us at claritychallengecontact@gmail.com","keywords":"","version":null},{"title":"The speech-in-noise problem part two","type":0,"sectionRef":"#","url":"/blog/The speech-in-noise problem part two","content":"","keywords":"","version":null},{"title":"Machine learning‚Äã","type":1,"pageTitle":"The speech-in-noise problem part two","url":"/blog/The speech-in-noise problem part two#machine-learning","content":" In recent years, there has been increasing interest in what machine learning methods can do for hearing aids. Machine learning is a branch of artificial intelligence where computers learn directly from example data. One machine learning method is the neural network. This is an algorithm formed from layers of simple computational units connected to each other in a way that is inspired by connections between neurons in the brain. Deep (3+ layer) neural networks are able to learn complex, non-linear mapping functions, which makes them ideal candidates for noise reduction tasks.  We anticipate that machine learning can help tackle the challenge of speech in noise for hearing aids, providing a tailored solution for each individual and listening situation. For example, one thing machine learning could do is to sense the acoustic environment the listener is in, and choose the most suitable processing settings.    Image via www.vpnsrus.com  In recent years, a machine learning approach for noise reduction has become popular. Neural networks are used to estimate time-frequency masks (a set of gains for each time-frequency unit that, when multiplied by the signal, produce less noisy speech; see, e.g., Zhao et al., 2018).  Machine learning systems for noise reduction are trained on artificially mixed speech and noise. Some operate on a single channel, i.e., using spectral cues, and some work with multiple channels using spatial cues. We expect that future hearing aids built on machine learning will perform best if they combine the left and right microphones to work binaurally.  Most of these noise reduction systems have been designed and evaluated in an off-line mode where they process pre-recorded signals. This isn‚Äôt much use for hearing aids that need to work in real-time with low latency (i.e., short delays). One challenge for hearing aids is to redesign off-line approaches to work quickly enough without too much loss of performance.  The potential for machine learning to produce better approaches to hearing aid processing is what motivated the Clarity Project. If you‚Äôre interested in hearing more as the challenges develop, please sign up.  ","version":null,"tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"The speech-in-noise problem part two","url":"/blog/The speech-in-noise problem part two#references","content":" Brons, I., Houben, R., and Dreschler, W. A. (2014). Effects of noise reduction on speech intelligibility, perceived listening effort, and personal preference in hearing-impaired listeners. Trends in hearing, 18, 1-10.Van den Bogaert, T., Doclo, S., Wouters, J., and Moonen, M. (2009). Speech enhancement with multichannel Wiener filter techniques in multimicrophone binaural hearing aids. The Journal of the Acoustical Society of America, 125(1), 360-371.Zhao, Y., Wang, D., Johnson, E. M., and Healy, E. W. (2018). A deep learning based segregation algorithm to increase speech intelligibility for hearing-impaired listeners in reverberant-noisy conditions. The Journal of the Acoustical Society of America, 144(3), 1627-1637.  ","version":null,"tagName":"h2"},{"title":"Credits‚Äã","type":1,"pageTitle":"The speech-in-noise problem part two","url":"/blog/The speech-in-noise problem part two#credits","content":" Photograph of hearing aid wearer, copyright University of Nottingham.  Image of brain with overlaid circuity made available by www.vpnsrus.com. ","version":null,"tagName":"h2"},{"title":"The speech-in-noise problem","type":0,"sectionRef":"#","url":"/blog/The speech-in-noise problem","content":"","keywords":"","version":null},{"title":"References‚Äã","type":1,"pageTitle":"The speech-in-noise problem","url":"/blog/The speech-in-noise problem#references","content":" Akeroyd, M. A. (2008). Are individual differences in speech reception related to individual differences in cognitive ability? A survey of twenty experimental studies with normal and hearing-impaired adults. International Journal of Audiology, 47(sup2), S53-S71.Cherry, E. C. (1953). Some experiments on the recognition of speech, with one and with two ears. The Journal of the Acoustical Society of America, 25(5), 975-979.Heinrich, A., Henshaw, H., and Ferguson, M. A. (2015). The relationship of speech intelligibility with hearing sensitivity, cognition, and perceived hearing difficulties varies for different speech perception tests. Frontiers in Psychology, 6, 782.Vestergaard Knudsen, L., √ñberg, M., Nielsen, C., Naylor, G., and Kramer, S. E. (2010). Factors influencing help seeking, hearing aid uptake, hearing aid use and satisfaction with hearing aids: A review of the literature. Trends in Amplification, 14(3), 127-154.Kochkin, S. (2000). MarkeTrak V: ‚ÄúWhy my hearing aids are in the drawer‚Äù The consumers‚Äô perspective. The Hearing Journal, 53(2), 34-36.  ","version":null,"tagName":"h2"},{"title":"Credits‚Äã","type":1,"pageTitle":"The speech-in-noise problem","url":"/blog/The speech-in-noise problem#credits","content":" Photo of Cocktail party by Ross CC BY-NC-SA 2.0Ronan, N., &amp; Barrett, G. (2014). A 68 year old woman with deteriorating hearing. BMJ, 348, g2984. https://www.bmj.com/content/348/bmj.g2984 ","version":null,"tagName":"h2"},{"title":"Welcome to CPC1","type":0,"sectionRef":"#","url":"/blog/welcome to CPC1","content":"Welcome to the new Clarity CPC1 site for the first prediction challenge launching in autumn 2021. Feel free to look around. At the moment we're still doing listening tests and preparing the data, so the download links don't work. If anything is unclear or you've got questions, please contact us through the Google group.","keywords":"","version":null},{"title":"Why use machine learning challenges for hearing aids?","type":0,"sectionRef":"#","url":"/blog/Why use machine learning challenges for hearing aids","content":"","keywords":"","version":null},{"title":"Components of a challenge‚Äã","type":1,"pageTitle":"Why use machine learning challenges for hearing aids?","url":"/blog/Why use machine learning challenges for hearing aids#components-of-a-challenge","content":" There needs to be a common task based on a target application scenario to allow communities to gain from benchmarking and collaboration. Clarity project‚Äôs first enhancement challenge will be about hearing speech from a single talker in a typical living room, where there is one source of noise and a little reverberation.  We‚Äôre currently working on developing simulation tools to allow us to generate our living room data. The room acoustic will be simulated using RAVEN and the Hearing Device Head-related Transfer Functions will come from Denk‚Äôs work. We‚Äôre working on getting better, more ecologically valid speech than is often used in speech intelligibility work.    Entrants are then given training data and development (dev) test data along with a baseline system that represents the current state-of-the-art. You can find a post and video on the current thinking on the baseline here. We‚Äôre still working on the rules stipulating what is and what is not allowed (for example, will entrants be allowed to use data from outside the challenge).  Clarity‚Äôs first enhancement challenge is focussed on maximising the speech intelligibility (SI) score. We will evaluate this first through a prediciton model that is based on a hearing loss simulation and an objective metric for speech intellibility. Simulation has been hugely important for generating training data in the CHIME challenges and so we intend to use that approach in Clarity. But results from simulated test sets cannot be trusted and hence a second evaluation will come through perceptual tests on hearing impaired subjects. However, one of our current problems is that we can‚Äôt bring listeners into our labs because of COVID-19.  We‚Äôll actually be running two challenges in roughly parallel, because we‚Äôre also going to task the community to improve our prediction model for speech intelligibility.  We‚Äôre running a series of challenges over five years. What other scenarios should we consider? What speech? What noise? What environment? Please comment below.  ","version":null,"tagName":"h2"},{"title":"Acknowledgements‚Äã","type":1,"pageTitle":"Why use machine learning challenges for hearing aids?","url":"/blog/Why use machine learning challenges for hearing aids#acknowledgements","content":" Much of this text is based on Jon Barker‚Äôs 2020 SPIN keynote ","version":null,"tagName":"h2"},{"title":"The baseline","type":0,"sectionRef":"#","url":"/blog/The baseline","content":"An overview of the current state of the baseline we‚Äôre developing for the machine learning challenges We‚Äôre currently developing the baseline processing that challenge entrants will need. This takes a random listener and a random audio sample of speech in noise (SPIN) and passes that through a simulated hearing aid (the Enhancement Model). This improves the speech in noise. We then have an algorithm (the Prediction Model) to estimate the Speech Intelligibility that the listener would perceive (SI score). This score can then be used to drive machine learning to improve the hearing aid. A talk through the baseline model we‚Äôre developing. The first machine learning challenge is to improve the enhancement model, in other words, to produce a better processing algorithm for the hearing aid. The second challenge is to improve the prediction model using perceptual data we‚Äôll provide.","keywords":"","version":null},{"title":"The 1st Clarity Enhancement Challenge","type":0,"sectionRef":"#","url":"/docs/cec1/cec1_intro","content":"The 1st Clarity Enhancement Challenge warning The 1st Clarity Enhancement Challenge has now finished. For the details of the systems that were submitted and to see the table of results, please visit the Clarity-2021 Workshsop website. For details of information on forthcoming challenge see here. Dates - key challenge dates. Scenario - a description of the listening scenario and how it has been simulated. Baseline System - a description of the baseline hearing aid model. CEC1 Data - the data that can be used to train and evaluate your system during development. CEC1 Software - the software tools that we are providing to help you build and evaluate a challenge entry. Challenge Rules - the rules to which all challenge entries must adhere. Listening Tests - information about the listening tests which will be used to evaluate the best systems. Submission - information about how to prepare your submission. Prizes - information about our prizes. Download - where to go to download the software and challenge data.","keywords":"","version":"Next"},{"title":"Important Dates","type":0,"sectionRef":"#","url":"/docs/cec1/cec1_dates","content":"Important Dates We are operating a two-stage submission process with the following key dates. 1st June 2021: Evaluation data release for MBSTOI evaluation for all entrants. 11th June 2021: Registration deadline. 15th June 2021: All entrants submit (i) audio for MBSTOI evaluation and (ii) a draft of their technical report (details below). 22nd June 2021: Deadline by which all entrants must submit two page technical reports to Clarity-2021 workshop (details below). 22nd June 2021: Entrants informed which systems are going forward to the listening test evaluation stage. Evaluation data for listening tests released to those entrants. 29th June 2021: Entrants submit audio for listening tests.","keywords":"","version":"Next"},{"title":"Download","type":0,"sectionRef":"#","url":"/docs/cec1/cec1_download","content":"","keywords":"","version":"Next"},{"title":"Software‚Äã","type":1,"pageTitle":"Download","url":"/docs/cec1/cec1_download#software","content":" All the necessary software tools are available as a single package, pyClarity available on GitHub.  ","version":"Next","tagName":"h2"},{"title":"Data‚Äã","type":1,"pageTitle":"Download","url":"/docs/cec1/cec1_download#data","content":" To download data, please visit here.  The data is split into two packages:  clarity_CEC1_data.train.tgz [192 GB],clarity_CEC1_data.dev_eval_metadata.tgz [163 GB].  Please also download and unpack clarity_CEC1_data.anechoic.v1_3.tgz [11.4 GB], which contains the correct version of anechoic signals for reference, and use them to replace the incorrected anechoic signals within the clarity_CEC1_data.train.tgz and clarity_CEC1_data.dev_eval_metadata.tgz packages.  Unpack packages under the same root directory using  tar -xvzf &lt;PACKAGE_NAME&gt;   Training data is stored in clarity_CEC1_data.train.tgz with the following structure,  clarity_data | ‚îî‚îÄ‚îÄ‚îÄtrain ‚îî‚îÄ‚îÄ‚îÄinterferers | | nosie 3.9G | | speech 4.5G | ‚îî‚îÄ‚îÄ‚îÄrooms | | ac 48M | | brir 46G | | rpf 379M | ‚îî‚îÄ‚îÄ‚îÄscenes 166G | ‚îî‚îÄ‚îÄ‚îÄtargets 2.8G   The dev and eval data is stored in clarity_CEC1_data.dev_eval_metadata.tgz contains  clarity_data | ‚îî‚îÄ‚îÄ‚îÄdev | ‚îî‚îÄ‚îÄ‚îÄinterferers | | | nosie 587M | | | speech 1.4G | | | ‚îî‚îÄ‚îÄ‚îÄrooms | | | ac 20M | | | brir 20G | | | rpf 158M | | | ‚îî‚îÄ‚îÄ‚îÄscenes 72G | | | ‚îî‚îÄ‚îÄ‚îÄtargets 1.3G | ‚îî‚îÄ‚îÄ‚îÄeval | | | nosie 675M | | | speech 1.3G | | | ‚îî‚îÄ‚îÄ‚îÄrooms | | | ac 12M | | | brir 12G | | | rpf 95M | | | ‚îî‚îÄ‚îÄ‚îÄscenes 58G | | | ‚îî‚îÄ‚îÄ‚îÄtargets 749M | ‚îî‚îÄ‚îÄ‚îÄeval2/scenes 21G # eval signal processed by baseline system   For further details and code for running the baseline, see the baseline recipe in the pyClarity repository. ","version":"Next","tagName":"h3"},{"title":"Results","type":0,"sectionRef":"#","url":"/docs/cec1/cec1_results","content":"","keywords":"","version":"Next"},{"title":"Prizes‚Äã","type":1,"pageTitle":"Results","url":"/docs/cec1/cec1_results#prizes","content":" The Hearing Industry Research Consortium prizes were awarded as follows:  The prizes for the MBSTOI evaluation were awarded to the Brno University of Technology team (first place), the Music Tribe team (second place) and theELO-SPHERES Consortium (third place). The University of Sheffield team was not eligible to win prizes due to connections with the organisers.  The overall prizes for the listening test evaluation were awarded to the University of Oldenburg team (first place) and the ELO-SPHERES Consortium (second place). In the noise interferer category, the University of Oldenburg and Googlears teams were the best performers, while in the speech interferer category, the University of Oldenburg team and ELO-SPHERES Consortium were the best performers. For a discussion of the differences in performance for the noise and speech interferers, see the [YouTube video].  The Amazon student prizes were awarded to Katerina Zmolikova from the Brno University of Technology team (first place), Tomas Gajecki from the Medical University Hannover team (second place), and Xi Chen from Shenzhen University, who was an intern at Tencent while working on this challenge (third place).  Congratulations to all our prize the winners! ","version":"Next","tagName":"h2"},{"title":"Listening Tests","type":0,"sectionRef":"#","url":"/docs/cec1/taking_part/cec1_listening_tests","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"Listening Tests","url":"/docs/cec1/taking_part/cec1_listening_tests#overview","content":" Our panel members will be provided with a tablet preloaded with our Listen@Home software and headphones to complete the listening experiment.  They will listen to an entrant‚Äôs sentence, respond verbally with what they think was said by the target talker, and then move on to the next sentence. Their response will be recorded by the tablet microphone(s) and then processed using automatic speech recognition. Intelligibility will be evaluated as the number of words identified correctly in the sentence.  Our plan is that each listener will evaluate 1,200 sentences, which is about 4 hours of listening, and that every listener will evaluate sentences from every entrant. We will use a combinatorial design to equate this as far as possible.  Should a listener drop out from the panel, we will endeavour to replace them with someone with a similar hearing loss, but should that prove impractical we will reduce the size of the panel, and inform entrants which listener has withdrawn.  ","version":"Next","tagName":"h2"},{"title":"Listen@Home hardware‚Äã","type":1,"pageTitle":"Listening Tests","url":"/docs/cec1/taking_part/cec1_listening_tests#listenhome-hardware","content":" We will be using a Lenovo 10e Chromebook running Android 81.0 and Sennheiser PC-8 headsets to play the sounds to our participants. We will allow participants to set the volume so that the sounds are not so loud to be uncomfortable. Without loudness-recruitment measures for our listeners, we cannot be sure just what loudnesses every participant will hear, so we need to allow them to make the choice here.  We have measurements on the output capability of a system in the laboratory:  A 1 kHz pure tone set to be the most powerful it can be (i.e., an amplitude range of +/-1 = RMS amplitude of 0.707, and the volume controls at 100%) gave 99 dB(A) SPL on the PC-8 headphones.An ICRA speech-shaped noise [1], unmodulated in time, and scaled to an RMS of 0.3, gave 90 dB(A) at the same volume level. With this RMS, the noise had 0.1% of its samples clipped at +/- 1.  It is important to note that there is a convention for the prediction model that a +/-1 square wave has RMS = 0 dB FS and corresponds to 120 dB, while for listening tests, 0 dB FS corresponds to approximately 100 dB, given the above capabilities of the reproduction equipment.  For the listening tests, we will require the signals to be provided as 16-bit WAV files with a 32 kHz sampling rate (see this page). We will play the signals as is using a HTML/PHP audio player coded on a webpage. The responsibility for the final signal level is therefore yours. It‚Äôs worth bearing in mind that should your signals overall seem too loud to be comfortable to a participant, they may well turn down the volume themselves.    ","version":"Next","tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Listening Tests","url":"/docs/cec1/taking_part/cec1_listening_tests#references","content":" [1] ICRA standard noises, https://icra-audiology.org/Repository/icra-noise. We used track #1. ","version":"Next","tagName":"h2"},{"title":"Baseline System","type":0,"sectionRef":"#","url":"/docs/cec1/software/cec1_baseline","content":"Baseline System Figure 1 shows a simplified schematic of the baseline system. For simplicity, not all signal paths are shown. A scene generator (blue box) creates the speech in noise (SPIN) that the hearing aid model then enhances (yellow box). This enhancement is individualised for each listener, hence there is also a system to select a random listener (white ellipse) with a particular set of pure tone air-conduction thresholds or audiograms. The speech in noise that has been improved by the hearing aid is then passed to the prediction stage (orange box). This includes: (i) a simulation of hearing loss and (ii) a binaural model of intelligibility that estimates the speech intelligibility. Figure 1 Simplified overview of the baseline. As stated in the rules of the first Enhancement Challenge, you are free to choose which parts of the baseline are useful to your approach, and reconfigure the system as you feel fit. More details of the different parts of the baseline appear on the software page, see, Scene GeneratorHearing aid modelHearing loss modelSpeech intelligibility model Download baseline software and data.","keywords":"","version":"Next"},{"title":"CEC2 Download","type":0,"sectionRef":"#","url":"/docs/cec2/cec2_download","content":"","keywords":"","version":"Next"},{"title":"Software‚Äã","type":1,"pageTitle":"CEC2 Download","url":"/docs/cec2/cec2_download#software","content":" All the necessary software tools are available as a single GitHub repository.  We recommend installing the software first and then following the instructions in the repository's README for downloading and unpacking the data.  ","version":"Next","tagName":"h3"},{"title":"Data‚Äã","type":1,"pageTitle":"CEC2 Download","url":"/docs/cec2/cec2_download#data","content":" The data is available for download here.  On the download site you will see three data packages are available,  clarity_CEC2_core.v1_1.tgz [28 GB] - metadata and dev setclarity_CEC2_train.v1_1.tgz [69 GB] - scenes for training systemsclarity_CEC2_hoairs.v1_0.tgz [144 GB] - impulse responses for generating extended training data  All participants will require the core data package. Participants using machine learning approaches will additionally require the train data package. Participants wishing to extend the training set by using our provided scene rendering tools will also require the high order ambisonic impulse responses (i.e., the hoairs package).  To unpack the data we recommend you follow the instructions in the Clarity Challenge GitHub repository.  danger If you previously downloaded v1_0 of the core and train data, please replace your data with v1_1: an error was found in the head rotations for the initial data release. ","version":"Next","tagName":"h3"},{"title":"Prizes","type":0,"sectionRef":"#","url":"/docs/cec1/taking_part/cec1_prizes","content":"","keywords":"","version":"Next"},{"title":"The Team Prize‚Äã","type":1,"pageTitle":"Prizes","url":"/docs/cec1/taking_part/cec1_prizes#the-team-prize","content":" Team prizes have been made available by the generosity of the Hearing Industry Research ConsortiumThere will be separate MBSTOI and listening test prizes for the top systems.   MBSTOI prize 1st Place $1000 2nd Place $500 3rd Place $250 Listening Test prize 1st Place $1000 2nd Place $500 3rd Place $250  ","version":"Next","tagName":"h2"},{"title":"The Amazon Student Prize‚Äã","type":1,"pageTitle":"Prizes","url":"/docs/cec1/taking_part/cec1_prizes#the-amazon-student-prize","content":" Student prizes have been made available by the generosity of Amazon TTS Research   Amazon prize for top student contribution 1st Place $1000 2nd Place $500 3rd Place $250    The award will be judged by a panel formed from members of the Clarity-2021 workshop scientific committee.There will be a lightweight nomination process. Details to be announced.  info Anonymous entries and those with direct links to the Clarity project team are ineligible for cash prizes, sorry. ","version":"Next","tagName":"h2"},{"title":"Core Software","type":0,"sectionRef":"#","url":"/docs/cec1/software/cec1_software","content":"","keywords":"","version":"Next"},{"title":"A. Scene generator‚Äã","type":1,"pageTitle":"Core Software","url":"/docs/cec1/software/cec1_software#a-scene-generator","content":" Fully open-source python code for generating hearing aid inputs for each scene  Inputs: target and interferer signals, BRIRs, RAVEN project (rpf) files, scene description JSON filesOutputs: Mixed target+interferer signals for each hearing aid channel, direct path (simulating a measurement close to the eardrum). Reverberated pre-mixed signals can also be optionally generated.  ","version":"Next","tagName":"h2"},{"title":"B. Baseline hearing aid processor‚Äã","type":1,"pageTitle":"Core Software","url":"/docs/cec1/software/cec1_software#b-baseline-hearing-aid-processor","content":" The baseline hearing aid processor is based on openMHA. The python code configures openMHA with a Camfit compressive fitting for a specific listener‚Äôs audiogram. This includes a python implementation of the Camfit compressive prescription and python code for driving openMHA.  This configuration of openMHA includes multiband dynamic compression and non-adaptive differential processing. The intention was to produce a basic hearing aid without various aspects of signal processing that are common in high-end hearing aids, but tend to be implemented in proprietary forms so cannot be replicated exactly.  The main inputs and outputs for the processor are as follows:  Inputs: Mixed scene signals for each hearing aid channel, a listener ID drawn from scene-listener pairs identified in ‚Äòscenes_listeners.json‚Äô and an entry in the listener metadata json file ‚Äòlisteners.json‚Äô for that IDOutputs: The stereo hearing aid output signal, &lt;scene&gt;_&lt;listener&gt;_HA-output.wav  ","version":"Next","tagName":"h2"},{"title":"C. Hearing Loss model‚Äã","type":1,"pageTitle":"Core Software","url":"/docs/cec1/software/cec1_software#c-hearing-loss-model","content":" Open-source python implementation of the Cambridge Auditory Group Moore/Stone/Baer/Glasberg hearing loss model.  Inputs: A stereo wav audio signal, e.g., the output of the baseline hearing aid processor, and a set of audiograms (both L and R ears).Outputs: The signal after simulating the hearing loss as specified by the set of audiograms (stereo wav file), &lt;scene&gt;_&lt;listener&gt;_HL-output.wav  ","version":"Next","tagName":"h2"},{"title":"D. Speech Intelligibility model‚Äã","type":1,"pageTitle":"Core Software","url":"/docs/cec1/software/cec1_software#d-speech-intelligibility-model","content":" Python implementation of a binaural intelligibility model, Modified Binaural Short-Time Objective Intelligibility (MBSTOI). This is an experimental baseline tool that will be used in the stage 1 evaluation of entrants (see Rules). Note that MBSTOI requires signal time-alignment (and alignment within one-third octave bands).  Inputs: HL-model output signals, audiogram, reference target signal (i.e., the premixed target signal convolved with the BRIR with the reflections ‚Äúturned off‚Äù, specified as ‚Äòtarget_anechoic‚Äô), (scene metadata)Outputs: predicted intelligibility score ","version":"Next","tagName":"h2"},{"title":"Submission","type":0,"sectionRef":"#","url":"/docs/cec1/taking_part/cec1_submission","content":"","keywords":"","version":"Next"},{"title":"Registration‚Äã","type":1,"pageTitle":"Submission","url":"/docs/cec1/taking_part/cec1_submission#registration","content":" Teams are required to register using the form below. Please submit one form per team, i.e., providing a single contact email address. Once you have registered, you will receive an email confirmation with a team ID and an individualised link to a Google Drive for submitting materials.  Loading‚Ä¶  info It is important that all teams who are intending to submit an entry complete the registration form no later than 11th June.  ","version":"Next","tagName":"h2"},{"title":"What evaluation data is provided?‚Äã","type":1,"pageTitle":"Submission","url":"/docs/cec1/taking_part/cec1_submission#what-evaluation-data-is-provided","content":" The evaluation data consists of 1500 scenes. For each scene you are provided with the signals received at each of the three microphones on the left and right hearing aid device. You will also be provided with JSON formatted metadata consisting of  the audiograms of a set of listeners anda mapping of which listeners will listen to which scenes.  For the MBSTOI evaluation, there will be one listener per scene and the scene-listener mapping will be the same for all teams. For the listening test evaluation, there will be five listeners per scene and each team will have a separate scene-listener mapping. The file formats will be the same as used for the development data; for details see the CEC1 Data page.  ","version":"Next","tagName":"h2"},{"title":"What audio do I need to submit?‚Äã","type":1,"pageTitle":"Submission","url":"/docs/cec1/taking_part/cec1_submission#what-audio-do-i-need-to-submit","content":" You must submit the audio signals produced at the output of your simulated hearing aid for the evaluation datasets. You will be asked to provide two sets of signals: the first for the MBSTOI evaluation (due 15th June) and the second for the listening tests (due 29th June).  MBSTOI evaluation. Signals should be submitted in floating point WAV format with a 44.1 kHz sampling rate. For levels, we will follow the convention in the baseline hearing aid (at the output) and hearing loss models. That is, a +/-1 square wave has RMS = 0 dB FS and corresponds to 120 dB. Listening tests. Signals should be submitted as 16-bit WAV files with a 32 kHz sampling rate (due to hardware limitations). You should ensure that any samples that are greater than +1 or less than -1 have been hard-clipped at +/-1 before submission. Here, 0 dB FS corresponds to approximately 100 dB, given the capabilities of the reproduction equipment. These signals will be played as is to the listener panel.  We also encourage you to submit your simulated hearing aid code.  See the page on listening tests for more information about the levels that can be reproduced by the listening test equipment. When playing signals to listeners we will then play them as is. The responsibility for the final signal level is therefore yours. It‚Äôs worth bearing in mind that should your signals overall seem too loud to be comfortable to a participant, they may well turn down the volume themselves.  ","version":"Next","tagName":"h2"},{"title":"Naming and packaging signals‚Äã","type":1,"pageTitle":"Submission","url":"/docs/cec1/taking_part/cec1_submission#naming-and-packaging-signals","content":" Your processed signals should be named using the conventions used by the baseline system, i.e., &lt;Scene ID&gt;_&lt;Listener ID&gt;_HA-output.wav and explained on the CEC1 data page.  These should be placed in a directory whose name is the unique team ID that you will be sent, e.g., E001 and then packaged using zip or tar or any standard packaging tool.  The resulting file should be about 2 GB for the first round.  ","version":"Next","tagName":"h2"},{"title":"Technical report‚Äã","type":1,"pageTitle":"Submission","url":"/docs/cec1/taking_part/cec1_submission#technical-report","content":" The two page technical report must be submitted as a paper to the Clarity-2021 Workshop. Deadline 22nd June. An author kit and submission instructions are available at the workshop website.  A draft of the report needs to be uploaded to the Google Drive along with your MBSTOI signals by 15th June. The draft needs to be sufficiently complete for us to judge whether your system is compliant with the challenge rules.  Your report should include an abstract and introduction and sections on experimental setup/methodology including system information and model/network architecture, evaluation/results, discussion, conclusion and references. Please provide an estimation of the computational resources needed. You must describe any external data and pre-existing tools, software and models used.  Your report should cite the following document, which provides an overview of the challenge and the baseline system:  S. Graetzer, J. Barker, T. J. Cox, M. Akeroyd, J. F. Culling, G. Naylor, E. Porter, and R. Viveros Mu√±oz, ‚ÄúClarity-2021 challenges: Machine learning challenges for advancing hearing aid processing,‚Äù in Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2021, Brno, Czech Republic, 2021.  The document can be accessed here.  ","version":"Next","tagName":"h2"},{"title":"How will intellectual property be handled?‚Äã","type":1,"pageTitle":"Submission","url":"/docs/cec1/taking_part/cec1_submission#how-will-intellectual-property-be-handled","content":" See here under Intellectual Property.  ","version":"Next","tagName":"h2"},{"title":"Where do I submit the signals?‚Äã","type":1,"pageTitle":"Submission","url":"/docs/cec1/taking_part/cec1_submission#where-do-i-submit-the-signals","content":" When you have registered you will receive a link to a Google Drive to which you will be able to securely upload your signals. You will be able to use the same link to upload materials for both the 1st submission, and the 2nd submission if you are selected for the 2nd round. We also encourage you to submit your simulated hearing aid code via this link.  Materials uploaded will be visible to the Clarity Team but not to other entrants.  warning Note, in order to use the Google Drive you will need to have a Google account. If you anticipate problems using Google then please make arrangements to send us the materials by other means, e.g., via a service such as WeTransfer or similar. ","version":"Next","tagName":"h2"},{"title":"Rules","type":0,"sectionRef":"#","url":"/docs/cec1/taking_part/cec1_rules","content":"","keywords":"","version":"Next"},{"title":"Teams‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cec1/taking_part/cec1_rules#teams","content":" Teams must have pre-registered and nominated a contact person.Teams can be from one or more institutions.The organisers may enter the challenge themselves but will not be eligible to win the cash prizes.  ","version":"Next","tagName":"h2"},{"title":"Transparency‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cec1/taking_part/cec1_rules#transparency","content":" Teams must provide a technical document of up to 2 pages describing the system/model and any external data and pre-existing tools, software and models used.We will publish all technical documents (anonymous or otherwise).Teams are encouraged ‚Äì but not required ‚Äì to provide us with access to the system/model and to make their code open source.Anonymous entries are allowed but will not be eligible for cash prizes.All teams will be referred to using anonymous codenames in rank ordering.  ","version":"Next","tagName":"h2"},{"title":"Intellectual property‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cec1/taking_part/cec1_rules#intellectual-property","content":" The following terms apply to participation in this machine learning challenge (‚ÄúChallenge‚Äù). Entrants may create original solutions, prototypes, datasets, scripts, or other content, materials, discoveries or inventions (a ‚ÄúSubmission‚Äù). The Challenge is organised by the Challenge Organiser.  Entrants retain ownership of all intellectual and industrial property rights (including moral rights) in and to Submissions.  As a condition of submission, Entrant grants the Challenge Organiser, its subsidiaries, agents and partner companies, a perpetual, irrevocable, worldwide, royalty-free, and non-exclusive license to use, reproduce, adapt, modify, publish, distribute, publicly perform, create a derivative work from, and publicly display the Submission.  Entrants provide Submissions on an ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE.  ","version":"Next","tagName":"h2"},{"title":"What information can I use?‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cec1/taking_part/cec1_rules#what-information-can-i-use","content":" ","version":"Next","tagName":"h2"},{"title":"Training and development‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cec1/taking_part/cec1_rules#training-and-development","content":" There is no limit on the amount of training data that can be generated using our tools. Teams can also use their own data for training or expand the training data through simple automated modifications. However, teams that do this must make a second submission using only the official audio files and signal generation tool. Any audio or metadata can be used during training and development, but during evaluation the proposed simulated hearing aid or Enhancement Processor will not have access to all of the data (see next section).  ","version":"Next","tagName":"h3"},{"title":"Evaluation‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cec1/taking_part/cec1_rules#evaluation","content":" The only data that can be used by the Enhancement Processor during evaluation are  The audio input signals (the sum of the target and interferer for each hearing aid microphone), andThe listener characterisation (pure tone air-conduction audiograms).  ","version":"Next","tagName":"h3"},{"title":"Computational restrictions‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cec1/taking_part/cec1_rules#computational-restrictions","content":" Teams may choose to use all or some of the provided baseline models.Systems must be causal; the output at time t must not use any information from input samples more than 5 ms into the future (i.e., no information from input samples &gt;t+5 ms).There is no limit on computational cost.  Please see this blog post for further explanation of these last two rules about latency and computation time.  ","version":"Next","tagName":"h2"},{"title":"Submitting multiple entries‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cec1/taking_part/cec1_rules#submitting-multiple-entries","content":" If you wish to submit two entries, where one is optimised for MBSTOI and the other, for listening tests,  Both systems must be submitted for MBSTOI evaluation.You must register two teams, submitting each entry as a different team.In your documentation, you must make it clear which has been optimised for listening tests and the relationship between the two entries.  We will assume that if only one of these systems is to go forward to listening tests, your preference is to use the one optimised for listening tests.  ","version":"Next","tagName":"h2"},{"title":"Evaluation of systems‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cec1/taking_part/cec1_rules#evaluation-of-systems","content":" ","version":"Next","tagName":"h2"},{"title":"Stage 1: Objective evaluation‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cec1/taking_part/cec1_rules#stage-1-objective-evaluation","content":" Entries will be ranked according to average Modified Binaural Short-Time Objective Intelligibility (MBSTOI) score across all samples in the evaluation/test dataset (i.e., all signals submitted for the MBSTOI evaluation).  ","version":"Next","tagName":"h3"},{"title":"Stage 2: Listening test evaluation‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cec1/taking_part/cec1_rules#stage-2-listening-test-evaluation","content":" There is a limit on how many systems can be evaluated by the listener panel.A maximum of two entries can go through to the listener panel from any individual entrant. Furthermore, a second will only be allowed if it is judged by us to use significantly different signal processing approaches.We will choose which will go to the listener panel based on The top N scored using the objective evaluation method.A sample of M others that use contrasting and promising approaches. ","version":"Next","tagName":"h3"},{"title":"Results","type":0,"sectionRef":"#","url":"/docs/cec2/cec2_results","content":"","keywords":"","version":"Next"},{"title":"Prizes‚Äã","type":1,"pageTitle":"Results","url":"/docs/cec2/cec2_results#prizes","content":" The Hearing Industry Research Consortium prizes for systems with the best HASPI score were awarded as follows:  1st place: System E009, Cornell et al, Multi-channel Target Speaker Extraction with Refinement: The WAVLAB Submission to the Second Clarity Enhancement Challenge 2nd place: System E031, Liu and Zhang, DRC-NET for The 2nd Clarity Enhancement Challenge 3rd place: System E008_hr, Ouyang et al., The Orka Inc Entry to the 2nd Clarity Enhancement Challenge   The Hearing Industry Research Consortium prizes for systems providing the best listener intelligibility scores were awarded as follows:  1st place: System E009, Cornell et al, Multi-channel Target Speaker Extraction with Refinement: The WAVLAB Submission to the Second Clarity Enhancement Challenge 2nd place: System E031, Liu and Zhang, DRC-NET for The 2nd Clarity Enhancement Challenge 3rd place: System E037, Lei et al., The Nanjing University / Horizon Robotics system for the 2nd Clarity Enhancement Challenge  Congratulations to the winners! ","version":"Next","tagName":"h2"},{"title":"CEC2 Schedule","type":0,"sectionRef":"#","url":"/docs/cec2/cec2_dates","content":"CEC2 Schedule Key dates are as follows 30th March 2022: Challenge website launch 14th April 2022: Release of training and development data, plus core software. 30th April 2022: Release of full toolset and baseline system. 1st May 2022: Registration for challenge entrants opens. 25th July 2022: Evaluation data released 1st Sept 2022: 1st round submission deadline for evaluation by objective measure 15th Sept 2022: 2nd round submission deadline for listening tests Sept-Nov 2022: Listening test evaluation period. 12th Dec 2022: Results announced at a Clarity Challenge Workshop; prizes awarded. Workshop likely to be a one-day virtual event","keywords":"","version":"Next"},{"title":"Scene Generation","type":0,"sectionRef":"#","url":"/docs/cec2/data/cec2_scene_generation","content":"","keywords":"","version":"Next"},{"title":"References‚Äã","type":1,"pageTitle":"Scene Generation","url":"/docs/cec2/data/cec2_scene_generation#references","content":"   Schr√∂der, D. and Vorl√§nder, M., 2011, January. RAVEN: A real-time framework for the auralization of interactive virtual environments. In Proceedings of Forum Acusticum 2011 (pp. 1541-1546). Denmark: Aalborg. ","version":"Next","tagName":"h2"},{"title":"The 2nd Clarity Enhancement Challenge","type":0,"sectionRef":"#","url":"/docs/cec2/cec2_intro","content":"","keywords":"","version":"Next"},{"title":"Overview of challenge‚Äã","type":1,"pageTitle":"The 2nd Clarity Enhancement Challenge","url":"/docs/cec2/cec2_intro#overview-of-challenge","content":" We want you to improve speech in the presence of background noise - see Figure 1. On the left there is a person with a quantified hearing loss. They are listening to speech from the target talker on the right. They are both in a living room. There is interfering noise from a number of sources (TV and washing machine in this case). The speech and noise is sensed by microphones on the hearing aids of the listener. Your task is to take these microphone feeds and the listener‚Äôs hearing characteristics, to produce signals where the speech is more intelligible. We will evaluate the success of your processing using an objective speech intelligibility metric. Some entrants will also be evaluated by a panel of listeners with a hearing impairment.  Figure 1. The scenario involves one talker, a listener who rotates their head, and at least two common sources of unwanted sound.  The scenario has been made more difficult than the first Clarity Enhancement Challenge by having:  More noise sourcesMore varied noise sources (speech, music, appliances)The listener turns their head during the talking.Less predictable target onset timing.  For more details use the contents pane on the left to navigate the CEC2 site. ","version":"Next","tagName":"h2"},{"title":"Additional Tools","type":0,"sectionRef":"#","url":"/docs/cec2/software/cec2_additional_tools","content":"","keywords":"","version":"Next"},{"title":"Hearing loss model‚Äã","type":1,"pageTitle":"Additional Tools","url":"/docs/cec2/software/cec2_additional_tools#hearing-loss-model","content":" This is an open-source python implementation of a hearing loss model developed by Brian Moore, Michael Stone and other members of the Auditory Perception Group, University of Cambridge [1, 2].  Inputs: A stereo wav audio signal, e.g., the output of the hearing aid model and audiograms for left and right ear.Outputs: The signal after simulating the hearing loss as specified by the set of audiograms (stereo wav file), &lt;scene&gt;_&lt;listener&gt;_HL-output.wav  ","version":"Next","tagName":"h2"},{"title":"Differentiable source separation and hearing aid amplification modules‚Äã","type":1,"pageTitle":"Additional Tools","url":"/docs/cec2/software/cec2_additional_tools#differentiable-source-separation-and-hearing-aid-amplification-modules","content":" The modules are from the Sheffield E009 system in CEC1. The source separation module is a multi-channel Conv-TasNet optimised with a SNR objective. The hearing aid amplification module is an FIR filter optimised with an objective, which is the combination of a differentiable approximation to the hearing loss model and a STOI loss.  Inputs: six channels of mixed signals, i.e., mixed_CH1.wav, mixed_CH2.wav, and mixed_CH3.wavOutputs: a single channel enhanced signal, therefore two source separation and amplification modules for left and right ears need to be optimised for the enhanced binaural signal.  ","version":"Next","tagName":"h2"},{"title":"Speech intelligibility model (MBSTOI)‚Äã","type":1,"pageTitle":"Additional Tools","url":"/docs/cec2/software/cec2_additional_tools#speech-intelligibility-model-mbstoi","content":" Python implementation of a binaural intelligibility model, Modified Binaural Short-Time Objective Intelligibility (MBSTOI) [3]. Note that MBSTOI requires signal time-alignment (and alignment within one-third octave bands).  Inputs: HL-model output signals, audiogram, reference target signal (i.e., the premixed target signal convolved with the BRIR with the reflections 'turned off', specified as 'target_anechoic'), (scene metadata)Outputs: predicted intelligibility score  ","version":"Next","tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Additional Tools","url":"/docs/cec2/software/cec2_additional_tools#references","content":"   Moore, B. C. J., Alcantara, J. I., Stone, M. and Glasberg, B. R., 1999. Use of a loudness model for hearing aid fitting: II. Hearing aids with multi-channel compression. British Journal of Audiology, 33(3), pp. 157-170.Nejime, Y. and Moore, B. C., 1997. Simulation of the effect of threshold elevation and loudness recruitment combined with reduced frequency selectivity on the intelligibility of speech in noise. Journal of the Acoustical Society of America, 102(1), pp. 603-615.Andersen, A. H., de Haan, J. M., Tan, Z. H. and Jensen, J., 2018. Refinement and validation of the binaural short-time objective intelligibility measure for spatially diverse conditions. Speech Communication, 102, pp. 1-13. ","version":"Next","tagName":"h2"},{"title":"Baseline System","type":0,"sectionRef":"#","url":"/docs/cec2/software/cec2_baseline","content":"","keywords":"","version":"Next"},{"title":"Baseline performance‚Äã","type":1,"pageTitle":"Baseline System","url":"/docs/cec2/software/cec2_baseline#baseline-performance","content":" The average speech intelligibility (HASPI) score for the unprocessed development test set is 0.1615. When processed with the simple baseline hearing aid (i.e., NALR amplification followed by a simple automatic gain compressor) the average HASPI score increases to 0.2493. These results are summarised in the table below. Your task is to improve on the 0.2493 baseline HASPI score.  System\tHASPIUnprocessed\t0.1615 NAL-R baseline\t0.2493  ","version":"Next","tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Baseline System","url":"/docs/cec2/software/cec2_baseline#references","content":"   Kates, J.M. and Arehart, K.H., 2021. The hearing-aid speech perception index (haspi) version 2. Speech Communication, 131, pp.35-46. ","version":"Next","tagName":"h2"},{"title":"Find collaborators","type":0,"sectionRef":"#","url":"/docs/cec2/taking_part/cec2_find_a_team","content":"Find collaborators If you'd like to team up with someone else to compete in the challenges, we can help. Please complete this Google form to let us know your own expertise, and what you're looking for in a collaborator. We'll then put people in contact with possible collaborators. We encourage everyone to join the Clarity Challenge‚Äôs Google group to stay updated with project news and announcements. We post in there when we have new people seeking team members (we don't share any personally-identifying details to the group). You are welcome to contact us if you have any questions about forming a team or participating in the challenge: Email the Clarity Team","keywords":"","version":"Next"},{"title":"Core Software","type":0,"sectionRef":"#","url":"/docs/cec2/software/cec2_core_software","content":"","keywords":"","version":"Next"},{"title":"A. Scene generator‚Äã","type":1,"pageTitle":"Core Software","url":"/docs/cec2/software/cec2_core_software#a-scene-generator","content":" Fully open-source python code for generating hearing aid inputs for each scene  Inputs: target and interferer signals, HOA-IRs, RAVEN project (rpf) files, scene description JSON filesOutputs: Mixed target+interferer signals for each hearing aid channel, direct path (simulating a measurement close to the eardrum). Reverberated pre-mixed signals can also be optionally generated.  ","version":"Next","tagName":"h2"},{"title":"B. Baseline hearing aid processor‚Äã","type":1,"pageTitle":"Core Software","url":"/docs/cec2/software/cec2_core_software#b-baseline-hearing-aid-processor","content":" The baseline hearing aid consists of a NAL-R fitting amplification stage [1] followed by a simple automatic gain compressor. It produces output signals in 16-bit wav format ready for HASPI or listening test evaluation.  Inputs: Inputs for each hearing aid channel and audiograms to characterise the listeners.Outputs: Stereo hearing aid (HA) outputs signals.  ","version":"Next","tagName":"h2"},{"title":"C. HASPI Speech Intelligibility model‚Äã","type":1,"pageTitle":"Core Software","url":"/docs/cec2/software/cec2_core_software#c-haspi-speech-intelligibility-model","content":" Python implementation of the Hearing Aid Speech Perception Index (HASPI) model which is used for objective intelligibility estimation. This will be used in the stage 1 evaluation of entrants (see Rules).  Inputs: reference target signal (i.e., the premixed target signal convolved with the BRIR with the reflections ‚Äúturned off‚Äù, specified as ‚Äòtarget_anechoic‚Äô), HA output signals, audiogram, level reference (level in dB SPL which corresponds to 0 dB FS)Outputs: predicted intelligibility score It is important to remember that both reference target and HA output signals have to be calibrated to the same dB SPL level before calculating HASPI.  ","version":"Next","tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Core Software","url":"/docs/cec2/software/cec2_core_software#references","content":"   [1] Byrne, Denis, and Harvey Dillon. &quot;The National Acoustic Laboratories'(NAL) new procedure for selecting the gain and frequency response of a hearing aid.&quot; Ear and hearing 7.4 (1986): 257-265. ","version":"Next","tagName":"h2"},{"title":"CEC2 FAQ","type":0,"sectionRef":"#","url":"/docs/cec2/taking_part/cec2_faq","content":"","keywords":"","version":"Next"},{"title":"Speech Intelligibility‚Äã","type":1,"pageTitle":"CEC2 FAQ","url":"/docs/cec2/taking_part/cec2_faq#speech-intelligibility","content":" ","version":"Next","tagName":"h2"},{"title":"What is Speech Intelligibility?‚Äã","type":1,"pageTitle":"CEC2 FAQ","url":"/docs/cec2/taking_part/cec2_faq#what-is-speech-intelligibility","content":" The term Speech Intelligibility is generally used in two different ways. It can refer to how much speech is understood by a listener, or to the number of words correctly identified by a listener as a proportion or percentage of the total number of words. In the Clarity project, we are using the latter definition, i.e., the percentage of words in a sentence that a listener identified correctly. This percentage is the target for your prediction models.  Speech intelligibility captures how a listener's ability to participate in conversation is changed when the speech signal is degraded, e.g., by background noise and room reverberation, or is processed, e.g., by a hearing aid. Your prediction model will need to incorporate a model of the hearing abilities of each listener.  ","version":"Next","tagName":"h3"},{"title":"How is Speech Intelligibility measured with listeners?‚Äã","type":1,"pageTitle":"CEC2 FAQ","url":"/docs/cec2/taking_part/cec2_faq#how-is-speech-intelligibility-measured-with-listeners","content":" In the Clarity project, a set of listeners listen to a sentence and then say what words they heard. In this project, speech intelligibility is measured as the number of words identified correctly as a percentage of the total number of words in a sentence.  You might consider looking at other metrics, such as Word Error Rate (WER), which picks up on, e.g., where listeners insert words not in the original sentence. You might do this if you think that an estimate of WER or other metrics would help your system to estimate speech intelligibility, as defined in the Clarity project.  ","version":"Next","tagName":"h3"},{"title":"How is Speech Intelligibility objectively measured by a computer?‚Äã","type":1,"pageTitle":"CEC2 FAQ","url":"/docs/cec2/taking_part/cec2_faq#how-is-speech-intelligibility-objectively-measured-by-a-computer","content":" When fitting a hearing aid, it would be beneficial for an audiologist to be able to use an objective measure of speech intelligibility to determine what signal processing algorithm(s) should be used to compensate for the listener's hearing impairment. Objective measures are also useful when measured speech intelligibility scores are unavailable, such as when developing a machine learning-based hearing aid algorithm or some other speech enhancement method. Another advantage of non-intrusive measures is that they do not require time-alignment of processed and reference signals.  Objective measures - or metrics - of speech intelligibility are used to allow a computer to estimate the likely performance of humans in listening tests. The main goal of entries to the prediction challenge is to produce one of these measures that performs well for listeners with hearing loss. There are two broad classes of speech intelligibility models:  Intrusive metrics (also known as double-ended) are most common. This is where the intelligibility is estimated by comparing the degraded or processed speech signal with the original clean speech signal.Non-intrusive metrics (also known as single-ended or blind) are less well developed. This is where intelligibility is estimated from the degraded or processed speech signal alone.  In the Clarity project, both types of metrics are of interest. Intrusive metrics will be more accurate in many cases. However, there are hearing aid processes where the speech content is shifted in frequency, which will defeat most current intrusive speech intelligibility metrics. We also hypothesise that there might be issues with intrusive metrics and machine learning approaches in hearing aids that revoice the original speech.  ","version":"Next","tagName":"h3"},{"title":"What speech intelligibility models already exist and what are they used for?‚Äã","type":1,"pageTitle":"CEC2 FAQ","url":"/docs/cec2/taking_part/cec2_faq#what-speech-intelligibility-models-already-exist-and-what-are-they-used-for","content":" There aren't many speech intelligibility models that consider hearing impairment, but one that does is HASPI by Kates and Arehart. In this seminar from the first Clarity workshop, James Kates discusses speech intelligibility models with a focus on the ones he has developed. He also discusses the speech quality metric HASQI. If you're interested in using HASPI or HASQI for the challenge, James Kates has kindly made the MATLAB code and user guide available for download.    Click arrow to see synopsis. Signal degradations, such as additive noise and nonlinear distortion, can reduce the intelligibility and quality of a speech signal. Predicting intelligibility and quality for hearing aids is especially difficult since these devices may contain intentional nonlinear distortion designed to make speech more audible to a hearing-impaired listener. This speech processing often takes the form of time-varying multichannel gain adjustments. Intelligibility and quality metrics used for hearing aids and hearing-impaired listeners must therefore consider the trade-offs between audibility and distortion introduced by hearing-aid speech envelope modifications. This presentation uses the Hearing Aid Speech Perception Index (HASPI) and the Hearing Aid Speech Quality Index (HASQI) to predict intelligibility and quality, respectively. These indices incorporate a model of the auditory periphery that can be adjusted to reflect hearing loss. They have been trained on intelligibility scores and quality ratings from both normal-hearing and hearing-impaired listeners for a wide variety of signal and processing conditions. The basics of the metrics are explained, and the metrics are then used to analyse the effects of additive noise on speech, to evaluate noise suppression algorithms, and to measure differences among commercial hearing aids.  ","version":"Next","tagName":"h3"},{"title":"Hearing Loss‚Äã","type":1,"pageTitle":"CEC2 FAQ","url":"/docs/cec2/taking_part/cec2_faq#hearing-loss","content":" There are many types of hearing loss, but the focus of the Clarity project is the hearing loss that happens with ageing. This is a form of sensorineural hearing loss.  ","version":"Next","tagName":"h2"},{"title":"How does hearing loss affect the perception of audio signals, and how do modern hearing aids process sound to help with this?‚Äã","type":1,"pageTitle":"CEC2 FAQ","url":"/docs/cec2/taking_part/cec2_faq#how-does-hearing-loss-affect-the-perception-of-audio-signals-and-how-do-modern-hearing-aids-process-sound-to-help-with-this","content":" In this seminar from the first Clarity workshop, Karolina Smeds from ORCA Europe and WS Audiology discusses the effects of hearing loss and the hearing aid processing strategies that are typically used to counter the sensory deficits.    Click arrow to see synopsis. Hearing loss leads to several unwanted effects. Loss of audibility for soft sounds is one effect, but also when amplification is used to create audibility for soft sounds, many suprathreshold deficits remain. The most common type of hearing loss is a cochlear hearing loss, where haircells or nerve synapses in the cochlea are damaged. Ageing and noise exposure are the most common causes of cochlear hearing loss. This type of hearing loss is associated with atypical loudness perception and difficulties in noisy situations. Background noise masks for instance speech to a higher degree than for a person with healthy hair cells. This explains why listening to speech-in-noise (SPIN) is such an important topic to work on. A brief introduction to signal processing in hearing aids will be presented. With the use of frequency-specific amplification and compression (automatic gain control, AGC), hearing aids are usually doing a good job in compensating for reduced audibility and for atypical suprathreshold loudness perception. However, it is more difficult to compensate for the increased masking effect. Some examples of strategies will be presented. Finally, natural conversations in noise will be discussed. The balance between being able to have a conversation with a specific communication partner in a group of people and being able to switch attention if someone else starts to talk will be touched upon. ","version":"Next","tagName":"h3"},{"title":"Listening Tests","type":0,"sectionRef":"#","url":"/docs/cec2/taking_part/cec2_listening_tests","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"Listening Tests","url":"/docs/cec2/taking_part/cec2_listening_tests#overview","content":" The listeners will be provided with a USB stereo headset to complete the listening experiment. The experiment will be run by our ‚ÄúListen@Home‚Äù web software running on either the participant‚Äôs own computer or a tablet we supply. The software plays each sentence once, then the participant speaks aloud what they think was said by the target talker. Their response is recorded by the headset‚Äôs microphone for offline scoring. We ask that the participant uses a quiet room for the experiment.  Intelligibility will be evaluated as the number of words identified correctly in the sentence.  Our plan is that each listener will undertake a few hours of listening and evaluate sentences from every entrant. We will use a combinatorial design to equate this as far as possible.  Should a listener drop out from the panel, we will endeavour to replace them with someone with a similar hearing loss, but should that prove impractical we will reduce the size of the panel, and inform entrants which listener has withdrawn.  ","version":"Next","tagName":"h2"},{"title":"Listen@Home hardware‚Äã","type":1,"pageTitle":"Listening Tests","url":"/docs/cec2/taking_part/cec2_listening_tests#listenhome-hardware","content":" We will be using Sennheiser PC-8 headsets to play the sounds to our participants. We will allow participants to set the volume so that the sounds are not so loud to be uncomfortable. Without loudness-recruitment measures for our listeners, we cannot be sure just what loudnesses every participant will hear, so we need to allow them to make the choice here.  We have measurements of the output capability of a system in the laboratory:  A 1 kHz pure tone set to be the most powerful it can be (i.e., an amplitude range of +/-1 = RMS amplitude of 0.707, and the volume controls at 100%) gave 99 dB(A) SPL on the PC-8 headphones.An ICRA speech-shaped noise [1], unmodulated in time, and scaled to an RMS of 0.3, gave 90 dB(A) at the same volume level. With this RMS, the noise had 0.1% of its samples clipped at +/- 1.  Due to the above capabilities of the reproduction equipment, in the submitted signals, 0 dB FS should correspond to 100 dB SPL. We will also require the signals to be provided as 16-bit WAV files with a 32 kHz sampling rate.  We will play the signals as is using an HTML/PHP audio player coded on a webpage. The responsibility for the final signal level is therefore yours. It‚Äôs worth bearing in mind that should your signals overall seem too loud to be comfortable to a participant, they may well turn down the volume themselves.  ","version":"Next","tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Listening Tests","url":"/docs/cec2/taking_part/cec2_listening_tests#references","content":"   ICRA standard noises, https://icra-audiology.org/Repository/icra-noise. We used track #1. ","version":"Next","tagName":"h2"},{"title":"CEC2 Registration","type":0,"sectionRef":"#","url":"/docs/cec2/taking_part/cec2_registration","content":"CEC2 Registration Teams are required to register using the form below. Please submit one form per team, providing a single contact email address. Once you have registered, you will receive an email confirmation with a team ID. When the submission date approaches, you will be sent an individualised link to a Google Drive for submitting materials. Loading‚Ä¶ Registration closes on August 28th, but earlier registration will help us to plan for the listening tests.","keywords":"","version":"Next"},{"title":"CEC2 Prizes","type":0,"sectionRef":"#","url":"/docs/cec2/taking_part/cec2_prizes","content":"","keywords":"","version":"Next"},{"title":"The Team Prize‚Äã","type":1,"pageTitle":"CEC2 Prizes","url":"/docs/cec2/taking_part/cec2_prizes#the-team-prize","content":" Team prizes have been made available by the generosity of the Hearing Industry Research ConsortiumThere will be separate HASPI and listening test prizes for the top systems.   HASPI prize 1st Place $1000 2nd Place $500 3rd Place $250 Listening Test prize 1st Place $1000 2nd Place $500 3rd Place $250      info Anonymous entries and those with direct links to the Clarity project team are ineligible for cash prizes, sorry. ","version":"Next","tagName":"h2"},{"title":"CEC2 Rules","type":0,"sectionRef":"#","url":"/docs/cec2/taking_part/cec2_rules","content":"","keywords":"","version":"Next"},{"title":"Teams‚Äã","type":1,"pageTitle":"CEC2 Rules","url":"/docs/cec2/taking_part/cec2_rules#teams","content":" Teams must have pre-registered and nominated a contact person.Teams can be from one or more institutions.The organisers may enter the challenge themselves but will not be eligible to win prizes.  ","version":"Next","tagName":"h2"},{"title":"Transparency‚Äã","type":1,"pageTitle":"CEC2 Rules","url":"/docs/cec2/taking_part/cec2_rules#transparency","content":" Teams must provide a technical document of up to 2 pages describing the system/model and any external data and pre-existing tools, software and models used.We will publish all technical documents (anonymous or otherwise).Teams are encouraged ‚Äì but not required ‚Äì to provide us with access to the system/model and to make their code open source.Anonymous entries are allowed but will not be eligible for prizes.Teams may reserve the right to be referred to using anonymous code names in the published rank ordering.  ","version":"Next","tagName":"h2"},{"title":"What information can I use?‚Äã","type":1,"pageTitle":"CEC2 Rules","url":"/docs/cec2/taking_part/cec2_rules#what-information-can-i-use","content":" ","version":"Next","tagName":"h2"},{"title":"Training and development‚Äã","type":1,"pageTitle":"CEC2 Rules","url":"/docs/cec2/taking_part/cec2_rules#training-and-development","content":" There is no limit on the amount of training data that can be generated using our tools and training data sets. Teams can also use their own data for training or expand the training data through simple automated modifications. However, teams that do this must make a second submission using only the official audio files and signal generation tool. Any audio or metadata can be used during training and development, but during evaluation, the proposed simulated hearing aid or Enhancement Processor will not have access to all of the data (see next section).  ","version":"Next","tagName":"h3"},{"title":"Evaluation‚Äã","type":1,"pageTitle":"CEC2 Rules","url":"/docs/cec2/taking_part/cec2_rules#evaluation","content":" The only data that can be used by the Enhancement Processor during evaluation are  The audio input signals (the sum of the target and interferers for each hearing aid microphone).The listener characterisation (pure tone air-conduction audiograms and/or digit triple test results).The provided clean audio examples for the target talker (these will not be the same as any of the target utterances.)The head-rotation signal (but if used, a version of the system that does not use it should also be prepared for comparison.)  ","version":"Next","tagName":"h3"},{"title":"Computational restrictions‚Äã","type":1,"pageTitle":"CEC2 Rules","url":"/docs/cec2/taking_part/cec2_rules#computational-restrictions","content":" Teams may choose to use all, some or none of the parts of the baseline model.Systems must be causal; the output from the hearing aid at time t must not use any information from input samples more than 5 ms into the future (i.e., no information from input samples &gt;t+5 ms).There is no limit on computational cost.  Please see this blog post for further explanation of these last two rules about latency and computation time.  ","version":"Next","tagName":"h2"},{"title":"Submitting multiple entries‚Äã","type":1,"pageTitle":"CEC2 Rules","url":"/docs/cec2/taking_part/cec2_rules#submitting-multiple-entries","content":" You can submit two entries, where one is optimised for HASPI and the other for listening tests if you wish. In this case:  Both systems must be submitted for HASPI evaluation.You must register two teams, submitting each entry as a different team.In your documentation, you must make it clear which has been optimised for listening tests and the relationship between the two entries.head-rotation: if the head-rotation signal is used then a second entry must be submitted that does not use it and allows the benefit to be measured.  We will assume that if only one of these systems is to go forward to listening tests, your preference is to use the one optimised for listening tests.  ","version":"Next","tagName":"h2"},{"title":"Evaluation of systems‚Äã","type":1,"pageTitle":"CEC2 Rules","url":"/docs/cec2/taking_part/cec2_rules#evaluation-of-systems","content":" ","version":"Next","tagName":"h2"},{"title":"Stage 1: Objective evaluation‚Äã","type":1,"pageTitle":"CEC2 Rules","url":"/docs/cec2/taking_part/cec2_rules#stage-1-objective-evaluation","content":" Entries will be ranked according to average HASPI score across all signals in the evaluation dataset. We will use the HASPI implementation in the baseline system.  ","version":"Next","tagName":"h3"},{"title":"Stage 2: Listening test evaluation‚Äã","type":1,"pageTitle":"CEC2 Rules","url":"/docs/cec2/taking_part/cec2_rules#stage-2-listening-test-evaluation","content":" There is a limit on how many systems can be evaluated by the listener panel.A maximum of two entries can go through to the listener panel from any individual entrant. Furthermore, a second will only be allowed if it is judged by us to use significantly different signal processing approaches.We will choose which will go to the listener panel based on The top N scored using the objective metric HASPI.A sample of M others that use contrasting and promising approaches.  ","version":"Next","tagName":"h3"},{"title":"Intellectual property‚Äã","type":1,"pageTitle":"CEC2 Rules","url":"/docs/cec2/taking_part/cec2_rules#intellectual-property","content":" The following terms apply to participation in this machine learning challenge (‚ÄúChallenge‚Äù). Entrants may create original solutions, prototypes, datasets, scripts, or other content, materials, discoveries or inventions (a ‚ÄúSubmission‚Äù). The Challenge is organised by the Challenge Organiser.  Entrants retain ownership of all intellectual and industrial property rights (including moral rights) in and to Submissions.  As a condition of submission, Entrant grants the Challenge Organiser, its subsidiaries, agents and partner companies, a perpetual, irrevocable, worldwide, royalty-free, and non-exclusive licence to use, reproduce, adapt, modify, publish, distribute, publicly perform, create a derivative work from, and publicly display the Submission.  Entrants provide Submissions on an ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. ","version":"Next","tagName":"h2"},{"title":"CEC2 Submission","type":0,"sectionRef":"#","url":"/docs/cec2/taking_part/cec2_submission","content":"","keywords":"","version":"Next"},{"title":"What evaluation data is provided?‚Äã","type":1,"pageTitle":"CEC2 Submission","url":"/docs/cec2/taking_part/cec2_submission#what-evaluation-data-is-provided","content":" The evaluation data consists of 1500 scenes. For each scene, you are provided with the signals received at each of the three microphones on the left and right hearing aid device. You will also be provided with JSON or csv formatted metadata consisting of  the audiograms and DTT results for a set of listeners anda mapping of which listeners will listen to which scenes.  There will also be some clean audio examples for the target talker, that are not the same as the target utterance.  For the stage 1 HASPI evaluation, there will be one listener per scene and the scene-listener mapping will be the same for all teams.  For the stage 2 listening test evaluation, there will be five listeners per scene and each team will have a separate scene-listener mapping. The file formats will be the same as used for the development data; for details see the CEC2 Data page.  ","version":"Next","tagName":"h2"},{"title":"What audio do I need to submit?‚Äã","type":1,"pageTitle":"CEC2 Submission","url":"/docs/cec2/taking_part/cec2_submission#what-audio-do-i-need-to-submit","content":" You must submit the audio signals produced at the output of your simulated hearing aid for the evaluation datasets. You will be asked to provide two sets of signals: the first for the HASPI evaluation and the second for the listening tests (see submission dates above).  Signals should be submitted as 16-bit WAV files with a 32 kHz sampling rate, and 0 dB FS corresponds to 100 dB SPL, given the capabilities of the listening test reproduction equipment. The format of signals submitted for HASPI evaluation and for the listening tests is the same.  We also encourage you to submit your simulated hearing aid code.  See the page on listening tests for more information about the levels that can be reproduced by the listening test equipment. When playing signals to listeners we will then play them as is. The responsibility for the final signal level is therefore yours. It‚Äôs worth bearing in mind that should your signals overall seem too loud to be comfortable to a participant, they may well turn down the volume themselves.  ","version":"Next","tagName":"h2"},{"title":"Naming and packaging signals‚Äã","type":1,"pageTitle":"CEC2 Submission","url":"/docs/cec2/taking_part/cec2_submission#naming-and-packaging-signals","content":" Your processed signals should be named using the conventions used by the baseline system, i.e., &lt;Scene ID&gt;_&lt;Listener ID&gt;_HA-output.wav and explained on the CEC2 data page. These should be placed in a directory whose name is the unique team ID that you will be sent, e.g., E001 and then packaged using zip or tar or any standard packaging tool. The resulting file should be about 2 GB for the first round.  ","version":"Next","tagName":"h2"},{"title":"Technical report‚Äã","type":1,"pageTitle":"CEC2 Submission","url":"/docs/cec2/taking_part/cec2_submission#technical-report","content":" The two page technical report must be submitted as a paper to the Clarity-2022 Workshop. Deadline - see date above. An author kit and submission instructions will be made available.A draft of the report needs to be uploaded to the Google Drive along with your HASPI signals - see above for deadline. The draft needs to be sufficiently complete for us to judge whether your system is compliant with the challenge rules.Your report should include an abstract and introduction and sections on experimental setup/methodology including system information and model/network architecture, evaluation/results, discussion, conclusion and references. Please provide an estimation of the computational resources needed. You must describe any external data and pre-existing tools, software and models used.  ","version":"Next","tagName":"h2"},{"title":"How will intellectual property be handled?‚Äã","type":1,"pageTitle":"CEC2 Submission","url":"/docs/cec2/taking_part/cec2_submission#how-will-intellectual-property-be-handled","content":" See here under Intellectual Property.  ","version":"Next","tagName":"h2"},{"title":"Where do I submit the signals?‚Äã","type":1,"pageTitle":"CEC2 Submission","url":"/docs/cec2/taking_part/cec2_submission#where-do-i-submit-the-signals","content":" When you have registered you will receive a link to a Google Drive to which you will be able to securely upload your signals. You will be able to use the same link to upload materials for both the 1st submission and the 2nd submission if you are selected for the 2nd round. We also encourage you to submit your simulated hearing aid code via this link.  Materials uploaded will be visible to the Clarity Team but not to other entrants.  warning Note, in order to use the Google Drive you will need to have a Google account. If you anticipate problems using Google then please make arrangements to send us the materials by other means, e.g., via a service such as WeTransfer or similar. ","version":"Next","tagName":"h2"},{"title":"CEC1 Data","type":0,"sectionRef":"#","url":"/docs/cec1/data/cec1_data","content":"","keywords":"","version":"Next"},{"title":"A. Training, development, evaluation data‚Äã","type":1,"pageTitle":"CEC1 Data","url":"/docs/cec1/data/cec1_data#a-training-development-evaluation-data","content":" The dataset is split into these three subsets: training (train), development (dev) and evaluation (eval).  You should only train on the training set.The system submitted should be chosen on the evidence provided by the development set.The final listening and ranking will be performed with the (held-out) evaluation set.For more information on supplementing the training data, please see the rules. The evaluation dataset will be made available one month before the challenge submission deadline.  ","version":"Next","tagName":"h2"},{"title":"B. The scene dataset‚Äã","type":1,"pageTitle":"CEC1 Data","url":"/docs/cec1/data/cec1_data#b-the-scene-dataset","content":" The complete dataset is composed of 10,000 scenes split into the following sets:  Training (6000 scenes, 24 speakers);Development (2500 scenes, 10 speakers);Evaluation (1500 scenes, 6 speakers).  Each scene corresponds to a unique target utterance and a unique segment of noise from an interferer. The training, development and evaluation sets are disjoint for target speaker. The three sets are balanced for target speaker gender.  Binaural Room Impulse Responses (BRIRs) are used to model how the sound is altered as it propagates through the room and interacts with the head. The audio signals for the scenes are generated by convolving source signals with the BRIRs and summing. See the page on modelling the scenario for more details. Randomised room dimensions, target and interferer locations are used.  The BRIRs are generated for:  A hearing aid with 3 microphone inputs (front, mid, rear). The hearing aid has a Behind-The-Ear (BTE) form factor; see Figure 1. The distance between microphones is approx. 7.6 mm. The properties of the tube and ear mould are not considered.Close to the eardrum.The anechoic target reference (front microphone).    Figure 1. Front (Fr), Middle (Mid) and Rear microphones on a BTE hearing aid form.  Head Related Impulse Responses (HRIRs) are used to model how sound is altered as it propagates in a free-field and interacts with the head (i.e., no room is included). These are taken from the OlHeadHRTF database with permission. These include HRIRs for human heads and for three types of head-and-torso simulator/mannekin. The eardrum HRIRs (labelled ED) are for a position close to the eardrum of the open ear.  rpf files are specification files for the geometric room acoustic model that include a complete description of the room.  ","version":"Next","tagName":"h2"},{"title":"B.1 Training data‚Äã","type":1,"pageTitle":"CEC1 Data","url":"/docs/cec1/data/cec1_data#b1-training-data","content":" For each scene in the training data the following signals and metadata are available:  The target and interferer BRIRs (4 pairs: front, mid, rear and eardrum for left and right ears).HRIRs including those corresponding to the target azimuth.The mono target and interferer signals (pre-convolution).For each hearing aid microphone (channels 1-3 where channel 1 is front, channel 2 is mid and channel 3 is rear) and a position close to the eardrum (channel 0): The target convolved with the appropriate BRIR;The interferer convolved with the appropriate BRIR;The sum of the target and interferer convolved. The target convolved with the anechoic BRIR (channel 1) for each ear (‚Äòtarget_anechoic‚Äô).Metadata describing the scene: a JSON file containing, e.g., the filenames of the sources, the location of the sources, the viewvector of the target source, the location and viewvector of the receiver, the room dimensions (see specification below), and the room number, which corresponds to the RAVEN BRIR, rpf and ac files.  Software for generating more training data is also available.  ","version":"Next","tagName":"h3"},{"title":"B.2 Development data‚Äã","type":1,"pageTitle":"CEC1 Data","url":"/docs/cec1/data/cec1_data#b2-development-data","content":" The same data as for the training will be made available to allow you to fully examine the performance of your system. Note, that the data available for the evaluation will be much more limited (see B.3).  For each scene, during development, your hearing aid enhancement model must only use the following input signals/data:  The sum of the target and interferer ‚Äì mixed at the SNR specified in the scene metadata ‚Äì at one or more hearing aid microphones (CH1, CH2 and/or CH3).The IDs of the listeners assigned to the scene in the metadata provided.The audiograms of these listeners.  ","version":"Next","tagName":"h3"},{"title":"B.3 Evaluation scene data‚Äã","type":1,"pageTitle":"CEC1 Data","url":"/docs/cec1/data/cec1_data#b3-evaluation-scene-data","content":" For each scene in the evaluation data only the following will be available:  The sum of the target and interferer for each hearing aid microphone.The ID of the evaluation panel members/listeners who will be listening to the processed scene.The audiograms of these listeners.  ","version":"Next","tagName":"h3"},{"title":"C Listener data‚Äã","type":1,"pageTitle":"CEC1 Data","url":"/docs/cec1/data/cec1_data#c-listener-data","content":" ","version":"Next","tagName":"h2"},{"title":"C.1 Training and development data‚Äã","type":1,"pageTitle":"CEC1 Data","url":"/docs/cec1/data/cec1_data#c1-training-and-development-data","content":" A sample of pure tone air-conduction audiograms that characterise the hearing impairment of potential listeners, split into training and development sets.  ","version":"Next","tagName":"h3"},{"title":"C.2 Evaluation data‚Äã","type":1,"pageTitle":"CEC1 Data","url":"/docs/cec1/data/cec1_data#c2-evaluation-data","content":" You will be given the left and right pure tone air-conduction audiograms for the listening panel, so the signals you generate for evaluation can be individualised to the listeners.  A panel of 50 hearing-aided listeners will be recruited for the evaluation panel. We plan that they will be experienced bilateral hearing-aid users (they use two hearing aids but the hearing loss may be asymmetrical) with an averaged hearing loss as measured by pure tone air-conduction of between 25 and about 60 dB in the better ear, with fluent speaking of (and listening to) British English.  ","version":"Next","tagName":"h3"},{"title":"D Data file formats and naming conventions‚Äã","type":1,"pageTitle":"CEC1 Data","url":"/docs/cec1/data/cec1_data#d-data-file-formats-and-naming-conventions","content":" ","version":"Next","tagName":"h2"},{"title":"D.1 Abbreviations in Filenames‚Äã","type":1,"pageTitle":"CEC1 Data","url":"/docs/cec1/data/cec1_data#d1-abbreviations-in-filenames","content":" R ‚Äì ‚Äúroom‚Äù: e.g., ‚ÄúR02678‚Äù # Room ID linking to RAVEN rpf fileS ‚Äì ‚Äúscene‚Äù: e.g., S00121 # Scene ID for a particular setup in a room I.e., room + choice of target and interferer signalsBNC ‚Äì BNC sentence identifier e.g. BNC_A06_01702CH ‚Äì CH0 ‚Äì eardrum signalCH1 ‚Äì front signal, hearing aid channelCH2 ‚Äì middle signal, hearing aid channelCH3 ‚Äì rear signal, hearing aid channel I/i1 ‚Äì Interferer, i.e., noise or sentence ID for the interferer/maskerT ‚Äì talker who produced the target speech sentencesL ‚Äì listenerE ‚Äì entrant (identifying a team participating in the challenge)t ‚Äì target (used in BRIRs and RAVEN project ‚Äòrpf‚Äô files)  ","version":"Next","tagName":"h3"},{"title":"D.2 General‚Äã","type":1,"pageTitle":"CEC1 Data","url":"/docs/cec1/data/cec1_data#d2-general","content":" Audio and BRIRs will be 44.1 kHz 32 bit wav files in either mono or stereo as appropriate.Where stereo signals are provided the two channels represent the left and right signals of the ear or hearing aid microphones.HRIRs have a sampling rate of 48 kHz.Metadata will be stored in JSON format wherever possible.Room descriptions are stored as RAVEN project ‚Äòrpf‚Äô configuration files.Signals are saved within the Python code as 32-bit floating point by default.  ","version":"Next","tagName":"h3"},{"title":"D.3 Prompt and transcription data‚Äã","type":1,"pageTitle":"CEC1 Data","url":"/docs/cec1/data/cec1_data#d3-prompt-and-transcription-data","content":" The following text is available for the target speech:  Prompts are the text that was supposed to be spoken as presented to the readers.‚ÄòDot‚Äô transcriptions contain the text as it was spoken in a form more suitable for scoring tools.These are stored in the master json metadata file.  ","version":"Next","tagName":"h3"},{"title":"D.4 Source audio files‚Äã","type":1,"pageTitle":"CEC1 Data","url":"/docs/cec1/data/cec1_data#d4-source-audio-files","content":" Wav files containing the original source materials.Original target sentence recordings:   &lt;Talker ID&gt;_&lt;BNC sentence identifier&gt;.wav   ","version":"Next","tagName":"h3"},{"title":"D.5 Preprocessed scene signals‚Äã","type":1,"pageTitle":"CEC1 Data","url":"/docs/cec1/data/cec1_data#d5-preprocessed-scene-signals","content":" Audio files storing the signals picked up by the hearing aid microphone ready for processing. Separate signals are generated for each hearing aid microphone pair or ‚Äòchannel‚Äô.  &lt;Scene ID&gt;_target_&lt;Channel ID&gt;.wav &lt;Scene ID&gt;_interferer_&lt;Channel ID&gt;.wav &lt;Scene ID&gt;_mixed_&lt;Channel ID&gt;.wav &lt;Scene ID&gt;_target_anechoic.wav   Scene ID ‚Äì S00001 to S10000  S followed by 5 digit integer with 0 pre-padding  Channel ID  CH0 ‚Äì Eardrum signalCH1 ‚Äì Hearing aid front microphoneCH2 ‚Äì Hearing aid middle microphoneCH3 ‚Äì Hearing aid rear microphone  ","version":"Next","tagName":"h3"},{"title":"D.6 Enhanced signals‚Äã","type":1,"pageTitle":"CEC1 Data","url":"/docs/cec1/data/cec1_data#d6-enhanced-signals","content":" The signals that are output by the enhancement (hearing aid) model.  &lt;Scene ID&gt;_&lt;Listener ID&gt;_HA-output.wav #HA output signal (i.e., as submitted by the challenge entrants)  Listener ID ‚Äì ID of the listener panel member, e.g., L001 to L100 for initial ‚Äòpseudo-listeners‚Äô, etc. We are no longer providing the script for post-processing signals in preparation for the listener panel.  ","version":"Next","tagName":"h3"},{"title":"D.7 Enhanced signals processed by the hearing loss model‚Äã","type":1,"pageTitle":"CEC1 Data","url":"/docs/cec1/data/cec1_data#d7-enhanced-signals-processed-by-the-hearing-loss-model","content":" The signals that are produced by the hearing loss (HL) model.  &lt;Scene ID&gt;_&lt;Listener ID&gt;_HL-output.wav HL output signal&lt;Scene ID&gt;_&lt;Listener ID&gt;_HL-mixoutput.wav HL-processed CH0 signal, bypassing HA processing, for comparison&lt;Scene ID&gt;_&lt;Listener ID&gt;_flat0dB_HL-output HL-output for flat 0 dB audiogram processed signal for comparison&lt;Scene ID&gt;_&lt;Listener ID&gt;_HLddf-output unit impulse signal output by HL model for time-alignment of signals before processing by the baseline speech intelligibility model  ","version":"Next","tagName":"h3"},{"title":"D.8 Scene metadata‚Äã","type":1,"pageTitle":"CEC1 Data","url":"/docs/cec1/data/cec1_data#d8-scene-metadata","content":" JSON file containing a description of the scene and assigns the scene to a specific member of the listening panel. It is a hierarchical dictionary, with the top level being scenes indexed by unique scene ID, and each scene described by a second-level dictionary. Here, viewvector indicates the direction vector or line of sight.  scenes.json [ { &quot;scene&quot;: &quot;S00001&quot;, &quot;room&quot;: { &quot;name&quot;: &quot;R00001&quot;, &quot;dimensions&quot;: &quot;5.9x3.4186x2.9&quot; // Room dimensions in metres }, &quot;SNR&quot;: 3.8356, &quot;hrirfilename&quot;: &quot;VP_N5-ED&quot;, // HRIR filename &quot;target&quot;: { // target positions (x,y,z) and view vectors (look directions, x,y,z) &quot;Positions&quot;: [ -0.5, 3.4, 1.2 ], &quot;ViewVectors&quot;: [ 0.291, -0.957, 0 ], &quot;name&quot;: &quot;T022_HCS_00002&quot;, // target speaker code and BNCid &quot;nsamples&quot;: 153468, // length of target speech in samples }, &quot;listener&quot;: { &quot;Positions&quot;: [ 0.2, 1.1, 1.2 ], &quot;ViewVectors&quot;: [ -0.414, 0.91, 0 ] }, &quot;interferer&quot;: { &quot;Positions&quot;: [ 0.4, 3.2, 1.2 ], &quot;name&quot;: &quot;CIN_dishwasher_012&quot;, // interferer name &quot;nsamples&quot;: 1190700, // interferer length in samples &quot;duration&quot;: 27, // interferer duration in seconds &quot;type&quot;: &quot;noise&quot;, // interferer type: noise or speech &quot;offset&quot;: 182115, // interferer segment starts at n samples from beginning of recording }, &quot;azimuth_target_listener&quot;: -7.55, // angle azimuth in degrees of target for receiver &quot;azimuth_interferer_listener&quot;: -29.92, // angle azimuth in degrees of interferer for receiver &quot;dataset&quot;: &quot;train&quot;, // dataset: train, dev or eval/test &quot;pre_samples&quot;: 88200, // number of samples of interferer before target onset &quot;post_samples&quot;: 44100 // number of samples of interferer after target offset }, { // etc. } ]   There are JSON files containing the scene specifications per dataset, e.g., scenes.train.json.- Note, that the scene ID and room ID might have a one-to-one mapping in the challenge, but are not necessarily the same. Multiple scenes can be made by changing the target and masker choices for a given room. E.g., participants wanting to expand the training data could remix multiple scenes from the same room.A scene is completely described by the room ID and target and interferer source IDs, as all other information, e.g., source + target geometry are already in the RAVEN project rpf files. Only the room ID is needed to identify the BRIR files.The listener ID is not stored in the scene metadata; this information is stored separately in a scenes_listeners.json file.Non-speech interferers are labelled CIN_&lt;noise type&gt;_XXX, while speech interferers are labelled &lt;three letter code including dialect and talker gender&gt;_XXXXX .  ","version":"Next","tagName":"h3"},{"title":"D.9 Listener metadata‚Äã","type":1,"pageTitle":"CEC1 Data","url":"/docs/cec1/data/cec1_data#d9-listener-metadata","content":" Listener data stored in a single JSON file with the following format.  listeners.json { &quot;L0001&quot;: { &quot;name&quot;: &quot;L0001&quot;, &quot;audiogram_cfs&quot;: [250, 500, 1000, 2000, 3000, 4000, 6000, 8000], &quot;audiogram_levels_l&quot;: [10, 10, 20, 30, 40, 55, 55, 60], &quot;audiogram_levels_r&quot;: [ 10, 15, 25, 40, 50, 65, 65, 70 ], }, &quot;L0002&quot;: { // ... etc }, // ... etc }   ","version":"Next","tagName":"h3"},{"title":"D.10 Scene-Listener map‚Äã","type":1,"pageTitle":"CEC1 Data","url":"/docs/cec1/data/cec1_data#d10-scene-listener-map","content":" JSON file named scenes_listeners.json dictates which scenes are to be processed by which listeners.  scenes_listeners.json { &quot;S00001&quot;: [&quot;L0001&quot;, &quot;L0002&quot;, &quot;L0003&quot;], &quot;S00002&quot;: [&quot;L0003&quot;. &quot;L0005&quot;, &quot;L0007&quot;], // etc }  ","version":"Next","tagName":"h3"},{"title":"CEC3 Download","type":0,"sectionRef":"#","url":"/docs/cec3/cec3_download","content":"","keywords":"","version":"Next"},{"title":"Software‚Äã","type":1,"pageTitle":"CEC3 Download","url":"/docs/cec3/cec3_download#software","content":" All the necessary software tools are available as part of the PyClarity software package available on GitHub. The challenge is using version 0.5.x of the package.  We recommend that you download and install the latest packaged release in the 0.5.x series from GitHub. Doing this will make it easy to access the command-line recipes for the baseline system.  The software can be installed as a library using  pip install &quot;pyclarity~=0.5.1&quot;  Further installation instructions are available in the repository's README.  ","version":"Next","tagName":"h3"},{"title":"Data‚Äã","type":1,"pageTitle":"CEC3 Download","url":"/docs/cec3/cec3_download#data","content":" The data is available for download here.  On the download site you will see the following packages are available,  clarity_CEC3_data.v1_0.tar.gz [74 GB] - data from Task 1 and Task 2clarity_CEC3_task3_data.v1_0.tar.gz - data from Task 3 (to appear, 1st March)  To unpack the data, use the following command:  tar -xvzf clarity_CEC3_data.v1_0.tar.gz   If unpacking more than one package, please unpack them in the same directory so that the unpackaged data is merged into the same root directory.  ","version":"Next","tagName":"h3"},{"title":"Additional Training Data‚Äã","type":1,"pageTitle":"CEC3 Download","url":"/docs/cec3/cec3_download#additional-training-data","content":" For Task 1, participants are encouraged to use the training data set from CEC2.  The data is available for download here.  On the download site you will see the following package,  clarity_CEC2_train.v1_1.tgz [69 GB] - scenes for training systems ","version":"Next","tagName":"h3"},{"title":"The 3rd Clarity Enhancement Challenge","type":0,"sectionRef":"#","url":"/docs/cec3/cec3_intro","content":"","keywords":"","version":"Next"},{"title":"Overview of challenge‚Äã","type":1,"pageTitle":"The 3rd Clarity Enhancement Challenge","url":"/docs/cec3/cec3_intro#overview-of-challenge","content":" The challenge provides participants with hearing aid input signals representing scenes containing a target speaker. Participants are asked to process the signals to provide hearing aid output signals that will be intelligible to hearing-impaired listeners. The challenge is evaluated using standard objective speech intelligibility metrics but also with listening tests with hearing-impaired listeners.  The challenge is formed of three enhancement tasks that add realism to the fully simulated scenes used in the previous 2nd Clarity Enhancement Challenge. Participants are welcome to submit to one or more tasks. We are particularly interested in systems that handle all three cases with little or no redesign/retraining. Further details of the tasks are presented below.  ","version":"Next","tagName":"h2"},{"title":"Task 1: Real ambisonic room impulse responses üî•üî•üî•‚Äã","type":1,"pageTitle":"The 3rd Clarity Enhancement Challenge","url":"/docs/cec3/cec3_intro#task-1-real-ambisonic-room-impulse-responses-firefirefire","content":" In the previous CEC1 and CEC2 challenges, hearing aid input signals were simulated using pre-recorded audio sources mixed with simulated room impulse responses. These simulated responses were then used to make training and evaluation data. In this first task, we are rerunning the CEC2 scenario, but with a new evaluation set that is using real impulse responses measured using an ambisonic microphone array in a real room. We are interested in how well systems that are trained on simulated data can generalise to this new evaluation set.  .css-6ii3fu{font-family:&quot;Roboto&quot;,&quot;Helvetica&quot;,&quot;Arial&quot;,sans-serif;font-weight:500;font-size:0.875rem;line-height:1.75;letter-spacing:0.02857em;text-transform:uppercase;min-width:64px;padding:5px 15px;border-radius:4px;-webkit-transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;border:1px solid rgba(25, 118, 210, 0.5);color:#1976d2;}.css-6ii3fu:hover{-webkit-text-decoration:none;text-decoration:none;background-color:rgba(25, 118, 210, 0.04);border:1px solid #1976d2;}@media (hover: none){.css-6ii3fu:hover{background-color:transparent;}}.css-6ii3fu.Mui-disabled{color:rgba(0, 0, 0, 0.26);border:1px solid rgba(0, 0, 0, 0.12);} .css-79xub{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;box-sizing:border-box;-webkit-tap-highlight-color:transparent;background-color:transparent;outline:0;border:0;margin:0;border-radius:0;padding:0;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;vertical-align:middle;-moz-appearance:none;-webkit-appearance:none;-webkit-text-decoration:none;text-decoration:none;color:inherit;font-family:&quot;Roboto&quot;,&quot;Helvetica&quot;,&quot;Arial&quot;,sans-serif;font-weight:500;font-size:0.875rem;line-height:1.75;letter-spacing:0.02857em;text-transform:uppercase;min-width:64px;padding:5px 15px;border-radius:4px;-webkit-transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;border:1px solid rgba(25, 118, 210, 0.5);color:#1976d2;}.css-79xub::-moz-focus-inner{border-style:none;}.css-79xub.Mui-disabled{pointer-events:none;cursor:default;}@media print{.css-79xub{-webkit-print-color-adjust:exact;color-adjust:exact;}}.css-79xub:hover{-webkit-text-decoration:none;text-decoration:none;background-color:rgba(25, 118, 210, 0.04);border:1px solid #1976d2;}@media (hover: none){.css-79xub:hover{background-color:transparent;}}.css-79xub.Mui-disabled{color:rgba(0, 0, 0, 0.26);border:1px solid rgba(0, 0, 0, 0.12);} Task 1 Details...  ","version":"Next","tagName":"h3"},{"title":"Task 2: Real hearing aid signals üî•üî•üî•‚Äã","type":1,"pageTitle":"The 3rd Clarity Enhancement Challenge","url":"/docs/cec3/cec3_intro#task-2-real-hearing-aid-signals-firefirefire","content":" In all previous Clarity, hearing aid input signals are simulated using room impulse responses and head-related transfer function. In this task, we provide participants with real microphone signals. We have recorded scenes using microphones on a behind-the-ear hearing-aid worn by a real listener attending a target speaker. The scenario closely follows CEC2, i.e. the same noise interferers, etc, but the data is more challenging because it includes real room acoustics, real head movements and real microphone characteristics. A matched training set has been provided and we are interested in how well systems can cope with the inherently more complex data. Ground truth head motion data is also provided and we are interested in whether systems can exploit this information.  .css-6ii3fu{font-family:&quot;Roboto&quot;,&quot;Helvetica&quot;,&quot;Arial&quot;,sans-serif;font-weight:500;font-size:0.875rem;line-height:1.75;letter-spacing:0.02857em;text-transform:uppercase;min-width:64px;padding:5px 15px;border-radius:4px;-webkit-transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;border:1px solid rgba(25, 118, 210, 0.5);color:#1976d2;}.css-6ii3fu:hover{-webkit-text-decoration:none;text-decoration:none;background-color:rgba(25, 118, 210, 0.04);border:1px solid #1976d2;}@media (hover: none){.css-6ii3fu:hover{background-color:transparent;}}.css-6ii3fu.Mui-disabled{color:rgba(0, 0, 0, 0.26);border:1px solid rgba(0, 0, 0, 0.12);} .css-79xub{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;box-sizing:border-box;-webkit-tap-highlight-color:transparent;background-color:transparent;outline:0;border:0;margin:0;border-radius:0;padding:0;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;vertical-align:middle;-moz-appearance:none;-webkit-appearance:none;-webkit-text-decoration:none;text-decoration:none;color:inherit;font-family:&quot;Roboto&quot;,&quot;Helvetica&quot;,&quot;Arial&quot;,sans-serif;font-weight:500;font-size:0.875rem;line-height:1.75;letter-spacing:0.02857em;text-transform:uppercase;min-width:64px;padding:5px 15px;border-radius:4px;-webkit-transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;border:1px solid rgba(25, 118, 210, 0.5);color:#1976d2;}.css-79xub::-moz-focus-inner{border-style:none;}.css-79xub.Mui-disabled{pointer-events:none;cursor:default;}@media print{.css-79xub{-webkit-print-color-adjust:exact;color-adjust:exact;}}.css-79xub:hover{-webkit-text-decoration:none;text-decoration:none;background-color:rgba(25, 118, 210, 0.04);border:1px solid #1976d2;}@media (hover: none){.css-79xub:hover{background-color:transparent;}}.css-79xub.Mui-disabled{color:rgba(0, 0, 0, 0.26);border:1px solid rgba(0, 0, 0, 0.12);} Task 2 Details...  ","version":"Next","tagName":"h3"},{"title":"üîú Task 3: Real dynamic backgrounds (launching 1st May)‚Äã","type":1,"pageTitle":"The 3rd Clarity Enhancement Challenge","url":"/docs/cec3/cec3_intro#-task-3-real-dynamic-backgrounds-launching-1st-may","content":" In all previous Clarity challenges, the interfering signals have been static and carefully controlled. In this task, we will use naturally occurring, dynamic noise backgrounds. We are collecting a dataset of 64-channel ambisonic audio recordings from settings that hearing-impaired listeners find challenging. These include train stations, roadsides and large social gatherings (i.e., the 'cocktail party' scenario). Using these recordings and measured impulse responses, we will create a dataset of hearing aid input signals feature target sentences in dynamic background noise.  .css-6ii3fu{font-family:&quot;Roboto&quot;,&quot;Helvetica&quot;,&quot;Arial&quot;,sans-serif;font-weight:500;font-size:0.875rem;line-height:1.75;letter-spacing:0.02857em;text-transform:uppercase;min-width:64px;padding:5px 15px;border-radius:4px;-webkit-transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;border:1px solid rgba(25, 118, 210, 0.5);color:#1976d2;}.css-6ii3fu:hover{-webkit-text-decoration:none;text-decoration:none;background-color:rgba(25, 118, 210, 0.04);border:1px solid #1976d2;}@media (hover: none){.css-6ii3fu:hover{background-color:transparent;}}.css-6ii3fu.Mui-disabled{color:rgba(0, 0, 0, 0.26);border:1px solid rgba(0, 0, 0, 0.12);} .css-79xub{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;box-sizing:border-box;-webkit-tap-highlight-color:transparent;background-color:transparent;outline:0;border:0;margin:0;border-radius:0;padding:0;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;vertical-align:middle;-moz-appearance:none;-webkit-appearance:none;-webkit-text-decoration:none;text-decoration:none;color:inherit;font-family:&quot;Roboto&quot;,&quot;Helvetica&quot;,&quot;Arial&quot;,sans-serif;font-weight:500;font-size:0.875rem;line-height:1.75;letter-spacing:0.02857em;text-transform:uppercase;min-width:64px;padding:5px 15px;border-radius:4px;-webkit-transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;border:1px solid rgba(25, 118, 210, 0.5);color:#1976d2;}.css-79xub::-moz-focus-inner{border-style:none;}.css-79xub.Mui-disabled{pointer-events:none;cursor:default;}@media print{.css-79xub{-webkit-print-color-adjust:exact;color-adjust:exact;}}.css-79xub:hover{-webkit-text-decoration:none;text-decoration:none;background-color:rgba(25, 118, 210, 0.04);border:1px solid #1976d2;}@media (hover: none){.css-79xub:hover{background-color:transparent;}}.css-79xub.Mui-disabled{color:rgba(0, 0, 0, 0.26);border:1px solid rgba(0, 0, 0, 0.12);} Task 3 Details...  ","version":"Next","tagName":"h3"},{"title":"All tasks‚Äã","type":1,"pageTitle":"The 3rd Clarity Enhancement Challenge","url":"/docs/cec3/cec3_intro#all-tasks","content":" For all tasks, we will be providing standard training, development and evaluation datasets. The training and development datasets will be released at the start of the challenge. The evaluation dataset will be released shortly before the submission deadline without reference signals. Participants will then be asked to submit their processed signals for remote evaluation.  Note, if you are interested in participating, please sign up to our Clarity Challenge‚Äôs Google group so that we can keep you posted on the latest developments. ","version":"Next","tagName":"h3"},{"title":"3rd Clarity Enhancement Challenge Registration","type":0,"sectionRef":"#","url":"/docs/cec3/taking_part/cec3_registration","content":"3rd Clarity Enhancement Challenge Registration Teams are required to register using the form below. Please register as soon as possible. Please submit one form per team, providing a single contact email address. Once you have registered, you will receive an email confirmation with a team ID. When the submission date approaches, you will be sent an individualised link to a Google Drive for submitting materials. Loading‚Ä¶ A single registration form covers all three tasks. You do not need to specify which tasks you are entering at this stage. You can enter one, two or all three tasks. You can also change your mind later.","keywords":"","version":"Next"},{"title":"Prizes","type":0,"sectionRef":"#","url":"/docs/cec3/taking_part/cec1_prizes","content":"","keywords":"","version":"Next"},{"title":"Task 1‚Äã","type":1,"pageTitle":"Prizes","url":"/docs/cec3/taking_part/cec1_prizes#task-1","content":" There is no prize for Task 1.  ","version":"Next","tagName":"h3"},{"title":"Task 2‚Äã","type":1,"pageTitle":"Prizes","url":"/docs/cec3/taking_part/cec1_prizes#task-2","content":" There will be separate HASPI and listening test prizes for the top systems.   HASPI prize 1st Place $1000 2nd Place $500 3rd Place $250 Listening Test prize 1st Place $1000 2nd Place $500 3rd Place $250  ","version":"Next","tagName":"h3"},{"title":"Task 3‚Äã","type":1,"pageTitle":"Prizes","url":"/docs/cec3/taking_part/cec1_prizes#task-3","content":" Prizes to be decided. ","version":"Next","tagName":"h3"},{"title":"CEC3 Schedule","type":0,"sectionRef":"#","url":"/docs/cec3/cec3_dates","content":"CEC3 Schedule 2nd April 2024: Challenge launch with dataset and baseline software (Task 1 and 2) 1st May 2024: Task 3 dataset released 25th July 2024: Evaluation data released 2nd Sept 2024: First round submission deadline for evaluation by objective measure 16th Sept 2024: Second round submission deadline for listening tests Sept-Nov 2024: Listening test evaluation period. Dec 2024: Results announced at a Clarity Challenge Workshop; prizes awarded. The workshop is likely to be a one-day virtual event. Date to be decided.","keywords":"","version":"Next"},{"title":"Find collaborators","type":0,"sectionRef":"#","url":"/docs/cec3/taking_part/cec3_find_a_team","content":"Find collaborators If you'd like to team up with someone else to compete in the challenges, we can help. Please complete this Google form to let us know your own expertise, and what you're looking for in a collaborator. We'll then put people in contact with possible collaborators. We encourage everyone to join the Clarity Challenge‚Äôs Google group to stay updated with project news and announcements. We post in there when we have new people seeking team members (we don't share any personally-identifying details to the group). You are welcome to contact us if you have any questions about forming a team or participating in the challenge: Email the Clarity Team","keywords":"","version":"Next"},{"title":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","type":0,"sectionRef":"#","url":"/docs/cec3/taking_part/icassp2023_faq","content":"","keywords":"","version":"Next"},{"title":"Speech Intelligibility‚Äã","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"/docs/cec3/taking_part/icassp2023_faq#speech-intelligibility","content":" ","version":"Next","tagName":"h2"},{"title":"What is Speech Intelligibility?‚Äã","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"/docs/cec3/taking_part/icassp2023_faq#what-is-speech-intelligibility","content":" The term Speech Intelligibility is generally used in two different ways. It can refer to how much speech is understood by a listener, or to the number of words correctly identified by a listener as a proportion or percentage of the total number of words. In the Clarity project, we are using the latter definition, i.e., the percentage of words in a sentence that a listener identified correctly. This percentage is the target for your prediction models.  Speech intelligibility captures how a listener's ability to participate in conversation is changed when the speech signal is degraded, e.g., by background noise and room reverberation, or is processed, e.g., by a hearing aid. Your prediction model will need to incorporate a model of the hearing abilities of each listener.  ","version":"Next","tagName":"h3"},{"title":"How is Speech Intelligibility measured with listeners?‚Äã","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"/docs/cec3/taking_part/icassp2023_faq#how-is-speech-intelligibility-measured-with-listeners","content":" In the Clarity project, a set of listeners listen to a sentence and then say what words they heard. In this project, speech intelligibility is measured as the number of words identified correctly as a percentage of the total number of words in a sentence.  You might consider looking at other metrics, such as Word Error Rate (WER), which picks up on, e.g., where listeners insert words not in the original sentence. You might do this if you think that an estimate of WER or other metrics would help your system to estimate speech intelligibility, as defined in the Clarity project.  ","version":"Next","tagName":"h3"},{"title":"How is Speech Intelligibility objectively measured by a computer?‚Äã","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"/docs/cec3/taking_part/icassp2023_faq#how-is-speech-intelligibility-objectively-measured-by-a-computer","content":" When fitting a hearing aid, it would be beneficial for an audiologist to be able to use an objective measure of speech intelligibility to determine what signal processing algorithm(s) should be used to compensate for the listener's hearing impairment. Objective measures are also useful when measured speech intelligibility scores are unavailable, such as when developing a machine learning-based hearing aid algorithm or some other speech enhancement method. Another advantage of non-intrusive measures is that they do not require time-alignment of processed and reference signals.  Objective measures - or metrics - of speech intelligibility are used to allow a computer to estimate the likely performance of humans in listening tests. The main goal of entries to the prediction challenge is to produce one of these measures that performs well for listeners with hearing loss. There are two broad classes of speech intelligibility models:  Intrusive metrics (also known as double-ended) are most common. This is where the intelligibility is estimated by comparing the degraded or processed speech signal with the original clean speech signal.Non-intrusive metrics (also known as single-ended or blind) are less well developed. This is where intelligibility is estimated from the degraded or processed speech signal alone.  In the Clarity project, both types of metrics are of interest. Intrusive metrics will be more accurate in many cases. However, there are hearing aid processes where the speech content is shifted in frequency, which will defeat most current intrusive speech intelligibility metrics. We also hypothesise that there might be issues with intrusive metrics and machine learning approaches in hearing aids that revoice the original speech.  ","version":"Next","tagName":"h3"},{"title":"What speech intelligibility models already exist and what are they used for?‚Äã","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"/docs/cec3/taking_part/icassp2023_faq#what-speech-intelligibility-models-already-exist-and-what-are-they-used-for","content":" There aren't many speech intelligibility models that consider hearing impairment, but one that does is HASPI by Kates and Arehart. In this seminar from the first Clarity workshop, James Kates discusses speech intelligibility models with a focus on the ones he has developed. He also discusses the speech quality metric HASQI. If you're interested in using HASPI or HASQI for the challenge, James Kates has kindly made the MATLAB code and user guide available for download.    Click arrow to see synopsis. Signal degradations, such as additive noise and nonlinear distortion, can reduce the intelligibility and quality of a speech signal. Predicting intelligibility and quality for hearing aids is especially difficult since these devices may contain intentional nonlinear distortion designed to make speech more audible to a hearing-impaired listener. This speech processing often takes the form of time-varying multichannel gain adjustments. Intelligibility and quality metrics used for hearing aids and hearing-impaired listeners must therefore consider the trade-offs between audibility and distortion introduced by hearing-aid speech envelope modifications. This presentation uses the Hearing Aid Speech Perception Index (HASPI) and the Hearing Aid Speech Quality Index (HASQI) to predict intelligibility and quality, respectively. These indices incorporate a model of the auditory periphery that can be adjusted to reflect hearing loss. They have been trained on intelligibility scores and quality ratings from both normal-hearing and hearing-impaired listeners for a wide variety of signal and processing conditions. The basics of the metrics are explained, and the metrics are then used to analyse the effects of additive noise on speech, to evaluate noise suppression algorithms, and to measure differences among commercial hearing aids.  ","version":"Next","tagName":"h3"},{"title":"Hearing Loss‚Äã","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"/docs/cec3/taking_part/icassp2023_faq#hearing-loss","content":" There are many types of hearing loss, but the focus of the Clarity project is the hearing loss that happens with ageing. This is a form of sensorineural hearing loss.  ","version":"Next","tagName":"h2"},{"title":"How does hearing loss affect the perception of audio signals, and how do modern hearing aids process sound to help with this?‚Äã","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"/docs/cec3/taking_part/icassp2023_faq#how-does-hearing-loss-affect-the-perception-of-audio-signals-and-how-do-modern-hearing-aids-process-sound-to-help-with-this","content":" In this seminar from the first Clarity workshop, Karolina Smeds from ORCA Europe and WS Audiology discusses the effects of hearing loss and the hearing aid processing strategies that are typically used to counter the sensory deficits.    Click arrow to see synopsis. Hearing loss leads to several unwanted effects. Loss of audibility for soft sounds is one effect, but also when amplification is used to create audibility for soft sounds, many suprathreshold deficits remain. The most common type of hearing loss is a cochlear hearing loss, where haircells or nerve synapses in the cochlea are damaged. Ageing and noise exposure are the most common causes of cochlear hearing loss. This type of hearing loss is associated with atypical loudness perception and difficulties in noisy situations. Background noise masks for instance speech to a higher degree than for a person with healthy hair cells. This explains why listening to speech-in-noise (SPIN) is such an important topic to work on. A brief introduction to signal processing in hearing aids will be presented. With the use of frequency-specific amplification and compression (automatic gain control, AGC), hearing aids are usually doing a good job in compensating for reduced audibility and for atypical suprathreshold loudness perception. However, it is more difficult to compensate for the increased masking effect. Some examples of strategies will be presented. Finally, natural conversations in noise will be discussed. The balance between being able to have a conversation with a specific communication partner in a group of people and being able to switch attention if someone else starts to talk will be touched upon. ","version":"Next","tagName":"h3"},{"title":"Modelling the scenario","type":0,"sectionRef":"#","url":"/docs/cec1/data/cec1_scenario","content":"","keywords":"","version":"Next"},{"title":"Simulating the audio signals received by the hearing aid‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cec1/data/cec1_scenario#simulating-the-audio-signals-received-by-the-hearing-aid","content":" A listener ‚Äì or receiver ‚Äì is sitting or standing in a small room that has low to moderate reverberation. The person is listening to a target talker, who is selected from our set of 40 speakers. The target talker is producing one of our unique 7-10 word Clarity utterances. Simultaneously, an interferer sound is playing; this is either a competing talker or a continuous noise source (e.g., a washing machine). The target and interferer are at the same height as the listener. The room dimensions, boundary materials, and the locations of the listener, target and interferer are randomised (discussed below). An example scenario is shown in Figure 1. The room geometry showing origin location is defined in Figure 2.  Example SceneRoom Geometry Figure 1. Example overview.  Figure 3, below, shows the basic scene generator. The sound at the receiver is generated first by convolving the source signals with Binaural Room Impulse Responses (BRIRs). The reverberated speech and noise signals are then summed after appropriate gains are applied. The gains are set to achieve a Signal-to-Noise Ratio (SNR), which is chosen pseudo-randomly between limits. The BRIRs are generated using the RAVEN Geometric Room Acoustic Model [1].  There are additional signal paths and outputs generated that have been omitted from Figure 3 for clarity. In addition to the reverberated signals associated with the hearing aid microphones, the signal close to the eardrum is also generated. You can also access the reverberated speech and noise signals before they are mixed.    Figure 3. Simplified diagram of the scene generator. RIR refers to Room Impulse Response, HRTFs refers to Head Related Transfer Functions, SNRs are signal-to-noise ratios, and gain calc. indicates gain calculation. Dry here means anechoic. The outputs are noisy speech signals.  ","version":"Next","tagName":"h2"},{"title":"Room Geometry‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cec1/data/cec1_scenario#room-geometry","content":" Cuboid rooms with dimensions length, LLL, by width, WWW, by height, HHH.Length LLL set using a uniform probability distribution random number generator with 3‚â§L(m)‚â§83 \\le L (m) \\le 83‚â§L(m)‚â§8.Height HHH set using a Gaussian distribution random number generator with a mean of 2.7m2.7 m2.7m and standard deviation of 0.8m0.8 m0.8m.Area L√óWL \\times WL√óW set using a Gaussian distribution random number generator with mean 17.7m217.7 m^217.7m2 and standard deviation of 5.5m25.5 m^25.5m2.  ","version":"Next","tagName":"h2"},{"title":"Room Materials‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cec1/data/cec1_scenario#room-materials","content":" One of the walls of the room is randomly selected for the location of the door. The door can be at any position with the constraint of being at least at 20 cm from the corner of the wall.  A window is placed on one of the other three walls. The window could be at any position of the wall but at 1.9 m height and at 0.4 m from any corner. The curtains are simulated to the side of the window. For larger rooms, a second window and curtains are simulated following a similar methodology.  A sofa is simulated at a random position as a layer on the wall and the floor. Finally, a rug is simulated at a random location on the floor.  ","version":"Next","tagName":"h2"},{"title":"The receiver‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cec1/data/cec1_scenario#the-receiver","content":" The listener has position, r‚Éó=(xr,yr,zr)\\vec{r} = (x_r,y_r,z_r)r=(xr‚Äã,yr‚Äã,zr‚Äã)  This is positioned within the room using uniform probability distribution random number generators for the x and y coordinates (see Figure 2 for origin location). There are constraints to ensure that the receiver is not too close to the wall:  ‚àíW/2+1‚â§xr‚â§W/2‚àí1-W/2+1 \\le x_r \\le W/2-1‚àíW/2+1‚â§xr‚Äã‚â§W/2‚àí11‚â§yr‚â§L‚àí11 \\le y_r \\le L-11‚â§yr‚Äã‚â§L‚àí1zrz_rzr‚Äã either 1.2m1.2 m1.2m (sitting) or 1.6m1.6 m1.6m (standing).  The receiver is positioned so as to be roughly facing the target talker. That is to say, within ¬±30\\pm 30¬±30 degrees of target. The angle = 7.5n7.5n7.5n where nnn is an integer and ‚à£n‚à£‚â§4|n| \\le 4‚à£n‚à£‚â§4.  ","version":"Next","tagName":"h2"},{"title":"The target talker‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cec1/data/cec1_scenario#the-target-talker","content":" ‚Äã‚ÄãThe target talker has position t‚Éó=(xt,yt,zt)\\vec{t} = (x_t,y_t,z_t)t=(xt‚Äã,yt‚Äã,zt‚Äã)  The target talker is positioned within the room using uniform probability distribution random number generators for the coordinates. Constraints ensure the target is not too close to the wall or receiver. It is set to have the same height as the receiver.  ‚àíW/2+1‚â§xt‚â§W/2‚àí1-W/2+1 \\le x_t \\le W/2-1‚àíW/2+1‚â§xt‚Äã‚â§W/2‚àí11‚â§yt‚â§L‚àí11 \\le y_t \\le L-11‚â§yt‚Äã‚â§L‚àí1‚à£r‚àít‚à£&gt;1|r-t| &gt; 1‚à£r‚àít‚à£&gt;1zt=zrz_t=z_rzt‚Äã=zr‚Äã  A speech directivity pattern is used, which is directed at the listener.  ","version":"Next","tagName":"h2"},{"title":"The interferer‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cec1/data/cec1_scenario#the-interferer","content":" The interferers have position i‚Éó1,2,3=(xi,yi,zi)\\vec{i}_{1,2,3} = (x_i,y_i,z_i)i1,2,3‚Äã=(xi‚Äã,yi‚Äã,zi‚Äã)  The interferer is a single point source radiating speech or non-speech noise omnidirectionally. It is placed within the room using uniform probability distribution random number generators for the coordinates. These constraints ensure the interferer is not too close to the wall or receiver. It is set to be at the same height as the receiver. Note, this means that the interferer can be at any angle relative to the receiver.  ‚àíW/2+1‚â§xi‚â§W/2‚àí1-W/2+1 \\le x_i \\le W/2-1‚àíW/2+1‚â§xi‚Äã‚â§W/2‚àí11‚â§yi‚â§L‚àí11 \\le y_i \\le L-11‚â§yi‚Äã‚â§L‚àí1‚à£r‚àíi‚à£&gt;1|r-i| \\gt 1‚à£r‚àíi‚à£&gt;1zi=zrz_i = z_rzi‚Äã=zr‚Äã  ","version":"Next","tagName":"h2"},{"title":"Timing‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cec1/data/cec1_scenario#timing","content":" The target sound starts 2 seconds after the start of the interferer. This is so the target is clear and unambiguously identifiable for listening tests. This also gives the hearing aid algorithms some time to adjust to the background noise.The interferer continues 1 second after the target has finished, so that all words in the target utterance can be masked.  ","version":"Next","tagName":"h2"},{"title":"Signal-to-Noise Ratio (SNR)‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cec1/data/cec1_scenario#signal-to-noise-ratio-snr","content":" The mixtures are engineered such that the target utterances are at an appropriate level of intelligibility when processed by the default hearing aid software. This is achieved by scaling the interferer. Pilot tests have been conducted to get this approximately correct. Scaling is done this way because it does not require recomputing the BRIRs. Note that the interferer can be at any azimuth from the point of view of the listener/receiver.  A desired signal-to-noise ratio, SNRD_DD‚Äã (dB), is chosen using a uniform probability distribution random number generator between the limits of ranges specified for the speech and non-speech interferers. The better ear SNR, here termed BE_SNR, which models the better ear effect in binaural listening, is calculated for the reference channel (channel 1, which corresponds to the front microphone of the hearing aid). This value is used to scale all interferer channels. The procedure is described below.  For the reference channel,  The segment of the interferer that overlaps with the target (without padding) , i‚Äò, and the target (without padding), t‚Äò, are extractedSpeech-weighted SNRs are calculated for each ear, SNRL_LL‚Äã and SNRR_RR‚Äã: Signals i‚Äò and t‚Äô are separately convolved with a speech-weighting filter, h (specified below).The rms is calculated for each convolved signal.SNRL_LL‚Äã and SNRR_RR‚Äã are calculated as the ratio of these rms values. The BE_SNR is selected as the maximum of the two SNRs: BE_SNR = max(SNRL_LL‚Äã and SNRR_RR‚Äã).  Then per channel,  The whole interferer signal, i, is scaled by the BE_SNR i=i√ói = i \\timesi=i√ó BE_SNR Finally, i is scaled as follows: i=i√ó10‚àíSNRD/20i = i \\times 10^{-SNR_D/20}i=i√ó10‚àíSNRD‚Äã/20  The speech-weighting filter is an FIR designed using the host window method [2, 3]. The specification is:  Frequency (Hz) = [0, 150, 250, 350, 450, 4000, 4800, 5800, 7000, 8500, 9500, 22050];Magnitude of transfer function at each frequency = [0.0001, 0.0103, 0.0261, 0.0419, 0.0577, 0.0577, 0.046, 0.0343, 0.0226, 0.0110, 0.0001, 0.0001];      Figure 4, Speech weighting filter transfer function graph.  ","version":"Next","tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cec1/data/cec1_scenario#references","content":" Schr√∂der, D. and Vorl√§nder, M., 2011, January. RAVEN: A real-time framework for the auralization of interactive virtual environments. In Proceedings of Forum Acusticum 2011 (pp. 1541-1546). Denmark: Aalborg.Abed, A.H.M. and Cain, G.D., 1978. Low-pass digital filtering with the host windowing design technique. Radio and Electronic Engineer, 48(6), pp.293-300.Abed, A.E. and Cain, G., 1984. The host windowing technique for FIR digital filter design. IEEE transactions on acoustics, speech, and signal processing, 32(4), pp.683-694. ","version":"Next","tagName":"h2"},{"title":"Rules","type":0,"sectionRef":"#","url":"/docs/cec3/task_1/task1_rules","content":"","keywords":"","version":"Next"},{"title":"What information can I use?‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cec3/task_1/task1_rules#what-information-can-i-use","content":" ","version":"Next","tagName":"h2"},{"title":"Training and development‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cec3/task_1/task1_rules#training-and-development","content":" Teams can expand the training data provided using augmentation or by supplementing it with data from other publicly available sources, excluding datasets that may appear in the evaluation (this prohibits training on any previous Clarity challenge evaluation data, speech from Crowdsourced high-quality UK and Ireland English Dialect speech data set, or music from the MTG-Jamendo Dataset). Any additional data used must be made clear in the technical report. Teams can also use publicly available pre-trained models, provided they weren't trained on the prohibited data mentioned in this paragraph.  Any of the CEC3 metadata can be used during training and development, but during evaluation, the system will only have access to the hearing aid input signals and the listener audiograms.  Teams that augment or extend the training dataset must also submit a version of the system using only the standard dataset. etc  ","version":"Next","tagName":"h3"},{"title":"Evaluation‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cec3/task_1/task1_rules#evaluation","content":" The only data that can be used during evaluation are  The 6-channels hearing aid input signalsThe listener characterisation (pure tone air-conduction audiograms and/or digit triple test results).The provided clean audio examples for the target talker (these will not be the same as any of the target utterances.)The head-rotation signal (but if used, a version of the system that does not use it should also be prepared for comparison.)  ","version":"Next","tagName":"h3"},{"title":"Computational restrictions‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cec3/task_1/task1_rules#computational-restrictions","content":" Systems must be causal; the output from the hearing aid at time t must not use any information from input samples more than 5 ms into the future (i.e., no information from input samples &gt;t+5 ms).There is no limit on computational requirements but memory and processing requirements should be clearly stated in the technical reports.  Please see this blog post for further explanation of these last two rules about latency and computation time. ","version":"Next","tagName":"h2"},{"title":"3rd Clarity Enhancement Challenge Rules","type":0,"sectionRef":"#","url":"/docs/cec3/taking_part/cec3_rules","content":"","keywords":"","version":"Next"},{"title":"Teams‚Äã","type":1,"pageTitle":"3rd Clarity Enhancement Challenge Rules","url":"/docs/cec3/taking_part/cec3_rules#teams","content":" Teams must have pre-registered and nominated a contact person.Teams can be from one or more institutions.  ","version":"Next","tagName":"h2"},{"title":"Transparency‚Äã","type":1,"pageTitle":"3rd Clarity Enhancement Challenge Rules","url":"/docs/cec3/taking_part/cec3_rules#transparency","content":" Teams must provide a technical document of up to 2 pages describing the system/model and any external data and pre-existing tools, software and models used.We will publish all technical documents (anonymous or otherwise).Teams are encouraged ‚Äì but not required ‚Äì to provide us with access to the system/model and to make their code open source.  ","version":"Next","tagName":"h2"},{"title":"Intellectual property‚Äã","type":1,"pageTitle":"3rd Clarity Enhancement Challenge Rules","url":"/docs/cec3/taking_part/cec3_rules#intellectual-property","content":" The following terms apply to participation in this machine learning challenge (‚ÄúChallenge‚Äù). Entrants may create original solutions, prototypes, datasets, scripts, or other content, materials, discoveries or inventions (a ‚ÄúSubmission‚Äù). The Challenge is organised by the Challenge Organiser.  Entrants retain ownership of all intellectual and industrial property rights (including moral rights) in and to Submissions.  As a condition of submission, Entrant grants the Challenge Organiser, its subsidiaries, agents and partner companies, a perpetual, irrevocable, worldwide, royalty-free, and non-exclusive licence to use, reproduce, adapt, modify, publish, distribute, publicly perform, create a derivative work from, and publicly display the Submission.  Entrants provide Submissions on an ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE.  ","version":"Next","tagName":"h2"},{"title":"Task-specific rules‚Äã","type":1,"pageTitle":"3rd Clarity Enhancement Challenge Rules","url":"/docs/cec3/taking_part/cec3_rules#task-specific-rules","content":" For task-specific rules regarding system training and design see the Task 1 Rules and Task 2 Rules pages. ","version":"Next","tagName":"h2"},{"title":"Task 1 - Using real room impulse responses","type":0,"sectionRef":"#","url":"/docs/cec3/task_1/cec3_task1_overview","content":"","keywords":"","version":"Next"},{"title":"Motivation‚Äã","type":1,"pageTitle":"Task 1 - Using real room impulse responses","url":"/docs/cec3/task_1/cec3_task1_overview#motivation","content":" In the 2nd Clarity Enhancement Challenge (CEC2), the best systems were able to achieve extremely high levels of performances. We believe this is in part because the training data and evaluation data were fully simulated and produced with the same simulation tools. Evaluating in this way may favour systems that are able to exploit the consistency of the simulation, but which do not generalise well to real-world scenarios. In the Clarity ICASSP 2023 challenge [1], it was seen that systems struggled with a real room evaluation set, even those that did well on the simulated evaluation set. So, in this first task, we are taking a step towards understanding the generalisation problem by constructing an evaluate set using impulse responses that have been recorded in a real room, and which therefore cannot be so easily modelled by simulation.  Figure 1. 6th Order Ambisonic Impulse responses recorded with an MH Acoustics' em64 Eigenmike  ","version":"Next","tagName":"h3"},{"title":"Task Description‚Äã","type":1,"pageTitle":"Task 1 - Using real room impulse responses","url":"/docs/cec3/task_1/cec3_task1_overview#task-description","content":" The scenario is of a hearing aid user listening to a target speaker in a domestic living room while two or three interfering sounds are also active. Participants are provided with signals representing the six input channels of a binaural hearing aid device (three left, three right). You will process the signals so as to produce hearing aid outputs that maximise the intelligibility of the target speech. For each scene that needs to be processed, the audiogram of the hearing aid user will be provided.  Figure 2. The scenario involves one talker, a listener who rotates their head, and at least two sources of unwanted sound.  Participants are asked to train their systems using the training data and training data generation tools provided with CEC2, or with any other data they choose so long as it is openly available. Submissions will be evaluated using the HASPI objective metric. The evaluation data has been produced to have the same inherent degree of difficulty as the previous CEC2 challenge, so that results can be compared across the challenges. We are interested to see whether systems are able to achieve similar levels of performance despite the extra challenge of not having such closely matched training data provided.  In the sections that follow we describe how that data has been collected and the what data has been provided; the rules that all systems need to follow; the evaluation metric and the performance of a baseline system. Near the submission date we will publish a final evaluation set and instructions on how to submit your signals for evaluation.  ","version":"Next","tagName":"h3"},{"title":"References‚Äã","type":1,"pageTitle":"Task 1 - Using real room impulse responses","url":"/docs/cec3/task_1/cec3_task1_overview#references","content":" [1] Cox, T.J., Barker, J., Bailey, W., Graetzer, S., Akeroyd, M.A., Culling, J.F. and Naylor, G., 2023, June. Overview of the 2023 ICASSP SP Clarity Challenge: Speech Enhancement for Hearing Aids. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Institute of Electrical and Electronics Engineers. ","version":"Next","tagName":"h3"},{"title":"Task 1 Submission","type":0,"sectionRef":"#","url":"/docs/cec3/task_1/cec3_task1_submission","content":"Task 1 Submission Detail will appear later in the competition. If you are considering participating in any of the CEC3 tasks, please register early so that we can keep you informed.","keywords":"","version":"Next"},{"title":"CEC2 Data","type":0,"sectionRef":"#","url":"/docs/cec2/data/cec2_data","content":"","keywords":"","version":"Next"},{"title":"A. Training, development, evaluation data‚Äã","type":1,"pageTitle":"CEC2 Data","url":"/docs/cec2/data/cec2_data#a-training-development-evaluation-data","content":" The dataset of 10,000 scenes is split into three:  Training (train).Development (dev).Evaluation (eval).  Uses of the data:  You should not use the development or evaluation data set for training.The system submitted should be chosen on the evidence provided by the development set.The final listening and ranking will be performed with the (held-out) evaluation set.For more information on augmenting and supplementing the training data, please see the rules.The evaluation dataset will be made available one month before the challenge submission deadline.  ","version":"Next","tagName":"h2"},{"title":"B. The scene dataset‚Äã","type":1,"pageTitle":"CEC2 Data","url":"/docs/cec2/data/cec2_data#b-the-scene-dataset","content":" The complete dataset is composed split into the following sets:  Training (6000 scenes, 24 speakers);Development (2500 scenes, 10 speakers);Evaluation (1500 scenes, 6 speakers).  Each scene corresponds to a unique target utterance and unique segment(s) of noise from the interferers. The training, development and evaluation sets are disjoint with respect to the target speakers. The three sets are balanced for the gender of the target talker.  High-Order Ambisonic Impulse Responses (HOA-IRs) and Head-Related Impulse Response (HRIRs) are used to model how the sound is altered as it propagates through the room and interacts with the head. See the page on scene generation for more details.  Time-domain acoustic signals are generated for:  A hearing aid with 3 microphone inputs (front, mid, rear). The hearing aid has a Behind-The-Ear (BTE) form factor; see Figure 1. The distance between microphones is approx. 7.6 mm. The properties of the tube and ear mould are not considered.Close to the eardrum.The anechoic target reference (front microphone).  Figure 1. Front (Fr), Middle (Mid) and Rear microphones on a BTE hearing aid form.  Head Related Impulse Responses (HRIRs) are used to model how sound is altered as it propagates in a free-field and interacts with the head (i.e., no room is included). These are taken from the OlHeadHRTF database with permission. These include HRIRs for human heads and for three types of head-and-torso simulator/mannekin. The eardrum HRIRs (labelled ED) are for a position close to the eardrum of the open ear.  rpf files and ac files are specification files for the geometric room acoustic model that include a complete description of the room, both in terms of geometry and room materials.  ","version":"Next","tagName":"h2"},{"title":"B.1 Training data‚Äã","type":1,"pageTitle":"CEC2 Data","url":"/docs/cec2/data/cec2_data#b1-training-data","content":" For each scene in the training data the following signals and metadata are available:  The target and interferer HOA-IRs (4 pairs: front, mid, rear and eardrum for left and right ears).The mono target and interferer signals (pre-convolution).For each hearing aid microphone (channels 1-3 where channel 1 is front, channel 2 is mid and channel 3 is rear) and a position close to the eardrum (channel 0): The target convolved with the appropriate HOA-IRs and downmixed;The interferers convolved with the appropriate HOA-IRs and downmixed;The sum of the target and interferer convolved with the appropriate HOA-IRs and downmixed; (i.e. the noisy signals that would be received by the hearing aid) The target convolved with the anechoic HOA-IRs and downmixed for channel 1 for each ear (‚Äòtarget_anechoic‚Äô). For use as a reference when computing HASPI scores.Metadata describing the scene: a JSON file containing, e.g., the filenames of the sources, the location of the sources, the viewvector of the target source, the location and viewvector of the receiver, the room dimensions (see specification below), and the room number, which corresponds to the RAVEN BRIR, rpf and ac files.A signal describing the head rotation (i.e. azimuthal angle at each sample)  ","version":"Next","tagName":"h3"},{"title":"B.2 Development data‚Äã","type":1,"pageTitle":"CEC2 Data","url":"/docs/cec2/data/cec2_data#b2-development-data","content":" This is made available to allow you to fully examine the performance of your system. Ground truth data (i.e., the premixed target and interferers are available in the development set)  Development data also contains target speaker adaptation sentences, i.e., four utterances from each of the target speakers. These will also be available in the evaluation data. i.e., systems can use these utterances in conjunction with the known target ID to inform their system of the which speaker in the scene should be attended.  Note, that the data available for the evaluation will be much more limited, e.g. it will not contain premixed ground truth signals or scene metadata, (see Section B.3).  When using the development data for evaluation, your hearing aid enhancement model should only be using the types of data available in the evaluation data set (see below).  ","version":"Next","tagName":"h3"},{"title":"B.3 Evaluation data‚Äã","type":1,"pageTitle":"CEC2 Data","url":"/docs/cec2/data/cec2_data#b3-evaluation-data","content":" The following data will only be available:  Audio: the sum of the target and interferers for each hearing aid microphone.The ID of the listener who will be auditioning the processed scene.The listener characterisation data for these listeners.ID of target talker and a few examples of clean audio that are not the same as the target utterance.The head rotation signal, i.e. as might be recovered from hearing aid motion sensors. (Systems can use this signal but should also be evaluated without using it.)Speaker adaptation sentence - 4 clean utterances for each target speaker.  One challenge will be identifying the target talker from the hearing aid microphone signals. There are two possibilities:  The ID of the target talker is given with examples of clean audio. This would allow an algorithm to learn characteristics of the target talker to then help it identify the voice in the mixture.The azimuth of the target and the starting time of the utterance are both roughly known from the scene generation metadata statistics.  These two approaches mimic what is available to human listeners. They might focus on a known voice or they might use visual cues to know roughly where and when someone is talking.  ","version":"Next","tagName":"h3"},{"title":"C Listener data‚Äã","type":1,"pageTitle":"CEC2 Data","url":"/docs/cec2/data/cec2_data#c-listener-data","content":" We will provide metadata characterising the hearing abilities of the listeners so the audio signals you generate for evaluation can be individualised to the specific listeners who will be hearing them.  The same types of data are available for training, development and evaluation.  A panel of hearing-aided listeners will be recruited for evaluation. They will be experienced bilateral hearing-aid users: they use two hearing aids but the hearing loss may be asymmetrical. The average pure tone air-conduction hearing loss will be between 25 and about 60 dB in the better ear. They will be fluent in British English.  The quantification of the listeners‚Äô hearing is done with:  Left and right pure tone air-conduction audiograms. These measure the threshold at which people can hear a pure-tone sound.Results from the DTT (digit-triplet test, also known as a triple digit test)‚Äã  The audiogram is the standard clinical measurement of hearing ability. It‚Äôs the pure-tone threshold of hearing in each ear, measured in quiet in a sound booth. The procedure is standardized e.g., British Society of Audiology Recommended Procedure. Typically it‚Äôs measured at octave frequencies and important intermediate frequencies.The values of the audiogram defines how much gain the hearing aid needs to apply, with the calculation typically done by one of a group of &quot;prescription rules&quot;, e.g. CAMFIT, NAL-NL2 or DSL .  Note that the scale of an audiogram is in ‚ÄúdB HL‚Äù = ‚ÄúdB Hearing Level‚Äù. This is not dB SPL; instead, it‚Äôs relative to an international standard such that 0-dB is ‚Äúnormal hearing‚Äù at every frequency. For background see Why the Audiogram Is Upside-down | The Hearing Review and The Quest for Audiometric Zero | The Hearing Review  The DTT is an adaptive test of speech-in-noise ability. In each trial a listener hears three spoken digits (e.g. 3-6-1) against a background of noise at a given signal-to-noise-ratio (SNR). The task is to respond on a keypad with those three digits in the order they were presented. If the listener gets all three correct, then the SNR is reduced for the next trial so making it slightly harder. If the listener makes any mistake (i.e., any digit wrong, or the order wrong) then the SNR is increased, so making the next trial slightly easier. The test carries on trial-by-trial. The test asymptotes to the SNR at which the participant is equally likely to get all three correct or not, with a few tens of trials needed to get an acceptable result. DTT tests are now used world-wide to measure hearing as they are easy to make in any local language, to explain to participants and to do, and moreover can be done over the internet or telephone as they measure a relative threshold (signal-to-noise ratio), not an absolute threshold in dB SPL. Listeners are encouraged to set a volume that is comfortable and that does not distort or crackle, but is not too quiet.  This paper is a recent scoping review of the field. The particular version we used is Vlaming et al.'s high-frequency DTT, which uses a high-pass noise as the masker. Ours starts at -14 dB SNR, goes up/down at 2 dB steps per trial, and continues for 40 trials.  In the datafile, an average of the SNR for the last 30 trials is provided (labelled 'threshold'). For reference, the SNRs are supplied for each trial as well. The very first trial is practice and is not scored.  ","version":"Next","tagName":"h2"},{"title":"D Data file formats and naming conventions‚Äã","type":1,"pageTitle":"CEC2 Data","url":"/docs/cec2/data/cec2_data#d-data-file-formats-and-naming-conventions","content":" ","version":"Next","tagName":"h2"},{"title":"D.1 Abbreviations used in filenames‚Äã","type":1,"pageTitle":"CEC2 Data","url":"/docs/cec2/data/cec2_data#d1-abbreviations-used-in-filenames","content":" The following abbreviations are used consistently throughout the filenames and references in the metadata.  R ‚Äì ‚Äúroom‚Äù: e.g., ‚ÄúR02678‚Äù # Room ID linking to RAVEN rpf fileS ‚Äì ‚Äúscene‚Äù: e.g., S00121 # Scene ID for a particular setup in a room I.e., room + choice of target and interferer signalsBNC ‚Äì BNC sentence identifier e.g. BNC_A06_01702CH ‚Äì CH0 ‚Äì eardrum signalCH1 ‚Äì front signal, hearing aid channelCH2 ‚Äì middle signal, hearing aid channelCH3 ‚Äì rear signal, hearing aid channel I/i1 ‚Äì Interferer, i.e., noise or sentence ID for the interferer/maskerT ‚Äì talker who produced the target speech sentencesL ‚Äì listenerE ‚Äì entrant (identifying a team participating in the challenge)t ‚Äì target (used in BRIRs and RAVEN project ‚Äòrpf‚Äô files)  ","version":"Next","tagName":"h3"},{"title":"D.2 General‚Äã","type":1,"pageTitle":"CEC2 Data","url":"/docs/cec2/data/cec2_data#d2-general","content":" Audio and HOA-IRs will be 44.1 kHz 32-bit wav files in either mono or stereo as appropriate.Where stereo signals are provided the two channels represent the left (0) and right (1) signals of the ear or hearing aid microphones.0 dB FS in the audio signals corresponds to 100 dB SPL.Metadata will be stored in JSON or csv format as appropriate with the exception of Room descriptions are stored as RAVEN project ‚Äòrpf‚Äô configuration files and ‚Äòac‚Äô files. (However, key details are reflected in the scene.json files) Signals are saved within the Python code as 32-bit floating point by default.Output signals for the listening tests will be required to be in 16-bit format.  ","version":"Next","tagName":"h3"},{"title":"D.3 Prompt and transcription data‚Äã","type":1,"pageTitle":"CEC2 Data","url":"/docs/cec2/data/cec2_data#d3-prompt-and-transcription-data","content":" The following text is available for the target speech:  Prompts are the text that was given to the talkers to say.‚ÄòDot‚Äô transcriptions contain the text as it was spoken in a form more suitable for scoring tools.These are stored in the master json metadata file.  ","version":"Next","tagName":"h3"},{"title":"D.4 Source audio files‚Äã","type":1,"pageTitle":"CEC2 Data","url":"/docs/cec2/data/cec2_data#d4-source-audio-files","content":" Wav files containing the original source materials. Original target sentence recordings:  &lt;Talker ID&gt;_&lt;BNC sentence identifier&gt;.wav  ","version":"Next","tagName":"h3"},{"title":"D.5 Preprocessed scene signals‚Äã","type":1,"pageTitle":"CEC2 Data","url":"/docs/cec2/data/cec2_data#d5-preprocessed-scene-signals","content":" Audio files storing the signals picked up by the hearing aid microphone that are ready for processing. Separate signals are generated for each hearing aid microphone pair or ‚Äòchannel‚Äô.  &lt;Scene ID&gt;_target_&lt;Channel ID&gt;.wav&lt;Scene ID&gt;_interferer_&lt;Channel ID&gt;.wav&lt;Scene ID&gt;_mixed_&lt;Channel ID&gt;.wav&lt;Scene ID&gt;_target_anechoic.wav - at hearing device front microphone&lt;Scene ID&gt;_hr.wav - head rotation signal  Scene ID ‚Äì S00001 to S10000  S followed by 5 digit integer with 0 pre-padding  Channel ID  CH0 ‚Äì Eardrum signalCH1 ‚Äì Hearing aid front microphoneCH2 ‚Äì Hearing aid middle microphoneCH3 ‚Äì Hearing aid rear microphone  The anechoic signal is the signal that will be used as the reference in the HASPI evaluation.  The head rotation signal indicates the precise azimuthal angle of the head at each sample. It is stored as a floating point wav file with values between -1 and +1 where the range maps linearly from -180 degrees to +180 degrees. Teams are free to use this signal in their hearing aid algorithms, but if you do so we will ask you to also submit a version of your system that does not use it, so that the benefit of known head motion can be measured.  ","version":"Next","tagName":"h3"},{"title":"D.6 Enhanced signals‚Äã","type":1,"pageTitle":"CEC2 Data","url":"/docs/cec2/data/cec2_data#d6-enhanced-signals","content":" The signals that are output by the baseline enhancement (hearing aid) model.  &lt;Scene ID&gt;_&lt;Listener ID&gt;_HA-output.wav #HA output signal (i.e., as submitted by the challenge entrants)  Listener ID ‚Äì ID of the listener panel member, e.g., L001 to L100 for initial ‚Äòpseudo-listeners‚Äô, etc. We are no longer providing the script for post-processing signals in preparation for the listener panel.  ","version":"Next","tagName":"h3"},{"title":"D.7 Room metadata‚Äã","type":1,"pageTitle":"CEC2 Data","url":"/docs/cec2/data/cec2_data#d7-room-metadata","content":" JSON file containing the description of a room. This is the data from which the ambisonic room impulse response are generated. It stores the fixed room, listener, target and interferer geometry but does not specify the dynamic factors (e.g. signals, SNRs, head movements etc) that are needed to fully define a scene.  rooms.json [ { &quot;name&quot;: &quot;R00001&quot;, // ID of room linking to RAVEN rpf and ac files &quot;dimensions&quot;: &quot;6.9933x3x3&quot;, // Room dimensions in metres &quot;target&quot;: { // # target positions (x,y,z) and view vectors (look directions, x,y,z) &quot;position&quot;: [-0.3, 2.4, 1.2], &quot;view_vector&quot;: [0.071, 0.997, 0.0], }, &quot;listener&quot;: { &quot;position&quot;: [-0.1, 5.2, 1.2], &quot;view_vector&quot;: [0.071, 0.997, 0.0], }, &quot;interferers&quot;: [ { &quot;position&quot;: [0.4, 4.0, 1.2], }, { // # etc, up to three interferers } ], }, ... ]   ","version":"Next","tagName":"h3"},{"title":"D.8 Scene metadata‚Äã","type":1,"pageTitle":"CEC2 Data","url":"/docs/cec2/data/cec2_data#d8-scene-metadata","content":" JSON file containing a description of the scene. It is a list of dictionaries with each entry representing a unique scene. A scene can be considered to be a room (see Section D.7) plus the full set of listener, target and interferer details. Note, many scenes can be generated from a single room, i.e. each using different listener, target and interferer settings.  scenes.json [ { &quot;scene&quot;: &quot;S00001&quot;, // the unique scene ID &quot;room&quot;:: &quot;R00001&quot;, // ID of room linking to rooms.json &quot;target&quot;: { &quot;name&quot;: &quot;T005_JYD_04274&quot;, // target speaker code and BNCid &quot;time_start&quot;: 107210, // start time of target in samples &quot;time_end&quot;: 217019 // end time of target in samples }, &quot;listener&quot;: { &quot;rotation&quot;: [ // Defines the head motion - list of time, direction pairs { &quot;sample&quot;: 88200, &quot;angle&quot;: 30 // Azimuth angle in degrees }, { &quot;sample&quot;: 176400, &quot;angle‚Äù: 50 } ], &quot;hrir_filename&quot;: [&quot;VP_N4-ED&quot;, &quot;VP_N4-BTE_fr&quot;, &quot;VP_N4-BTE_mid&quot;, &quot;VP_N4-BTE_rear&quot;] // HRIR filename for each channel to generate }, &quot;interferers&quot;: [ { &quot;position&quot;: 1, // Index of interferer position (See rooms.json) &quot;time_start&quot;: 0, // time of interferer onset in samples &quot;time_end&quot;: 261119, // time of interferer offset in samples &quot;name&quot;: &quot;track_1353255&quot;, // interferer name &quot;type&quot;: &quot;music&quot;, // interferer type: speech, noise or music &quot;offset&quot;: 4076256 // index into interferer file at which to extract sample }, { // etc, up to three interferers } ], &quot;dataset&quot;: &quot;train&quot;, // the dataset to which the scene belongs: train, dev or eval &quot;duration&quot;: 261119, // total duration of scene in samples &quot;SNR&quot;: 6.89 // targe SNR for the scene }, ... ]   There are JSON files containing the scene specifications per dataset, e.g., scenes.train.json.- Note, that the scene ID and room ID might have a one-to-one mapping in the challenge, but are not necessarily the same. Multiple scenes can be made by changing the target and masker choices for a given room. E.g., participants wanting to expand the training data could remix multiple scenes from the same room.  The listener ID is not stored in the scene metadata; this information is stored separately in a scenes_listeners.json file which maps scenes to listeners, ie. telling you which listener (or listeners) will be listening to which scenes in the evaluation (see Section D.9).  Noise interferers are labelled with a type ‚Äúmusic‚Äù, ‚Äúnoise‚Äù or ‚Äúspeech‚Äù and then have a unique name identifying the file.  For speech: &lt;ACCENT_CODE&gt;_&lt;SPEAKER_ID&gt; where ACCENT_CODE is a three letter code identify the accent region and gender of the speaker and SPEAKER_ID is a 5-digit ID specific to an individual speaker. E.g. &quot;mif_02484&quot; is a UK midlands accented female, speaker 02484. The speech comes from Demirshan et al. [1] which provides more details.For noise: CIN_&lt;NOISE_TYPE&gt;_&lt;NOISE_ID&gt; where NOISE_TYPE is one of dishwasher, fan, hairdryer, kettle, microwave, vacuum (vacuum cleaner) or washing (washing machine) and NOISE_ID is a unique 3-digit code for the sample.For music: track_&lt;TRACK_ID&gt; where TRACK_ID is unique 7-digit track identifier taken from the MTG Jamendo database. [2]  Given the type and name, further interferer metadata can be found in the files masker_speech_list.json, masker_noise_list.json and masker_music_list.json which are distributed with the challenge.  ","version":"Next","tagName":"h3"},{"title":"D.9 Listener metadata‚Äã","type":1,"pageTitle":"CEC2 Data","url":"/docs/cec2/data/cec2_data#d9-listener-metadata","content":" Audiogram data is stored in a single JSON file with the following format.  listeners.json { &quot;L0001&quot;: { &quot;name&quot;: &quot;L0001&quot;, &quot;audiogram_cfs&quot;: [250, 500, 1000, 2000, 3000, 4000, 6000, 8000], &quot;audiogram_levels_l&quot;: [10, 10, 20, 30, 40, 55, 55, 60], &quot;audiogram_levels_r&quot;: [ ‚Ä¶ ], }, &quot;L0002&quot;: { ... }, ... }   Additional metadata (e.g. digit triple test results) are stored in a csv file. DETAILS  ","version":"Next","tagName":"h3"},{"title":"D.10 Scene-Listener map‚Äã","type":1,"pageTitle":"CEC2 Data","url":"/docs/cec2/data/cec2_data#d10-scene-listener-map","content":" JSON file named scenes_listeners.json dictates which scenes are to be processed by which listeners.  scenes_listeners.json { &quot;S00001&quot;: [&quot;L0001&quot;, &quot;L0002&quot;, &quot;L0003&quot;], &quot;S00002&quot;: [&quot;L0003&quot;, &quot;L0005&quot;, &quot;L0007&quot;], ... }   ","version":"Next","tagName":"h3"},{"title":"References‚Äã","type":1,"pageTitle":"CEC2 Data","url":"/docs/cec2/data/cec2_data#references","content":" Demirsahin, Isin and Kjartansson, Oddur and Gutkin, Alexander and Rivera, Clara, &quot;Open-source Multi-speaker Corpora of the English Accents in the British Isles&quot;, Proceedings of The 12th Language Resources and Evaluation Conference (LREC), 6532--6541, 2020, Avialable OnlineBogdanov, Dmitry and Won, Minz and Tovstogan, Philip and Porter, Alastair and Serra, Xavier, &quot;The MTG-Jamendo Dataset for Automatic Music Tagging&quot;, In Proc. Machine Learning for Music Discovery Workshop, International Conference on Machine Learning (ICML 2019), 2019, Long Beach, CA, United States&quot;, Available Online ","version":"Next","tagName":"h2"},{"title":"Modelling the scenario","type":0,"sectionRef":"#","url":"/docs/cec2/data/cec2_scenario","content":"","keywords":"","version":"Next"},{"title":"Brief overview of random scenario generation‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cec2/data/cec2_scenario#brief-overview-of-random-scenario-generation","content":" The scenarios are for:  Small rooms that have low to moderate reverberation with randomized dimensions and locations of materials such as carpets.The locations of the listener, target and interferer are randomized.The target talker is selected from our set of 40 speakers.The target talker produces a randomly chosen 7-10 word utterance.There are two or three interferer sounds running throughout the audio. This can be a: stream of competing speech;continuous domestic noise source (e.g., a washing machine); ormusic source. The target speech source will onset about one second into the scene.The listener starts not looking at the target talker, but around the time the target speech starts, the listener rotates their head to approximately face towards the target.  An example scenario is shown in Figure 1. It also defines the coordinate system and origin for the room generation.  Figure 1, An example scenario with two noise interferers.  ","version":"Next","tagName":"h2"},{"title":"Room geometry‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cec2/data/cec2_scenario#room-geometry","content":" Cuboid rooms with dimensions length LLL by width WWW by height HHH.Length LLL set using a uniform probability distribution random number generator with 3&lt;L(m)‚â§83 &lt; L(m) \\le 83&lt;L(m)‚â§8.Height HHH set using a Gaussian distribution random number generator with a mean of 2.7 m and standard deviation of 0.8 m.Area L√óWL√óWL√óW set using a Gaussian distribution random number generator with mean 17.7 m2^22 and standard deviation of 5.5 m2^22  ","version":"Next","tagName":"h2"},{"title":"Room materials‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cec2/data/cec2_scenario#room-materials","content":" One of the walls of the room is randomly selected for the location of the door. The door can be at any position with the constraint of being at least 20 cm from the corner of the wall.  A window is placed on one of the other three walls. The window could be at any position of the wall but at 1.9 m height and at 0.4 m from any corner. The curtains are simulated to the side of the window. For larger rooms, a second window and curtains are simulated following a similar methodology.  A sofa is simulated at a random position as a layer on the wall and the floor. Finally, a rug is simulated at a random location on the floor.  ","version":"Next","tagName":"h2"},{"title":"The listener (receiver)‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cec2/data/cec2_scenario#the-listener-receiver","content":" The listener has position, r‚Éó=(xr,yr,zr)\\vec{r} = (x_r,y_r,z_r)r=(xr‚Äã,yr‚Äã,zr‚Äã)  This is positioned within the room using uniform probability distribution random number generators for the x and y coordinates (see Figure 2 for origin location). There are constraints to ensure that the receiver is not too close to the wall:  ‚àíW/2+1‚â§xr‚â§W/2‚àí1-W/2+1 \\le x_r \\le W/2-1‚àíW/2+1‚â§xr‚Äã‚â§W/2‚àí11‚â§yr‚â§L‚àí11 \\le y_r \\le L-11‚â§yr‚Äã‚â§L‚àí1zrz_rzr‚Äã either 1.2 m (sitting) or 1.6 m (standing).  ","version":"Next","tagName":"h2"},{"title":"Head rotation‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cec2/data/cec2_scenario#head-rotation","content":" The listener is initially oriented away from the target and will turn to be roughly facing the target talker around the time when the target speech starts  Orientation of listener at start of the sample ~25¬∞ from facing the target (standard deviation = 5¬∞), limited to +-2 standard deviations.Start of rotation is between -0.635 s to 0.865s (rectangular probability)The rotation lasts for 200 ms (standard deviation =10 ms)Orientation after rotation is 0-10¬∞ (random with rectangular probability distribution).  ","version":"Next","tagName":"h2"},{"title":"The target talker‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cec2/data/cec2_scenario#the-target-talker","content":" ‚Äã‚ÄãThe target talker has position t‚Éó=(xt,yt,zt)\\vec{t} = (x_t,y_t,z_t)t=(xt‚Äã,yt‚Äã,zt‚Äã)  The target talker is positioned within the room using uniform probability distribution random number generators for the coordinates. Constraints ensure the target is not too close to the wall or receiver. It is set to have the same height as the receiver.  ‚àíW/2+1‚â§xt‚â§W/2‚àí1-W/2+1 \\le x_t \\le W/2-1‚àíW/2+1‚â§xt‚Äã‚â§W/2‚àí11‚â§yt‚â§L‚àí11 \\le y_t \\le L-11‚â§yt‚Äã‚â§L‚àí1‚à£r‚àít‚à£&gt;1|r-t| &gt; 1‚à£r‚àít‚à£&gt;1zt=zrz_t=z_rzt‚Äã=zr‚Äã  A speech directivity pattern is used, which is directed at the listener. The target speech starts between 1.0 and 1.5 seconds into the mixed sound files (rectangular probability distribution).  ","version":"Next","tagName":"h2"},{"title":"The interferers‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cec2/data/cec2_scenario#the-interferers","content":" The interferers have position i‚Éó1,2,3=(xi,yi,zi)\\vec{i}_{1,2,3} = (x_i,y_i,z_i)i1,2,3‚Äã=(xi‚Äã,yi‚Äã,zi‚Äã)  Each interferer is modelled as an omnidirectional point source. They will be radiating: speech, noise or music. They are placed within the room using uniform probability distribution random number generators for the coordinates. The following constraints ensure the interferer is not too close to the wall or listener. However, interferers are independently positioned with no constraint on their position relative to each other. They are set to be at the same height as the listener. Note, this means that the interferers can be at any angle relative to the listener.  ‚àíW/2+1‚â§xi‚â§W/2‚àí1-W/2+1 \\le x_i \\le W/2-1‚àíW/2+1‚â§xi‚Äã‚â§W/2‚àí11‚â§yi‚â§L‚àí11 \\le y_i \\le L-11‚â§yi‚Äã‚â§L‚àí1‚à£r‚àíi‚à£&gt;1|r-i| \\gt 1‚à£r‚àíi‚à£&gt;1zi=zrz_i = z_rzi‚Äã=zr‚Äã  The interferers are present over the whole mixed sound file.  ","version":"Next","tagName":"h2"},{"title":"Signal-to-noise ratio (SNR)‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cec2/data/cec2_scenario#signal-to-noise-ratio-snr","content":" The SNR of the mixtures are engineered to achieve a suitable range of speech intelligibility values. A desired signal-to-noise ratio, SNRD_DD‚Äã (dB), is chosen at random. This is generated with a uniform probability distribution between limits determined by pilot listening tests. The better ear SNR (BE_SNR) models the better ear effect in binaural listening. It is calculated for the reference channel (channel 1, which corresponds to the front microphone of the hearing aid). This value is used to scale all interferer channels. The procedure is described below.  For the reference channel,  The segment of the summed interferers that overlaps with the target (without padding), i‚Ä≤i'i‚Ä≤, and the target (without padding), t‚Ä≤t't‚Ä≤, are extractedSpeech-weighted SNRs are calculated for each ear, SNRL_LL‚Äã and SNRR_RR‚Äã: Signals i‚Ä≤i'i‚Ä≤ and t‚Ä≤t't‚Ä≤ are separately convolved with a speech-weighting filter, h (specified below).The rms is calculated for each convolved signal.SNRL_LL‚Äã and SNRR_RR‚Äã are calculated as the ratio of these rms values. The BE_SNR is selected as the maximum of the two SNRs: BE_SNR = max(SNRL_LL‚Äã and SNRR_RR‚Äã).  Then per channel,  The summed interferer signal, i, is scaled by the BE_SNR i=i√ói = i \\timesi=i√ó BE_SNR Finally, i is scaled as follows: i=i√ó10‚àíSNRD/20i = i \\times 10^{-SNR_D/20}i=i√ó10‚àíSNRD‚Äã/20  The speech-weighting filter is an FIR designed using the host window method [2, 3]. The frequency response is shown in Figure 2. The specification is:  Frequency (Hz) = [0, 150, 250, 350, 450, 4000, 4800, 5800, 7000, 8500, 9500, 22050]Magnitude of transfer function at each frequency = [0.0001, 0.0103, 0.0261, 0.0419, 0.0577, 0.0577, 0.046, 0.0343, 0.0226, 0.0110, 0.0001, 0.0001]  Figure 2, Speech weighting filter transfer function graph.  ","version":"Next","tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cec2/data/cec2_scenario#references","content":"   Schr√∂der, D. and Vorl√§nder, M., 2011, January. RAVEN: A real-time framework for the auralization of interactive virtual environments. In Proceedings of Forum Acusticum 2011 (pp. 1541-1546). Denmark: Aalborg.Abed, A.H.M. and Cain, G.D., 1978. Low-pass digital filtering with the host windowing design technique. Radio and Electronic Engineer, 48(6), pp.293-300.Abed, A.E. and Cain, G., 1984. The host windowing technique for FIR digital filter design. IEEE transactions on acoustics, speech, and signal processing, 32(4), pp.683-694. ","version":"Next","tagName":"h2"},{"title":"Task 1 Baseline","type":0,"sectionRef":"#","url":"/docs/cec3/task_1/cec3_task1_baseline","content":"","keywords":"","version":"Next"},{"title":"Enhancement‚Äã","type":1,"pageTitle":"Task 1 Baseline","url":"/docs/cec3/task_1/cec3_task1_baseline#enhancement","content":" The enhance.py script performs the baseline enhancement. The baseline simply takes the 6-channel hearing aid inputs and reduces this to a stereo hearing aid output by passing through the 'front' microphone signal of the left and right ear.  The stereo pair is then passed through a hearing aid amplification stage using a NAL-R [1] fitting amplification and a simple automatic gain compressor. The amplification is determined by the audiograms defined by the scene-listener pairs in clarity_data/metadata/scenes_listeners.dev.json for the development set. After amplification, the evaluate function calculates the better-ear HASPI [2].  ","version":"Next","tagName":"h3"},{"title":"Evaluation‚Äã","type":1,"pageTitle":"Task 1 Baseline","url":"/docs/cec3/task_1/cec3_task1_baseline#evaluation","content":" Once the enhancement has been run, the evaluate.py script can compute the HASPI scores for the signals stored in the amplified_signals folder. The script will read the scene-listener pairs from the development set and calculate the HASPI score for each pair. The final score is the mean HASPI score across all pairs.  Once the enhancement has been run, the evaluate.py script can compute the HASPI scores for the signals stored in the amplified_signals folder. The script will read the scene-listener pairs from the development set and calculate the HASPI score for each pair. The final score is the mean HASPI score across all pairs.  The results are stored in a CSV file which is then read by the final report_scores.py script which will generate a report. This two step process means that it is easy to run evaluate on multiple processors on subsets of the full evaluation set: each processes will produce a separate CSV and report_score.py will collate the results, check their integrity and generate the final report.  The scripts have been designed to run with minimal configuration, but with flexible options for performing partial runs, parallel processing, or running on a cluster. For full documentation and examples of how to run the scripts see the README.md file in the CEC3 baseline recipe of the PyClarity toolkit on GitHub.  For examples of how to run the script see the README.md file in the CEC3 baseline recipe of the PyClarity toolkit on GitHub.  ","version":"Next","tagName":"h3"},{"title":"Baseline performance for Task 1‚Äã","type":1,"pageTitle":"Task 1 Baseline","url":"/docs/cec3/task_1/cec3_task1_baseline#baseline-performance-for-task-1","content":" Running all three scripts on Task 2 will lead to the following output:  Evaluation set size: 7500 Mean HASPI score: 0.22178678134846783 SNR haspi SNR (-12, -9] -10.498088 0.052545 (-9, -6] -7.541468 0.080589 (-6, -3] -4.477046 0.143096 (-3, 0] -1.432494 0.239527 (0, 3] 1.470118 0.352110 (3, 6] 4.492380 0.477001   The mean HASPI score (0.186) is the metric that will be used for ranking. The table shows the mean HASPI score for each SNR range to help you understand the performance of your system. ","version":"Next","tagName":"h3"},{"title":"Task 2 Submission","type":0,"sectionRef":"#","url":"/docs/cec3/task_2/cec3_task2_submission","content":"Task 2 Submission Detail will appear later in the competition. If you are considering participating in any of the CEC3 tasks, please register early so that we can keep you informed.","keywords":"","version":"Next"},{"title":"Task 3 Baseline","type":0,"sectionRef":"#","url":"/docs/cec3/task_3/cec3_task3_baseline","content":"Task 3 Baseline SITE UNDER CONSTRUCTION The 3rd task will be available from 1st May. When the data has been released full details will appear on this site and an announcement will be made on the Clarity Challenge Google group. If you are not already a member, please sign up here.","keywords":"","version":"Next"},{"title":"Task 2 - Using real hearing aid recordings","type":0,"sectionRef":"#","url":"/docs/cec3/task_2/cec3_task2_overview","content":"","keywords":"","version":"Next"},{"title":"Motivation‚Äã","type":1,"pageTitle":"Task 2 - Using real hearing aid recordings","url":"/docs/cec3/task_2/cec3_task2_overview#motivation","content":" Previous Clarity challenges have used fully simulated signals to train and evaluate systems. Not only does this make the task easier because the training and evaluation data are closely matched, it has also led to data that missing many of the complexities of real-world signals. In this task, we are moving closer to real world conditions by using signals that have been live-recorded over real microphones of a hearing aid worn by a participant listening to Clarity scenes reproduced over loudspeakers. This captures aspects of the problem that were previously neglected including complex head motions, real room acoustics, and real microphone characteristics. We are interested in how well systems can cope with this more challenging data.  Figure 1. Data is recorded over real microphones (left) and is accompanied by accurate head motion tracked using reflective markers (right)  ","version":"Next","tagName":"h3"},{"title":"Task Description‚Äã","type":1,"pageTitle":"Task 2 - Using real hearing aid recordings","url":"/docs/cec3/task_2/cec3_task2_overview#task-description","content":" The scenario is a hearing aid user listening to a target speaker in a domestic living room while two or three interfering sounds are also active. Participants will be provided with signals recorded over left and right hearing aid shells each with three microphones. These devices were worn by a participant who was seated in front of loudspeakers that were reproducing scenes similar to those used in previous Clarity challenges. Loudspeakers were moved between recording sessions in order to produce a large set of spatial configurations.  Figure 2. A domestic scenario for Task 2: one talker, a listener who rotates their head, and at least two sources of unwanted sound.  The task design has been modelled closely on CEC2, with a 6,000 scene training set recorded in the same way as the development and evaluation sets. Participants are asked to training systems using this data. They may also augment the training data using the simulation tools and simulated data from previous challenge rounds, or using any publicly available resources. We are also providing ground truth head motion data that can be used to improve system performance. We are interested in how well systems can exploit this information.  Evaluation sets will contain data that are closely matched to the training sets that will be used for ranking systems. However, we will also publish some additional ‚Äòsurprise‚Äô evaluation sets that are deliberately mismatched in a number of ways (listener head size, room T60 time, source location distribution). These will be used to study how well systems generalise outside of the training domain. Evaluation will initially be performed using the HASPI objective metric, but this will be followed by a round of subjective listening tests with hearing-impaired listeners.  In the sections that follow we provide a detailed description of the challenge data; the rules that all systems need to follow; the evaluation metric and the performance of a baseline system. Near the submission date we will publish a final evaluation set and instructions on how to submit your signals for evaluation. ","version":"Next","tagName":"h3"},{"title":"Task 2 Rules","type":0,"sectionRef":"#","url":"/docs/cec3/task_2/cec3_task2_rules","content":"","keywords":"","version":"Next"},{"title":"What information can I use?‚Äã","type":1,"pageTitle":"Task 2 Rules","url":"/docs/cec3/task_2/cec3_task2_rules#what-information-can-i-use","content":" ","version":"Next","tagName":"h2"},{"title":"Training and development‚Äã","type":1,"pageTitle":"Task 2 Rules","url":"/docs/cec3/task_2/cec3_task2_rules#training-and-development","content":" Teams can expand the training data provided using augmentation or by supplementing it with data from other publicly available sources, excluding datasets that may appear in the evaluation (this prohibits training on any previous Clarity challenge evaluation data, speech from Crowdsourced high-quality UK and Ireland English Dialect speech data set, or music from the MTG-Jamendo Dataset). Any additional data used must be made clear in the technical report. Teams can also use publicly available pre-trained models, provided they weren't trained on the prohibited data mentioned in this paragraph.  Any of the CEC3 metadata can be used during training and development, but during evaluation, the system will only have access to the hearing aid input signals and the listener audiograms.  Teams that augment or extend the training dataset must also submit a version of the system using only the standard dataset. etc  ","version":"Next","tagName":"h3"},{"title":"Evaluation‚Äã","type":1,"pageTitle":"Task 2 Rules","url":"/docs/cec3/task_2/cec3_task2_rules#evaluation","content":" The only data that can be used during evaluation are  The 6-channels hearing aid input signalsThe listener characterisation (pure tone air-conduction audiograms and/or digit triple test results).The provided clean audio examples for the target talker (these will not be the same as any of the target utterances.)The head-rotation signal (but if used, a version of the system that does not use it should also be prepared for comparison.)  ","version":"Next","tagName":"h3"},{"title":"Computational restrictions‚Äã","type":1,"pageTitle":"Task 2 Rules","url":"/docs/cec3/task_2/cec3_task2_rules#computational-restrictions","content":" Systems must be causal; the output from the hearing aid at time t must not use any information from input samples more than 5 ms into the future (i.e., no information from input samples &gt;t+5 ms).There is no limit on computational requirements but memory and processing requirements should be clearly stated in the technical reports.  Please see this blog post for further explanation of these last two rules about latency and computation time.  ","version":"Next","tagName":"h2"},{"title":"Objective vs Subjective evaluation‚Äã","type":1,"pageTitle":"Task 2 Rules","url":"/docs/cec3/task_2/cec3_task2_rules#objective-vs-subjective-evaluation","content":" You can submit two versions of your entry, where one is optimised for HASPI and the other for listening tests if you wish. In this case:  Both systems must be submitted for HASPI evaluation.Both versions must be described in the same technical report and differences between how they have been tuned must be made clear. to appear. ","version":"Next","tagName":"h2"},{"title":"Task 2 Baseline","type":0,"sectionRef":"#","url":"/docs/cec3/task_2/cec3_task2_baseline","content":"","keywords":"","version":"Next"},{"title":"Enhancement‚Äã","type":1,"pageTitle":"Task 2 Baseline","url":"/docs/cec3/task_2/cec3_task2_baseline#enhancement","content":" The enhance.py script performs the baseline enhancement. The baseline simply takes the 6-channel hearing aid inputs and reduces this to a stereo hearing aid output by passing through the 'front' microphone signal of the left and right ear.  The stereo pair is then passed through a hearing aid amplification stage using a NAL-R [1] fitting amplification and a simple automatic gain compressor. The amplification is determined by the audiograms defined by the scene-listener pairs in clarity_data/metadata/scenes_listeners.dev.json for the development set. After amplification, the evaluate function calculates the better-ear HASPI [2].  ","version":"Next","tagName":"h3"},{"title":"Evaluation‚Äã","type":1,"pageTitle":"Task 2 Baseline","url":"/docs/cec3/task_2/cec3_task2_baseline#evaluation","content":" Once the enhancement has been run, the evaluate.py script can compute the HASPI scores for the signals stored in the amplified_signals folder. The script will read the scene-listener pairs from the development set and calculate the HASPI score for each pair. The final score is the mean HASPI score across all pairs.  The results are stored in a CSV file which is then read by the final report_scores.py script which will generate a report. This two step process means that it is easy to run evaluate on multiple processors on subsets of the full evaluation set: each processes will produce a separate CSV and report_score.py will collate the results, check their integrity and generate the final report.  The scripts have been designed to run with minimal configuration, but with flexible options for performing partial runs, parallel processing, or running on a cluster. For full documentation and examples of how to run the scripts see the README.md file in the CEC3 baseline recipe of the PyClarity toolkit on GitHub.  ","version":"Next","tagName":"h3"},{"title":"Baseline performance for Task 2‚Äã","type":1,"pageTitle":"Task 2 Baseline","url":"/docs/cec3/task_2/cec3_task2_baseline#baseline-performance-for-task-2","content":" Running all three scripts on Task 2 will lead to the following output:  Evaluation set size: 7500 Mean HASPI score: 0.18643217215546573 SNR haspi SNR (-12, -9] -10.545927 0.034330 (-9, -6] -7.552687 0.055647 (-6, -3] -4.538335 0.096237 (-3, 0] -1.455963 0.178413 (0, 3] 1.434074 0.296364 (3, 6] 4.507484 0.432177   The mean HASPI score (0.186) is the metric that will be used for ranking. The table shows the mean HASPI score for each SNR range to help you understand the performance of your system. ","version":"Next","tagName":"h3"},{"title":"Task 3 Data","type":0,"sectionRef":"#","url":"/docs/cec3/task_3/cec3_task3_data","content":"Task 3 Data SITE UNDER CONSTRUCTION The 3rd task will be available from 1st May. When the data has been released full details will appear on this site and an announcement will be made on the Clarity Challenge Google group. If you are not already a member, please sign up here.","keywords":"","version":"Next"},{"title":"Download","type":0,"sectionRef":"#","url":"/docs/cpc1/cpc1_download","content":"Download The following challenge data are available for download: The challenge training data is available for download as a single 13 GB file, clarity_CPC1_data.v1_1.tgzThe evaluation data in now available (1st March) for download as a single 6 GB file, clarity_CPC1_data.test.v1.tgz. The evaluation data should be untarred into the same root as the training data. The Github repository containing the baseline code is here. The repository contains code for CPC1 and also for the earlier enhancement challenge CEC1. You will find all the necessary instructions for installing the data and setting up the baseline system: i.e. running the MSBG hearing loss model and MBSTOI intelligibility prediction stage. We will be making a further small release in early December to specify the final evaluation metrics that we will be using to rank entries. info The Challenge is now closed but the data is still available for anyone to use. If using the data please cite the following paper Jon Barker and Michael Akeroyd and Trevor J. Cox and John F. Culling and Jennifer Firth and Simone Graetzer and Holly Griffiths and Lara Harris and Graham Naylor and Zuzanna Podwinska and Eszter Porter and Rhoddy Viveros Munoz, ‚ÄúThe 1st Clarity Prediction Challenge: A machine learning challenge for hearing aid intelligibility prediction,‚Äù in Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2022, Incheon, South Korea, 2022.","keywords":"","version":"Next"},{"title":"Task 3 Submission","type":0,"sectionRef":"#","url":"/docs/cec3/task_3/cec3_task3_submission","content":"Task 3 Submission Detail will appear later in the competition. If you are considering participating in any of the CEC3 tasks, please register early so that we can keep you informed.","keywords":"","version":"Next"},{"title":"Contact Us","type":0,"sectionRef":"#","url":"/docs/contact","content":"","keywords":"","version":"Next"},{"title":"Send us an email‚Äã","type":1,"pageTitle":"Contact Us","url":"/docs/contact#send-us-an-email","content":" You can contact the Clarity Team by email at claritychallengecontact@gmail.com  ","version":"Next","tagName":"h2"},{"title":"Join the Google group‚Äã","type":1,"pageTitle":"Contact Us","url":"/docs/contact#join-the-google-group","content":" If you wish to stay updated with Clarity Challenges please sign up the Clarity Challenge‚Äôs Google group ","version":"Next","tagName":"h2"},{"title":"Important Dates","type":0,"sectionRef":"#","url":"/docs/cpc1/cpc1_dates","content":"Important Dates 16th November 2021: Launch of challenge, release of data.23rd November 2021: Webinar to introduce the challenge 15:00-17:00 UK time.1st March 2022: Release of evaluation data.21st March 2022: Submission deadline. All entrants submit their predictions plus a draft of their technical report. Scores will be returned with 24 hours of submission.28th March 2022: Deadline for Interspeech paper submission.25th April 2022: Deadline by which all entrants must submit two page technical reports to Clarity Prediction Challenge 2022 workshop.29th June 2022: Clarity Prediction Challenge 2022 workshop.Sept 18-22, 2022: Interspeech 2022 Special Session.","keywords":"","version":"Next"},{"title":"Task 3 - Using real noise backgrounds","type":0,"sectionRef":"#","url":"/docs/cec3/task_3/cec3_task3_overview","content":"Task 3 - Using real noise backgrounds SITE UNDER CONSTRUCTION The 3rd task will be available from 1st May. When the data has been released full details will appear on this site and an announcement will be made on the Clarity Challenge Google group. If you are not already a member, please sign up here. The task will be featuring real audio backgrounds that have been recorded with an mh acoustics em64 Eigenmike at a number of locations including roadsides, a train station platform and a social gathering. Figure 1. Newport train station - One of the recording locations that will feature in Task 3","keywords":"","version":"Next"},{"title":"The 1st Clarity Prediction Challenge","type":0,"sectionRef":"#","url":"/docs/cpc1/cpc1_intro","content":"","keywords":"","version":"Next"},{"title":"Key dates (updated 14/01/22)‚Äã","type":1,"pageTitle":"The 1st Clarity Prediction Challenge","url":"/docs/cpc1/cpc1_intro#key-dates-updated-140122","content":" 16th November 2021: Launch of challenge, release of data.23rd November 2021: Webinar to introduce the challenge 15:00-17:00 UK time.1st March 2022: Release of evaluation data.21st March 2022: Submission deadline. All entrants submit their predictions plus a draft of their technical report (details below). Scores will be returned with 24 hours of submission.28th March 2022: Deadline for Interspeech paper submission.25th April 2022: Deadline by which all entrants must submit two page technical reports to Clarity Prediction Challenge 2022 workshop.29th June 2022: Clarity Prediction Challenge 2022 workshop.Sept 18-22, 2022: Interspeech 2022 Special Session.  ","version":"Next","tagName":"h2"},{"title":"More details‚Äã","type":1,"pageTitle":"The 1st Clarity Prediction Challenge","url":"/docs/cpc1/cpc1_intro#more-details","content":" Scenario - a description of the listening scenario and how it has been simulated. Baseline System - a description of the baseline software model. Data - the data that can be used to train and evaluate your system during development. Software - the software tools that we are providing to help you build and evaluate a challenge entry. Challenge Rules - the rules to which all challenge entries must adhere. Submission - information about how to prepare your submission. Prizes - information about our prizes. Download - where to go to download the software and challenge data. Find a team - if you'd like to find collaborators to help you compete. FAQ - an extensive FAQ answering key questions and providing background knowledge to help you compete. ","version":"Next","tagName":"h2"},{"title":"Baseline System","type":0,"sectionRef":"#","url":"/docs/cpc1/software/cpc1_baseline","content":"Baseline System Figure 1 is a simplified schematic of the baseline system, where not all signal paths are shown. A scene generator (blue box) creates the speech in noise (SPIN) that the hearing aid model then enhances (yellow box). This enhancement is individualised for each listener; hence, there is also a system to select a random listener (white ellipse) with a particular set of characteristics (e.g., audiograms). The SPIN that has been improved by the hearing aid is then passed to the prediction stage (orange box). This comprises two models: a hearing loss model, anda binaural speech intelligibility model. This prediction stage (orange box) is what we want you to improve on in this challenge. Figure 1 Simplified overview of the baseline. You are free to choose which parts of the baseline you use and reconfigure the system as you see fit. You can use our hearing loss model as part of your entry, or produce a single model that combines the hearing loss and speech intelligibility models. For an introduction to elements of the prediction model, please see our FAQ, which includes an overview of Speech intelligibility, andHearing loss and what hearing aids do. For the prediction challenge, most examples of the improved SPIN shown in the centre of the diagram come from hearing aid models created by the entrants to the first Enhancement Challenge. Therefore, most audio signals in the prediction challenge data were not processed by the baseline hearing aid model. More details of the different parts of the baseline appear on the software page. See the following sections: Scene GeneratorHearing aid modelHearing loss modelSpeech intelligibility model Download baseline software and data.","keywords":"","version":"Next"},{"title":"Results","type":0,"sectionRef":"#","url":"/docs/cpc1/cpc1_results","content":"","keywords":"","version":"Next"},{"title":"Prizes‚Äã","type":1,"pageTitle":"Results","url":"/docs/cpc1/cpc1_results#prizes","content":" The Hearing Industry Research Consortium prizes for best system were awarded as follows:  1st place: System E33, Zezario et al, MBI-Net: A Non-Intrusive Multi-Branched Speech Intelligibility Prediction Model for Hearing Aids for best performance for a non-intrusive system, 2nd place: System E30, Huckvale et al, ELO-SPHERES Intelligibility Prediction Model for the Clarity Prediction Challenge 2022 for best performance of an intrusive system, 3rd place: System E019, R√∂ttges et al., Speech Intelligibility Prediction for Hearing-Impaired Listeners with the bBSIM-STI Model for best performance in the open-set track   The Hearing Industry Research Consortium prizes for best student contribution were awarded as follows:  1st place: Ryandhimas Zezario from the National Taiwan University, 2nd place: Franklin Yohan Alvarez Cardinale from Medizinische Hochschule Hannover, Germany 3rd place: Jana Rossbach from Carl von Ossietzky University, Oldenburg, Germany.  Congratulations to the winners!  (Note, systems E29 and E32 had links to the organizers and so were not eligible for prizes). ","version":"Next","tagName":"h2"},{"title":"Task 3 Rules","type":0,"sectionRef":"#","url":"/docs/cec3/task_3/cec3_task3_rules","content":"Task 3 Rules SITE UNDER CONSTRUCTION The 3rd task will be available from 1st May. When the data has been released full details will appear on this site and an announcement will be made on the Clarity Challenge Google group. If you are not already a member, please sign up here.","keywords":"","version":"Next"},{"title":"Task 2 Data","type":0,"sectionRef":"#","url":"/docs/cec3/task_2/cec3_task2_data","content":"","keywords":"","version":"Next"},{"title":"Recording setup‚Äã","type":1,"pageTitle":"Task 2 Data","url":"/docs/cec3/task_2/cec3_task2_data#recording-setup","content":" The scenes were recorded over the period of two weeks in a recording room at the University of Sheffield. The target and interferer signals were played using a set of 13 loudspeakers arranged around a listener. The listener was wearing hearing aid shells with three microphones (front, middle, back) and a pair of glasses with reflective markers that were tracked with a Vicon motion capture system. Nine loudspeakers were placed in front of the listener arranged at semi-randomly selected locations on a polar grid (the grid has distances of 2 m to 4 m spaced at 0.5 m and angles from -60 degrees to 60 degrees spaced at 7.5 degrees). The remaining four loudspeakers were placed in a line behind the listener. For each scene, one of the front loudspeakers was chosen to play the target utterances, and two or three of the 12 remaining speakers (front and back) were chosen to play the interferers.  Recording Room...another view... and another. Figure 1a. A view of the recording room with a B&amp;K HATS in the listener position. (The four rear loudspeakers are not visible in this in this view.)  Scenes were recording in blocks of 125 where each block had the same target speaker and the same speaker locations. The front speakers were labelled 1 to 9 so that they could be easily identified by the listener. For each scene one of the 9 front speakers, other than the target speaker, was chosen as the initial look direction speaker. Before the scene is played, the look-direction speaker plays it's number and the listener is instructed to turn to face it. Then after a short pause the multisource scene plays. Within the scene, the target speaker starts about two seconds after the interfering noise sources. The listener is asked to attend to the target speech source and turn to face it when it starts, and also to note down the speaker number. This process is repeated 125 times with a short pause between scenes. The entire block recording lasts about 20 minutes and once recording is complete the audio and headtracking data is segmented into individual scenes.  After each recording block the front speakers are moved to a new set of locations on the grid. Each of the 80 blocks has its own unique speaker layout. The speaker locations were randomised such there was only ever one loudspeaker along each radial direction and that the loudspeaker were spread across the full range of angles. Some examples are shown below. The precise locations of the speakers are provided in the metadata for each scene.  Figure 2. Six of the 80 different speaker layouts that were used.  Following CEC2, the scenes were designed to produce a large range of SNRs for the target. This was set to be between -12 dB and 6 dB in the condition that all the loudspeaker signals are instantaneously added. Note, the actual SNRs recorded at the microphones will vary further depending on the rooms acoustics and the listeners head rotation etc, e.g., SNRs will be higher than that recording in the meta data if the target is closer to the listener than the interferers, and lower if the target is further away. (We plan to release estimates of the SNRs at the microphones in the future.)  The signals were recorded using hearing aid shell with three microphones (front, middle and back). The figure below shows how these are positions when the shell are being worn. The microphones are each about 1 cm apart and form an approximately horizontal line when the head it level. Click on the image to enlarge.  Figure 3. (left) the locations of the three hearing aid microphone; (right) the glasses worn to track the head  ","version":"Next","tagName":"h2"},{"title":"Audio and Headtracking Data format‚Äã","type":1,"pageTitle":"Task 2 Data","url":"/docs/cec3/task_2/cec3_task2_data#audio-and-headtracking-data-format","content":" All audio data is provided in 16-bit PCM format at a sample rate of 48000 kHz. File names have been designed to be compatible with previous Clarity challenges. For each scene the following audio files are provided:  - &lt;SCENE_ID&gt;_mix_CH1.wav - the left and right stereo pair from the front microphone. - &lt;SCENE_ID&gt;_mix_CH2.wav - the left and right stereo pair from the middle microphone. - &lt;SCENE_ID&gt;_mix_CH3.wav - the left and right stereo pair from the back microphone. - &lt;SCENE_ID&gt;_target.wav - the target speech signal. - &lt;SCENE_ID&gt;_interferer.wav - the interferer signals stored as either 2 or 3 channel audio files. - &lt;SCENE_ID&gt;_reference.wav - the signal to be used as the reference for HASPI evaluation.   These signals are provided for both the training and development data set. When the evaluation data is released only the mix files will be provided.  The headtracking data is provided in a CSV file with a frame rate of 250 Hz,  - &lt;SCENE_ID&gt;_hr.csv - the audio-aligned 6 DOF head track   The headtracking has been carefully aligned with the audio recordings. The csv files have six columns: TX, TY, TZ, RX, RY and RZ. (TX, TY, TZ) are the locations of the head with respect to the tracker origin. (RX, RY, RZ) is the rotation of the head with respect to a level, forward facing head. The angles are stored in radians in `helical' form, i.e. they represent a vector which whose direction is the axis of rotation and whose magnitude is the size of the rotation in radian. The figure below plots the raw RX, RY and RZ data for three different scenes. Note, the helical representation makes interpretation a little difficult, but broadly speaking RY is the head turning left to right, RX is the head tipping up and down and RZ is the head tilting.  Head rotation example...and another... and another. Figure 4a. Typical head trajectory with head turning from left to right (red) and tipping down (green) at the end of the scene. The tilt (blue) occurs naturally as the head turns.)  The headtracking data is provided for both the training and development data set and will be provided for optional use during evaluation as well.  ","version":"Next","tagName":"h2"},{"title":"Speaker adaptation data‚Äã","type":1,"pageTitle":"Task 2 Data","url":"/docs/cec3/task_2/cec3_task2_data#speaker-adaptation-data","content":" The scenes that you have been asked to enhance often contain speech signals as interferers. This means that the task of enhancing the target speaker is ambiguous unless you are told which of the speaker to use as target. In this task, we follow the approach used in CEC2 and provide a small set of clean target speaker example utterances. So, for each scene, the ID of the target speaker is provided in the metadata, and systems can then use the examples and select the target as the one that has the matching voice.  Note, these same utterances will be used in the final subjective listening tests, i.e. the listeners will be presented with the examples before listening to the processed signal and told that these are examples of the target speaker that they are meant to be listening to.  ","version":"Next","tagName":"h2"},{"title":"Metadata Formats‚Äã","type":1,"pageTitle":"Task 2 Data","url":"/docs/cec3/task_2/cec3_task2_data#metadata-formats","content":" The following metadata files are provided  # The description of the scenes - scenes.[train|dev].json - metadata for the training/dev scenes - rooms.[train|dev].json - metadata for the training/dev rooms # Listener information - scenes_listeners.dev.json - the listeners/scene pairings to for the standard development set - listeners.json - the audiograms of the listeners # Materials used to make up the scenes - masker_music_list.json - the list of music interferers - masker_nonspeech_list.json - the list of non-speech interferers - masker_speech_list.json - the list of speech interferers - target_speech_list.json - the list target utterances   Most of these files follow the same format as in CEC2. The most important are the scenes.json and rooms.json files and these are described below.  The room describes the locations of the 13 loudspeaker and the listener. There are 80 different 'rooms' corresponding to the 80 different recording sessions, each of which had their own speaker layout. The 'scenes' are represented by a 'room' (i.e., a loudspeaker configuration) and a description of the target and interferers and which of the loudspeakers they were played from. Note, there is a one-to-many relationship between rooms and scenes, i.e., each room is used to record 125 different scenes. However, different subsets of the loudspeakers are used for each scene, so few scenes will have the target and maskers in the same location.  The room' andscene' metadata files are described in more detail below.  ","version":"Next","tagName":"h2"},{"title":"The Room Metadata‚Äã","type":1,"pageTitle":"Task 2 Data","url":"/docs/cec3/task_2/cec3_task2_data#the-room-metadata","content":" The room metadata is stored in a JSON file as a list of dictionaries, with one dictionary representing each room. The format is as follows:  [ { &quot;name&quot;: &quot;R001&quot;, // The Room identifier (R001 to R080) &quot;dimensions&quot;: &quot;6.0x6.0x3.0&quot;, // Approximate room dimensions (fixed) &quot;sources&quot;: [ // A list of the 13 loudspeaker locations { &quot;position&quot;: [ -2.5981, // In x,y,z coordinate but map onto the polar grid 1.5, 1.4 // This is the approximate height of the loudspeakers and is fixed ], &quot;view_vector&quot;: [ // Loudspeakers all directed towards listener 2.5981, -1.5, 0.0 ] }, // etc, 13 source positions in total ] &quot;listener&quot;: [ { &quot;position&quot;: [ // Note the origin is at the listener location 0.0, 0.0, 1.4 ], &quot;view_vector&quot;: [ // For compatibility with CEC2 but replace with headtracking data 0.0, 1.0, 0.0 ] } ] }, ... // more rooms ]   ","version":"Next","tagName":"h3"},{"title":"The Scene Metadata‚Äã","type":1,"pageTitle":"Task 2 Data","url":"/docs/cec3/task_2/cec3_task2_data#the-scene-metadata","content":" The scene metadata is stored in a JSON file as a list of dictionaries with one dictionary for each scene. There are 6000 scenes in the training set and 2500 in the development set. A further 1500 have been retained for the final evaluation.  [ { &quot;scene&quot;: &quot;S00508&quot;, // The Scene ID (S00001 to S10000) &quot;target&quot;: { // The target description &quot;name&quot;: &quot;T001_FRD_00503&quot;, // The utterance ID which starts with the talker ID (T001 to T040) &quot;time_start&quot;: 77020, // The sample at which the target starts &quot;time_end&quot;: 176245, // The sample at which the target ends &quot;position&quot;: 7, // The loudspeaker number (indexed 1 to 13). The location is in the rooms metadata. &quot;dataset&quot;: &quot;train&quot; // Can be train, dev or eval }, &quot;interferers&quot;: [ // The list of interferers. Either two or three. { &quot;name&quot;: &quot;scm_08421&quot;, // The interferer ID &quot;time_start&quot;: 0, // The sample at which the interferer starts (always 0) &quot;time_end&quot;: 220345, // The sample at which the interferer ends (always the end of the scene) &quot;position&quot;: 3, // The loudspeaker number (indexed 1 to 13). The location is in the rooms metadata. &quot;dataset&quot;: &quot;train&quot;, // Can be train, dev or eval &quot;type&quot;: &quot;speech&quot;, // The type of interferer (speech, music, noise) &quot;offset&quot;: 10306318 // The offset of the interferer in the complete audio file }, { &quot;name&quot;: &quot;nom_03397&quot;, &quot;time_start&quot;: 0, &quot;time_end&quot;: 220345, &quot;position&quot;: 10, &quot;dataset&quot;: &quot;train&quot;, &quot;type&quot;: &quot;speech&quot;, &quot;offset&quot;: 15349869 } ], &quot;listener&quot;: { // The listener information &quot;ID&quot;: &quot;L001&quot;, // The listener ID (always L001) &quot;initial_head_orientation&quot;: 3 // The initial look direction (1 to 9) }, &quot;room&quot;: &quot;R001&quot;, // The Room ID (R001 to R080) which provides the loudspeaker locations &quot;SNR&quot;: -1.1145819681656075, // The SNR of the target signal (-12 to 6 dB) &quot;duration&quot;: 220345, // The duration of the scene in samples &quot;dataset&quot;: &quot;train&quot; // Can be train, dev or eval },   Some important points to note are:  The timing of events is recorded in samples at 44100 Hz. This was the sampling rate at which all the audio target and interferer materials are stored and so was used for scene construction. However, signals were then upsampled to 48 kHz to meet the specification of the recording and playback hardware in the recording room. So take care to convert the sample numbers to the correct rate if you are using them to analyse the audio, e.g. to endpoint the target speech.The SNR is not the SNR at the microphones. It is the SNR that would be obtained if the signals delivered to the loudspeakers were summed instantaneously (i.e. without any room acoustic effects). They can be used as a rough guide. We plan to produce estimates of the true SNRs later in the challenge to allow for more accurate performance evaluation. The SNR is computed over the duration of the target signal and using a speech weighted filter. The signal generation code will be made available in the GitHub repository. ","version":"Next","tagName":"h3"},{"title":"Core Software","type":0,"sectionRef":"#","url":"/docs/cpc1/software/cpc1_software","content":"","keywords":"","version":"Next"},{"title":"A. Scene generator‚Äã","type":1,"pageTitle":"Core Software","url":"/docs/cpc1/software/cpc1_software#a-scene-generator","content":" The scene generator is fully open-source python code for generating hearing aid inputs for each scene  Inputs: target and interferer signals, BRIRs, RAVEN project (rpf) files, scene description JSON filesOutputs: Mixed target+interferer signals for each hearing aid channel, direct path (simulating a measurement close to the eardrum). Reverberated pre-mixed signals can also be optionally generated.  ","version":"Next","tagName":"h2"},{"title":"B. Baseline hearing aid processor‚Äã","type":1,"pageTitle":"Core Software","url":"/docs/cpc1/software/cpc1_software#b-baseline-hearing-aid-processor","content":" The baseline hearing aid processor is based on openMHA [1] but with a Python wrapper. The python code configures openMHA with a Camfit compressive fitting [2] for a specific listener‚Äôs audiogram.  This configuration of openMHA includes multiband dynamic compression, non-adaptive differential processing and a softclip plugin. The intention was to produce a basic hearing aid without various aspects of signal processing that are common in high-end hearing aids, but tend to be implemented in proprietary forms so cannot be replicated exactly.  The main inputs and outputs for the processor are as follows:  Inputs: Mixed scene signals for each hearing aid channel, a listener ID drawn from scene-listener pairs identified in ‚Äòscenes_listeners.json‚Äô and an entry in the listener metadata json file ‚Äòlisteners.json‚Äô for that IDOutputs: The stereo hearing aid output signal, &lt;scene&gt;_&lt;listener&gt;_HA-output.wav  ","version":"Next","tagName":"h2"},{"title":"C. Hearing Loss model‚Äã","type":1,"pageTitle":"Core Software","url":"/docs/cpc1/software/cpc1_software#c-hearing-loss-model","content":" Open-source python implementation of a hearing loss model developed by Brian Moore, Michael Stone and other members of the Auditory Perception Group, University of Cambridge (e.g., [3]).  Inputs: A stereo wav audio signal, e.g., the output of the baseline hearing aid processor, and a set of audiograms (both L and R ears).Outputs: The signal after simulating the hearing loss as specified by the set of audiograms (stereo wav file), &lt;scene&gt;_&lt;listener&gt;_HL-output.wav  ","version":"Next","tagName":"h2"},{"title":"D. Speech Intelligibility model‚Äã","type":1,"pageTitle":"Core Software","url":"/docs/cpc1/software/cpc1_software#d-speech-intelligibility-model","content":" Python implementation of a binaural intelligibility model, Modified Binaural Short-Time Objective Intelligibility (MBSTOI; [4]). This is an experimental baseline tool that is level-independent. Note that MBSTOI requires signal time-alignment (and alignment within one-third octave bands).  Inputs: HL-model output signals, audiogram, reference target signal (i.e., the premixed target signal convolved with the BRIR with the reflections ‚Äúturned off‚Äù, specified as ‚Äòtarget_anechoic‚Äô), (scene metadata)Outputs: predicted intelligibility score    ","version":"Next","tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Core Software","url":"/docs/cpc1/software/cpc1_software#references","content":" Kayser, H., Herzke, T., Maanen, P., Pavlovic, C. and Hohmann, V., 2019. Open Master Hearing Aid (openMHA): An integrated platform for hearing aid research. Journal of the Acoustical Society of America, 146(4), pp. 2879-2879.Moore, B. C. J., Alcantara, J. I., Stone, M. and Glasberg, B. R., 1999. Use of a loudness model for hearing aid fitting: II. Hearing aids with multi-channel compression. British Journal of Audiology, 33(3), pp. 157-170.Nejime, Y. and Moore, B. C., 1997. Simulation of the effect of threshold elevation and loudness recruitment combined with reduced frequency selectivity on the intelligibility of speech in noise. Journal of the Acoustical Society of America, 102(1), pp. 603-615.Andersen, A. H., de Haan, J. M., Tan, Z. H. and Jensen, J., 2018. Refinement and validation of the binaural short-time objective intelligibility measure for spatially diverse conditions. Speech Communication, 102, pp. 1-13. ","version":"Next","tagName":"h2"},{"title":"Prizes","type":0,"sectionRef":"#","url":"/docs/cpc1/taking_part/cpc1_prizes","content":"","keywords":"","version":"Next"},{"title":"The Team Prize‚Äã","type":1,"pageTitle":"Prizes","url":"/docs/cpc1/taking_part/cpc1_prizes#the-team-prize","content":" There will be separate prizes for the top contributions by students and non-students.There will be a separate prize for the best performing non-intrusive model.Students eligible for the prize are expected to have made a significant contribution and be first author on the workshop paper.Team prizes have been made available by the generosity of the Hearing Industry Research Consortium.   General prize 1st Place $1000 2nd Place $500 3rd Place $250 Student prize 1st Place $1000 2nd Place $500 3rd Place $250    info The 1st Clarity Prediction Challenge has now finished. For the details of the systems submitted, results and prize winners, please visit the Clarity-2022 Workshop website. ","version":"Next","tagName":"h2"},{"title":"Rules","type":0,"sectionRef":"#","url":"/docs/cpc1/taking_part/cpc1_rules","content":"","keywords":"","version":"Next"},{"title":"Teams‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cpc1/taking_part/cpc1_rules#teams","content":" Teams must have registered and nominated a contact person.Teams can be from one or more institutions.Teams can comprise up to 10 persons.The organisers - and any person forming a team with one or more organisers - may enter the challenge themselves but will not be eligible to win the cash prizes.  ","version":"Next","tagName":"h2"},{"title":"Transparency‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cpc1/taking_part/cpc1_rules#transparency","content":" Teams must provide a technical document of up to 2 pages describing the system/model and any external data and pre-existing tools, software and models used.We will publish all technical documents (anonymous or otherwise).Teams are encouraged ‚Äì but not required ‚Äì to provide us with access to the system(s)/model(s) and to make their code open source.Anonymous entries are allowed but will not be eligible for cash prizes.If a group of people submits multiple entries, they cannot win more than one prize in a given category.All teams will be referred to using anonymous codenames if the rank ordering is published before the final results are announced.  ","version":"Next","tagName":"h2"},{"title":"Intellectual property‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cpc1/taking_part/cpc1_rules#intellectual-property","content":" The following terms apply to participation in this machine learning challenge (‚ÄúChallenge‚Äù). Entrants may create original solutions, prototypes, datasets, scripts, or other content, materials, discoveries or inventions (a ‚ÄúSubmission‚Äù). The Challenge is organised by the Challenge Organiser.  Entrants retain ownership of all intellectual and industrial property rights (including moral rights) in and to Submissions.  As a condition of submission, Entrant grants the Challenge Organiser, its subsidiaries, agents and partner companies, a perpetual, irrevocable, worldwide, royalty-free, and non-exclusive license to use, reproduce, adapt, modify, publish, distribute, publicly perform, create a derivative work from, and publicly display the Submission.  Entrants provide Submissions on an ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE.  ","version":"Next","tagName":"h2"},{"title":"What information can I use?‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cpc1/taking_part/cpc1_rules#what-information-can-i-use","content":" ","version":"Next","tagName":"h2"},{"title":"Training and development‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cpc1/taking_part/cpc1_rules#training-and-development","content":" For  Track 1 (closed-set), teams should use the signals and listener responses provided in the CPC1.train.json file.Track 2 (open-set), teams should use the signals and listener responses provided in the smaller CPC1.train_indep.json.  In addition, teams can use their own data for training or expand the training data through simple automated modifications. Additional pre-training data could be generated by existing speech intelligibility and hearing loss models. The FAQ gives links to some models that might be used for this.  Any audio or metadata can be used during training and development, but during evaluation the prediction model(s) will not have access to all of the data (see next section).  ","version":"Next","tagName":"h3"},{"title":"Evaluation‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cpc1/taking_part/cpc1_rules#evaluation","content":" The only data that can be used by the prediction model(s) during evaluation are  The output of the hearing aid processor/system.The target convolved with the anechoic BRIR (channel 1) for each ear (‚Äòtarget_anechoic‚Äô).The IDs of the listeners assigned to the scene/hearing aid system in the metadata provided.The listener metadata.The prompt for the utterances (the text the actors were given to read)  If you use text from the speech prompts as part of evaluating the systems, we will classify that as an intrusive method for the purpose of awarding prizes.  ","version":"Next","tagName":"h3"},{"title":"Baseline models and computational restrictions‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cpc1/taking_part/cpc1_rules#baseline-models-and-computational-restrictions","content":" Teams may choose to use all or some of the provided baseline models.There is no limit on computational cost.Models can be non-causal.  ","version":"Next","tagName":"h2"},{"title":"What sort of model do I create?‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cpc1/taking_part/cpc1_rules#what-sort-of-model-do-i-create","content":" You can create either a single prediction model that calculates speech intelligibility given a listener's hearing characteristics (that is, the metadata provided), or you can submit separate models of hearing loss and speech intelligibility.You should report the speech intelligibility for the whole sentence for each audio sample/listener combination.  ","version":"Next","tagName":"h2"},{"title":"Submitting multiple entries‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cpc1/taking_part/cpc1_rules#submitting-multiple-entries","content":" If you wish to submit multiple entries,  All systems/models must be submitted for evaluation.Your systems must have significant differences in their approach.You must register multiple teams, submitting each entry as a different team.In your documentation, you must make it clear how the submissions differ.  ","version":"Next","tagName":"h2"},{"title":"Evaluation of systems‚Äã","type":1,"pageTitle":"Rules","url":"/docs/cpc1/taking_part/cpc1_rules#evaluation-of-systems","content":" Entries will be ranked according to their performance in predicting measured intelligibility scores. ","version":"Next","tagName":"h2"},{"title":"Registration","type":0,"sectionRef":"#","url":"/docs/cpc1/taking_part/cpc1_registration","content":"","keywords":"","version":"Next"},{"title":"Registration‚Äã","type":1,"pageTitle":"Registration","url":"/docs/cpc1/taking_part/cpc1_registration#registration","content":" Please use this Google form to register. Please submit one form per team, i.e., providing a single contact email address. Once you have registered, you will receive an email confirmation with a team ID. Please register early to help us organise the challenge.  ","version":"Next","tagName":"h2"},{"title":"Google group‚Äã","type":1,"pageTitle":"Registration","url":"/docs/cpc1/taking_part/cpc1_registration#google-group","content":" If you haven't done so already, please sign up to Clarity's Google group to keep up to date with the challenges. ","version":"Next","tagName":"h2"},{"title":"Baseline system","type":0,"sectionRef":"#","url":"/docs/cpc2/cpc2_baseline","content":"","keywords":"","version":"Next"},{"title":"References‚Äã","type":1,"pageTitle":"Baseline system","url":"/docs/cpc2/cpc2_baseline#references","content":"   Kates, J.M. and Arehart, K.H., 2021. The hearing-aid speech perception index (haspi) version 2. Speech Communication, 131, pp.35-46. ","version":"Next","tagName":"h2"},{"title":"Submission","type":0,"sectionRef":"#","url":"/docs/cpc1/taking_part/cpc1_submission","content":"","keywords":"","version":"Next"},{"title":"Registration‚Äã","type":1,"pageTitle":"Submission","url":"/docs/cpc1/taking_part/cpc1_submission#registration","content":" Teams are required to register to help us organise the challenge. Registered teams will be assigned a unique team ID.  ","version":"Next","tagName":"h2"},{"title":"What evaluation data is provided?‚Äã","type":1,"pageTitle":"Submission","url":"/docs/cpc1/taking_part/cpc1_submission#what-evaluation-data-is-provided","content":" The evaluation data consists of audio signals processed by hearing aid systems, clean reference signals, listener metadata, and a mapping of which listeners listened to which scenes/hearing aid systems.  The evaluation data is available for download here clarity_CPC1_data.test.v1.tgz. See the download page for more details.  Note, the evaluation data does not contain the listener responses. We will score your submission for you and return your score (we aim to do this within 24 hours for of submission). We will then release the true listener responses the day after the submission deadline to allow teams to perform analysis of their results.  ","version":"Next","tagName":"h2"},{"title":"What do I need to submit?‚Äã","type":1,"pageTitle":"Submission","url":"/docs/cpc1/taking_part/cpc1_submission#what-do-i-need-to-submit","content":" All teams must submit  Their predicted intelligibility scoresAn Interspeech paper describing their work (encouraged)A two page technical report (mandatory)  ","version":"Next","tagName":"h2"},{"title":"The predicted intellgibility scores‚Äã","type":1,"pageTitle":"Submission","url":"/docs/cpc1/taking_part/cpc1_submission#the-predicted-intellgibility-scores","content":" You must submit your predicted intelligibility scores for the signals provided.  The predictions should be sent in CSV format files with two columns:  signal_ID, intelligibility_score   Where the signal_ID is the unique signal identifier used for the wav file name (e.g., S08510_L0239_E001) and intelligibility_score is the predicted intelligibility given in terms of the percentage words recognised correctly for the signal (i.e., for 0 to 100).  Your CSV files should be named as follows CPC1_&lt;TEAM_ID&gt;.test.csv and CPC1_&lt;TEAM_ID&gt;.test_indep.csv for closed set and open set evaluations respectively, where &lt;TEAM_ID&gt; is your individual team ID, e.g. 'E001'.  The files should be sent as email attachments to the email address: claritychallengecontact@gmail.com  Please use &quot;CPC1 Submission &lt;TEAM_ID&gt;&quot; as the subject line.  We also encourage you to submit your prediction model(s) code.  info All registered teams will be emailed with their unique team ID shortly before the submission deadline. If you plan to submit please register before the submission deadline.  ","version":"Next","tagName":"h3"},{"title":"Interspeech paper submission‚Äã","type":1,"pageTitle":"Submission","url":"/docs/cpc1/taking_part/cpc1_submission#interspeech-paper-submission","content":" All teams are strongly encouraged to submit a paper describing their work to the Interspeech 2022 Special Session &quot;Speech Intelligibility Prediction for Hearing-Impaired Listeners&quot;.  Interspeech submission instructions are here https://interspeech2022.org/forauthor/submissions.php  The Interspeech papers need to be initially submitted by March 21st (title and abstract), with the full paper due on March 28th.  ","version":"Next","tagName":"h3"},{"title":"The technical report‚Äã","type":1,"pageTitle":"Submission","url":"/docs/cpc1/taking_part/cpc1_submission#the-technical-report","content":" The two page technical report must be submitted as a paper to the Clarity-CPC1-2022 Workshop. Deadline 25th April 2022. An author kit and submission instructions will be made available.  A draft of the report needs to be submitted along with your predictions by 21st March. The draft needs to be sufficiently complete for us to judge whether your system(s)/model(s) is compliant with the challenge rules. You can find a list of key challenge dates here.  Your report should include an abstract and introduction and sections on experimental setup/methodology including system/model information and model/network architecture, evaluation/results, discussion, conclusion and references. Please provide an estimation of the computational resources needed. You must describe any external data and pre-existing tools, software and models used. Please make it clear how your system(s)/model(s) meets the challenge rules.  Your report should cite the following document, which provides an overview of the challenge and the baseline system:  Jon Barker and Michael Akeroyd and Trevor J. Cox and John F. Culling and Jennifer Firth and Simone Graetzer and Holly Griffiths and Lara Harris and Graham Naylor and Zuzanna Podwinska and Eszter Porter and Rhoddy Viveros Munoz, ‚ÄúThe 1st Clarity Prediction Challenge: A machine learning challenge for hearing aid intelligibility prediction,‚Äù in Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2022, Incheon, South Korea, 2022.  The document can be accessed here.  ","version":"Next","tagName":"h3"},{"title":"How will intellectual property be handled?‚Äã","type":1,"pageTitle":"Submission","url":"/docs/cpc1/taking_part/cpc1_submission#how-will-intellectual-property-be-handled","content":" See here under Intellectual Property. ","version":"Next","tagName":"h2"},{"title":"Important Dates","type":0,"sectionRef":"#","url":"/docs/cpc2/cpc2_dates","content":"Important Dates All dates are to be intended anywhere on earth time (AoE). 1st March 2023: Launch of challenge, release of data.1st July 2023: Release of evaluation data and opening of submission window.31st July 2023: Submission deadline. All entrants must have submitted their predictions plus a draft of their technical report. Scores will be returned to entrants within 24 hours of submission. 19th August 2023: Clarity 2023 workshop.19th September 2023: Deadline for submission of finalised Workshop papers Please note that while workshop attendance is not a pre-requisite for participation in the challenge, we strongly encourage all entrants to attend the workshop to present their work.","keywords":"","version":"Next"},{"title":"Obtaining the data","type":0,"sectionRef":"#","url":"/docs/cpc2/cpc2_download","content":"Obtaining the data The following challenge data are available for download: The challenge data is available for download as a single 11 GB file, clarity_CPC2_data.v1_1.tgz.The evaluation data is now available for download as a single 478 MB file, clarity_CPC2_data.test.v1_0.tgz. The evaluation data should be untarred into the same root as the training data. The Github repository containing the baseline code is here. The repository contains code for CPC2 and also for the earlier enhancement and prediction challenges, i.e., CEC1, CEC2 and CPC1. You will find all the necessary instructions for installing the data and running the baseline system.","keywords":"","version":"Next"},{"title":"Data Specification","type":0,"sectionRef":"#","url":"/docs/cpc2/cpc2_data","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"Data Specification","url":"/docs/cpc2/cpc2_data#overview","content":" The training data essentially consists of signals and corresponding listener responses that you can use to train a prediction model.  To maximise the value of the data we have imposed a cross-validation evaluation design: The training data has been split into three partitions, train.1, train.2 and train.3 which are paired with three disjoint evaluation partitions eval.1, eval.2 and eval.3 which will be released. You are asked to train three versions of your final system, i.e., one for each training data subset. This will ensure that we can evaluate your system on unseen listeners and hearing aid systems.  Note, some signals and responses come from CEC1 and some from CEC2. CEC1 used simple scenes with a single interferer; CEC2 had multiple interferers. The evaluation data will only use CEC2 scenes.  ","version":"Next","tagName":"h3"},{"title":"The hearing aid output signals‚Äã","type":1,"pageTitle":"Data Specification","url":"/docs/cpc2/cpc2_data#the-hearing-aid-output-signals","content":" The hearing aid output signals are stored under clarity_data/HA_output and separated into three separate directories, train.1, train.2 and train.3. Each of these directories contains two subdirectories, CEC1 and CEC2, which contain the hearing aid output signals from the CEC1 and CEC2 datasets respectively.  The signals are stored in 16-bit stereo WAV format, with a sampling rate of 32 kHz. The signals are named according to the following convention:  &lt;SCENE_ID&gt;_&lt;LISTENER_ID&gt;_&lt;SYSTEM_ID&gt;.wav # e.g., S09463_L0242_E009.wav   Where &lt;SCENE_ID&gt; is the scene identifier, &lt;LISTENER_ID&gt; is the listener identifier and &lt;SYSTEM_ID&gt; is the hearing aid system identifier.  ","version":"Next","tagName":"h2"},{"title":"The scene reference signals‚Äã","type":1,"pageTitle":"Data Specification","url":"/docs/cpc2/cpc2_data#the-scene-reference-signals","content":" The target reference signals and hearing aid input signals are stored under clarity_data/scenes and separated into two separate directories, CEC1 and CEC2, which contain the target reference signals from the CEC1 and CEC2 datasets respectively. (Note, data for the three training set partitions is stored in the same directory.)  There are a set of stereo audio files for each scene, as follows:  &lt;SCENE_ID&gt;_target_ref.wav # The target reference signal for the intrusiveness intelligibility prediction task &lt;SCENE_ID&gt;_target_anechoic.wav # The anechoic speech target signal &lt;SCENE_ID&gt;_target_&lt;CHANNEL&gt;.wav # The target speech signal for the scene &lt;SCENE_ID&gt;_interferer_&lt;CHANNEL&gt;.wav # The interfering noise for the scene &lt;SCENE_ID&gt;_mixed_&lt;CHANNEL&gt;.wav # The mixed target and interfering noise.   where &lt;SCENE_ID&gt; is the scene identifier and &lt;CHANNEL&gt; can be either CH0, CH1, CH2 or CH3. The channels CH1, CH2 and CH3 are the front, middle and rear hearing aid microphones respectively (each is stereo pair). CH0 is the eardrum signal, i.e., as would be received by the listener's ear canal.  Of these signals, the following is the most important:  &lt;SCENE_ID&gt;_target_ref.wavThis is the signal that should be used as the reference for your intrusive intelligibility prediction model. Note, this is the only signal that will be available in the evaluation data. It is a non-reverberant version of the target signal aligned with the target component of the mixed signal received by the hearing aid. It has been scaled to have the same energy as the target component of the mixed signal received by the hearing aid.  The remaining hearing aid input signals are provided for completeness  &lt;SCENE_ID&gt;_mixed_&lt;CHANNEL&gt;.wav The noisy speech signals that were received by the hearing aid, i.e. the signals that were processed to produce the HA output signals.&lt;SCENE_ID&gt;_target_&lt;CHANNEL&gt;.wav The target speech component of the mixed signals that were received by the hearing aid.&lt;SCENE_ID&gt;_interferer_&lt;CHANNEL&gt;.wav The interfering noise component of the mixed signals that were received by the hearing aid.&lt;SCENE_ID&gt;_target_anechoic.wav The anechoic target speech signal (i.e., same as the target reference signal but without the correct scaling).  It is not anticipated that you will necessarily need these signals for training prediction models but they have been included to help participants gain a better understanding of the data.  ","version":"Next","tagName":"h2"},{"title":"The metadata‚Äã","type":1,"pageTitle":"Data Specification","url":"/docs/cpc2/cpc2_data#the-metadata","content":" The metadata directory (clarity_data/metadata) stores the listener responses to the signals, the listener characteristics and metadata related to each of the scenes (e.g., interferer types, input SNR, etc.).  You will find the following JSON format files,  CEC1.train.1.json, CEC2.train.1.json CEC1.train.2.json, CEC2.train.2.json CEC1.train.3.json, CEC2.train.3.json listeners.json scenes.CEC1.json, scenes.CEC2.json   The contents of these files are as follows.  ","version":"Next","tagName":"h2"},{"title":"The listener responses (CECx.train.x.json)‚Äã","type":1,"pageTitle":"Data Specification","url":"/docs/cpc2/cpc2_data#the-listener-responses-cecxtrainxjson","content":" The CEC1.train.x.json and CEC2.train.x.json files contains a list of dictionaries, each describing a listener response to a signal. The fields are as follows:  CEC&lt;x&gt;.train.&lt;y&gt;.json [ { &quot;prompt&quot;: &quot;i don't want us to apportion blame she said&quot;, &quot;scene&quot;: &quot;S08547&quot;, &quot;n_words&quot;: 9, &quot;hits&quot;: 4, &quot;listener&quot;: &quot;L0239&quot;, &quot;system&quot;: &quot;E001&quot;, &quot;correctness&quot;: 44.4444444444, &quot;response&quot;: &quot;i don't want to have to report he said&quot;, &quot;volume&quot;: 56, &quot;signal&quot;: &quot;S08547_L0239_E001&quot; }, // ... etc ]   In the above,  signal identifies the hearing aid output signal that you will find in the the HA_outputs\\train.1\\CEC1 or HA_outputs\\train.1\\CEC2 directorieslistener is the ID of the listener who provided the response. Using this you can look up the listener's audiogram in the listeners.json file.correctness is the percentage of words that the listener correctly identified. This is the number that you are being asked to predict.volume is the value of the volume control on the hearing aid that the listener used to listen to the signal. This is on a scale of 0 to 100 and was set by default to 50 but listeners were free to adjust it at the start of each session to achieve a comfortable listening level.  ","version":"Next","tagName":"h3"},{"title":"The listener characteristics (listeners.json)‚Äã","type":1,"pageTitle":"Data Specification","url":"/docs/cpc2/cpc2_data#the-listener-characteristics-listenersjson","content":" The listeners.json provides the pure tone audiogram of the left and right ear of each listener. This is stored as a dictionary with the listener ID as the key to facilitate easy look-up.  For each listener the audiogram is stored as a list of frequencies and the corresponding list of levels for the left and right ear. The frequencies are in Hz and the levels are in dB HL.  listeners.json { &quot;L0200&quot;: { &quot;name&quot;: &quot;L0200&quot;, &quot;audiogram_cfs&quot;: [250, 500, 1000, 2000, 3000, 4000, 6000, 8000], &quot;audiogram_levels_l&quot;: [35, 30, 25, 50, 55, 65, 70, 65], &quot;audiogram_levels_r&quot;: [45, 40, 35, 60, 65, 75, 80, 75], }, // ... etc }   Note, listener audiograms will also be provided in the evaluation data, however, the listeners in the evaluation sets will not overlap with listeners in the corresponding training sets, i.e., your predictions systems are expected to be able generalise to new listeners.  ","version":"Next","tagName":"h3"},{"title":"The scene metadata (scenes.CECx.json)‚Äã","type":1,"pageTitle":"Data Specification","url":"/docs/cpc2/cpc2_data#the-scene-metadata-scenescecxjson","content":" The scene metadata contains information about the scene, the target and interfering noise signals, and the SNR of the mixed signal, etc. The data will not be available for the evaluation signals and is being provided here for context and to help participants gain an understanding of the signals.  For a complete description of the scene metadata please see the documentation for the CEC1 and CEC2 challenges. The scenes.CEC1.json has the format described here, and the scenes.CEC2.json has the format described here. ","version":"Next","tagName":"h3"},{"title":"Results","type":0,"sectionRef":"#","url":"/docs/cpc2/cpc2_results","content":"","keywords":"","version":"Next"},{"title":"Prizes‚Äã","type":1,"pageTitle":"Results","url":"/docs/cpc2/cpc2_results#prizes","content":" The Hearing Industry Research Consortium best system prizes were awarded as follows:  1st place: Cuervo and Marxer, Temporal-heirarchical features from noise-robust speech foundation models for non-intrusive intelligibility prediction 2nd place: Mogridge, Close, Sutherland, Goetze and Ragni, Pre-training intermediate ASR features and Human memory simulation for non-intrusive speech intelligibility prediction in the Clarity Prediction Challenge 2   (Note, although 1st and 2nd place systems had very similar RMSE scores, a paired t-test showed that the difference was highly significant).  Congratulations to the winners! ","version":"Next","tagName":"h2"},{"title":"Task 1 Data","type":0,"sectionRef":"#","url":"/docs/cec3/task_1/cec3_task1_data","content":"","keywords":"","version":"Next"},{"title":"Recording setup‚Äã","type":1,"pageTitle":"Task 1 Data","url":"/docs/cec3/task_1/cec3_task1_data#recording-setup","content":" The task uses a novel set of impulse responses that were recorded at the University of Salford using an mh acoustics em64 Eigenmike. These responses can then be used in place of the 6th order ambisonic impulse responses that were generated using room simulation in the previous challenge.  Recordings were made for 32 random configurations of a listener, a target and up to three interferers. Configurations were randomised in advanced and marked out on the floor of the recording room. For each configuration, the microphone is placed at the position of the listener and a loudspeaker is placed, in turn, at each of the sound source positions and directed towards the microphone. The sine-sweep method is then used to estimate the impulse response. The process is repeat for all 32 configurations and for target and three interferers, i.e. 32 x (3 + 1) = 128 impulse responses are recorded in total.  The room is an acoustically treated recording room with approximate dimensions of 5m x 5m x 2m. Some images are provided below for context.  The recording room...another view... and another. Figure 1a. A view of the recording room with floor markings.  The room configurations were randomly generated by independently selecting the x and y coordinates of the listener, target and interferers. The positions were chosen uniformly at random within the dimensions of the room, excluding a 1m border around the walls. It was also imposed that no sound source should be within 1 m of the listener, i.e., samples were rejected and redrawn if this was the case. As with CEC2, for each configuration a height (z) was randomly chosen to be either 1.2m (simulating sitting) or 1.6m (simulating standing) and the microphone and loudspeakers were placed at this height.  The figure below shows the layouts of the 16 rooms used in the development data. The rooms are identified by the room number (R20001 to R20016) and the precise positions of the listener, target and interferers are given in the metadata files (see Section Metadata Formats).  Figure 2. Schematics showing the 16 room layouts used in the development data. T = Target talker; I = Interferer; L = Listener.  ","version":"Next","tagName":"h2"},{"title":"The hearing aid signal simulation‚Äã","type":1,"pageTitle":"Task 1 Data","url":"/docs/cec3/task_1/cec3_task1_data#the-hearing-aid-signal-simulation","content":" The hearing aid input signals are simulated using the same processes that were previously used in the 2nd Clarity Enhancement Challenge. The only difference is that we replaced the simulated impulse responses with the real recordings. Key details are repeated here for completeness, and more information can be found on the CEC2 data pages.  ","version":"Next","tagName":"h2"},{"title":"Head rotation‚Äã","type":1,"pageTitle":"Task 1 Data","url":"/docs/cec3/task_1/cec3_task1_data#head-rotation","content":" The listener is initially oriented away from the target and will turn to be roughly facing the target talker around the time when the target speech starts  Orientation of listener at start of the sample ~25¬∞ from facing the target (standard deviation = 5¬∞), limited to +-2 standard deviations.Start of rotation is between -0.635 s to 0.865s (rectangular probability)The rotation lasts for 200 ms (standard deviation =10 ms)Orientation after rotation is 0-10¬∞ (random with rectangular probability distribution).  ","version":"Next","tagName":"h3"},{"title":"Signal-to-noise ratio (SNR)‚Äã","type":1,"pageTitle":"Task 1 Data","url":"/docs/cec3/task_1/cec3_task1_data#signal-to-noise-ratio-snr","content":" The SNR of the mixtures are engineered to achieve a suitable range of speech intelligibility values. A desired signal-to-noise ratio, SNRD_DD‚Äã (dB), is chosen at random. This is generated with a uniform probability distribution between limits determined by pilot listening tests. The better ear SNR (BE_SNR) models the better ear effect in binaural listening. It is calculated for the reference channel (channel 1, which corresponds to the front microphone of the hearing aid). This value is used to scale all interferer channels. The procedure is described below.  For the reference channel,  The segment of the summed interferers that overlaps with the target (without padding), i‚Ä≤i'i‚Ä≤, and the target (without padding), t‚Ä≤t't‚Ä≤, are extractedSpeech-weighted SNRs are calculated for each ear, SNRL_LL‚Äã and SNRR_RR‚Äã: Signals i‚Ä≤i'i‚Ä≤ and t‚Ä≤t't‚Ä≤ are separately convolved with a speech-weighting filter, h (specified below).The rms is calculated for each convolved signal.SNRL_LL‚Äã and SNRR_RR‚Äã are calculated as the ratio of these rms values. The BE_SNR is selected as the maximum of the two SNRs: BE_SNR = max(SNRL_LL‚Äã and SNRR_RR‚Äã).  Then per channel,  The summed interferer signal, i, is scaled by the BE_SNR i=i√ó10‚àíBE_SNR/20i = i \\times 10^ {-BE\\_SNR/20}i=i√ó10‚àíBE_SNR/20 Finally, i is scaled as follows: i=i√ó10‚àíSNRD/20i = i \\times 10^ {-SNR_D/20}i=i√ó10‚àíSNRD‚Äã/20  The speech-weighting filter is an FIR designed using the host window method [2, 3]. The frequency response is shown in Figure 2. The specification is:  Frequency (Hz) = [0, 150, 250, 350, 450, 4000, 4800, 5800, 7000, 8500, 9500, 22050]Magnitude of transfer function at each frequency = [0.0001, 0.0103, 0.0261, 0.0419, 0.0577, 0.0577, 0.046, 0.0343, 0.0226, 0.0110, 0.0001, 0.0001]  Figure 3. Speech weighting filter transfer function graph.  ","version":"Next","tagName":"h3"},{"title":"Signal generation‚Äã","type":1,"pageTitle":"Task 1 Data","url":"/docs/cec3/task_1/cec3_task1_data#signal-generation","content":" The signal processing pipeline for generating the scene is shown in Figure 1.  The Scene Generator (middle blue box) works on the metadata (shown in green) to create the scene definition metadata, this includes: The metadata associated with the randomly generated scenarios in RPF and JSON format (described on the data page).The metadata for the target speech and the three types of noise interferer. The Scene Renderer takes the metadata, the ambisonics room impulse responses, and the audio of the target speech and interferers, and then produces: Ambisonic audio of the scene.Binaural audio for the hearing aid microphones.  Figure 4. The scene pipeline.  The scene renderer processing is shown in more detail in Figure 2 and is described below:  It takes the ambisonics room impulse responses (RIR); the target and interferer audio; and the scene definition metadata as the input (top line).First, it generates the HOA (High Order Ambisonic) signals through convolution (left blue box).Next, it applies the head rotations by rotating the HOA signals and creates ambisonic audio for both the target and interferer audio.The third row of three blue boxes is the process to take the ambisonic signals, apply the Head Related Room Impulse Responses (HRIR) to create the binaural signals at the hearing aid microphones (bottom line).  Figure 5. The scene renderer.  ","version":"Next","tagName":"h3"},{"title":"Audio data format‚Äã","type":1,"pageTitle":"Task 1 Data","url":"/docs/cec3/task_1/cec3_task1_data#audio-data-format","content":" All audio data is provided in 16-bit PCM format at a sample rate of 44100 kHz. File names have been designed to be compatible with previous Clarity challenges. For each scene the following audio files are provided:  - &lt;SCENE_ID&gt;_mix_CH1.wav - the left and right stereo pair from the front microphone. - &lt;SCENE_ID&gt;_mix_CH2.wav - the left and right stereo pair from the middle microphone. - &lt;SCENE_ID&gt;_mix_CH3.wav - the left and right stereo pair from the back microphone. - &lt;SCENE_ID&gt;_hr.wav - the head rotation signal - &lt;SCENE_ID&gt;_reference.wav - the signal to be used as the reference for HASPI evaluation.   The reference signal is an anechoic version of the target that is to be used as the standard reference for HASPI evaluation. Note, this signal has been processed with the appropriate HRTFs so that it matches the target signal recorded by the front microphone pair at the hearing aid.  The head rotation signal indicates the precise azimuthal angle of the head at each sample. It is stored as a floating point wav file with values between -1 and +1 where the range maps linearly from -180 degrees to +180 degrees. You are free to use this signal in your hearing aid algorithm, but if you do so we would also ask that you evaluate an equivalent version of the system that does not use it (i.e., an ablation study), so that the benefit of known head motion can be measured.  ","version":"Next","tagName":"h2"},{"title":"Speaker adaptation data‚Äã","type":1,"pageTitle":"Task 1 Data","url":"/docs/cec3/task_1/cec3_task1_data#speaker-adaptation-data","content":" The scenes that you have been asked to enhance often contain speech signals as interferers. This means that the task of enhancing the target speaker is ambiguous unless you are told which of the speaker to use as target. In this task, we follow the approach used in CEC2 and provide a small set of clean target speaker example utterances. So, for each scene, the ID of the target speaker is provided in the metadata, and systems can then use the examples and select the target as the one that has the matching voice.  Note, these same utterances will be used in the final subjective listening tests, i.e. the listeners will be presented with the examples before listening to the processed signal and told that these are examples of the target speaker that they are meant to be listening to.  ","version":"Next","tagName":"h2"},{"title":"Metadata formats‚Äã","type":1,"pageTitle":"Task 1 Data","url":"/docs/cec3/task_1/cec3_task1_data#metadata-formats","content":" The following metadata files are provided  # The description of the scenes - scenes.dev.json - metadata for the dev scenes - rooms.dev.json - metadata for the dev rooms - hrir_data.json - HRIRs using in the simulation # Listener information - scenes_listeners.dev.json - the listeners/scene pairings to for the standard development set - listeners.json - the audiograms of the listeners # Materials used to make up the scenes - masker_music_list.json - the list of music interferers - masker_nonspeech_list.json - the list of non-speech interferers - masker_speech_list.json - the list of speech interferers - target_speech_list.json - the list target utterances   Most of these files follow the same format as in CEC2. The most important are the scenes.json and rooms.json files and these are described below.  The room describes the locations of the three loudspeakers and the microphone. There are 16 different 'rooms' each describing a different loudspeaker and microphone layout (as shown in the earlier figure). The 'scenes' are represented by a 'room' (i.e., a loudspeaker configuration) and a description of the target and interferers and which of the loudspeakers they were played from. Note, there is a one-to-many relationship between rooms and scenes, i.e., each room is re-used in multiple scenes although not always with the same selection of interferer locations.  The room and scene metadata files are described in more detail below.  ","version":"Next","tagName":"h2"},{"title":"The room metadata‚Äã","type":1,"pageTitle":"Task 1 Data","url":"/docs/cec3/task_1/cec3_task1_data#the-room-metadata","content":" The room metadata is stored in a JSON file as a list of dictionaries, with one dictionary representing each room. There are 16 room layouts released for development data. A separate set of 15 will be used for evaluation. The metadata for the evaluation set will remain hidden.  The format is as follows:  [ { &quot;name&quot;: &quot;R001&quot;, // The Room identifier (R001 to R080) &quot;dimensions&quot;: &quot;5.0x5.0x2.0&quot;, // Approximate room dimensions (fixed) &quot;target&quot;: { // The target (i.e., the talker position) &quot;position&quot;: [ // The target position 3.0, 4.2, 1.2 // Heights are either 1.2 or 1.6 and target, listener and interferer heights are always match ], &quot;view_vector&quot;: [ // The target direction - will be towards the listener -0.179, 0.984, 0.0 ] }, &quot;listener&quot;: { // The listener (i.e., the microphone position) &quot;position&quot;: [ // The listener position 3.4, 2.0, 1.2 ], &quot;view_vector&quot;: [ // The listener default view direction 0.179, -0.984, 0.0 ] }, &quot;interferers&quot;: [ // A list of 3 interferer positions { &quot;position&quot;: [ 2.2, 2.0, 1.2 ] }, // ... followed by two more positions ] }, ... // more rooms ]   ","version":"Next","tagName":"h3"},{"title":"The scene metadata‚Äã","type":1,"pageTitle":"Task 1 Data","url":"/docs/cec3/task_1/cec3_task1_data#the-scene-metadata","content":" The scene metadata is stored in a JSON file as a list of dictionaries with one dictionary for each scene. There are 2500 scenes in the development set. A further 1500 have been retained for the final evaluation and these will use a different set of room layouts.  [ { &quot;dataset&quot;: &quot;dev&quot;, // The dataset (dev or eval) &quot;room&quot;: &quot;R20001&quot;, // The room identifier (R20001 to R20016) corresponds to R001 to R016 in the rooms metadata &quot;scene&quot;: &quot;S06001&quot;, // The Scene ID (S6001 to S8500) &quot;target&quot;: { &quot;name&quot;: &quot;T030_A08_01034&quot;, // The utterance ID which starts with the talker ID (T001 to T040) &quot;time_start&quot;: 80491, // The sample at which the target starts &quot;time_end&quot;: 214114 // The sample at which the target ends }, &quot;duration&quot;: 258214, // The duration of the scene in samples &quot;interferers&quot;: [ // Either 2 or 3 interferers in positions 1, 2 and/or 3 { &quot;position&quot;: 1, // The loudspeaker number (indexed 1 to 3). The location is in the rooms metadata. &quot;time_start&quot;: 0, // The sample at which the interferer starts (always 0) &quot;time_end&quot;: 258214, // The sample at which the interferer ends (always the end of the scene) &quot;type&quot;: &quot;music&quot;, // The type of interferer (speech, music, noise) &quot;name&quot;: &quot;51/662051.low.mp3&quot;, // The interferer ID &quot;offset&quot;: 2563402 // The offset of the interferer in the complete audio file }, // ... followed by one or two more interferers ], &quot;SNR&quot;: -9.5306, // The SNR of the target signal (-12 to 6 dB) &quot;listener&quot;: { &quot;rotation&quot;: [ // Describes a rotation in the horizontal plane { &quot;sample&quot;: 74252.5369, // The time (in samples) at which the rotation starts &quot;angle&quot;: 71.2240 // The initial angle in degrees }, { &quot;sample&quot;: 82883.5369, // The time (in samples) at which the rotation starts &quot;angle&quot;: 97.4871 // The final angle degrees } ], &quot;hrir_filename&quot;: [ // The HRIR used to simulate the hearing aid inputs &quot;BuK-ED&quot;, // The 'ear drum' HRIR &quot;BuK-BTE_fr&quot;, // The front microphone HRIR &quot;BuK-BTE_mid&quot;, // The middle microphone HRIR &quot;BuK-BTE_rear&quot; // The rear microphone HRIR ] } }, // ... more scenes ]   All times in the scenes.json file are measured in samples at 44100 Hz.In order to simulate the hearing aid signals, the HRIRs are taken from the OlHeadHRTF database with permission . Different scenes have used different heads as indicated by the hrir_filename field.  ","version":"Next","tagName":"h3"},{"title":"References‚Äã","type":1,"pageTitle":"Task 1 Data","url":"/docs/cec3/task_1/cec3_task1_data#references","content":"   Schr√∂der, D. and Vorl√§nder, M., 2011, January. RAVEN: A real-time framework for the auralization of interactive virtual environments. In Proceedings of Forum Acusticum 2011 (pp. 1541-1546). Denmark: Aalborg.Abed, A.H.M. and Cain, G.D., 1978. Low-pass digital filtering with the host windowing design technique. Radio and Electronic Engineer, 48(6), pp.293-300.Abed, A.E. and Cain, G., 1984. The host windowing technique for FIR digital filter design. IEEE transactions on acoustics, speech, and signal processing, 32(4), pp.683-694. ","version":"Next","tagName":"h2"},{"title":"The 2nd Clarity Prediction Challenge","type":0,"sectionRef":"#","url":"/docs/cpc2/cpc2_intro","content":"","keywords":"","version":"Next"},{"title":"Short Description‚Äã","type":1,"pageTitle":"The 2nd Clarity Prediction Challenge","url":"/docs/cpc2/cpc2_intro#short-description","content":" The task involves estimating the intelligibility of speech-in-noise signals that have been processed by hearing aid algorithms and presented to listeners with hearing loss. Each signal contains a short sentence that the listeners were asked to repeat. The system you build needs to be able to predict how many of the words were recognised correctly by the listeners. It is not expected that systems can do this accurately on a per sentence basis, but rather we will rank systems on this basis of how well they perform over a large evaluation set, i.e., which system produces the lowest average estimation error.  The hearing aid signals being assessed vary widely in quality. Examples of good, fair and poor signals are provided below. Your prediction algorithm needs to be able to cope with this variation.  Good\tFair\tPoor Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element.  ","version":"Next","tagName":"h2"},{"title":"The data‚Äã","type":1,"pageTitle":"The 2nd Clarity Prediction Challenge","url":"/docs/cpc2/cpc2_intro#the-data","content":" You will be provided with a set of training data which you can use to develop your systems. This data consists of  Audio produced by a variety of (simulated) hearing aids for speech-in-noise;The corresponding clean reference signals (the original speech);Characteristics of the listeners (pure tone audiograms, etc);The measured speech intelligibility scores from listening tests, where the listener was asked to say what they heard after listening to the hearing aid processed signal.  For full details of the data see the Data page.  ","version":"Next","tagName":"h3"},{"title":"The task‚Äã","type":1,"pageTitle":"The 2nd Clarity Prediction Challenge","url":"/docs/cpc2/cpc2_intro#the-task","content":" You will be provided with an evaluation set containing  Audio produced by a variety of (simulated) hearing aids for speech-in-noise;The audiogram of a listener;The clean reference signal (the original speech).  Your task will be to produce a score (0.0 to 1.0), which should predict the proportion of words in the reference signal that the listener would be able to repeat correctly after listening to the hearing aid processed signal.  We will be considering two types of system: intrusive and non-intrusive. Intrusive systems (also known as double-ended) are those that require a clean speech reference, and non-intrusive systems (also known as single-ended) are those that use the hearing aid output alone.  Intrusive and non-intrusive systems will be separately ranked according to the RMSE between their predictions and the true values.  To help you get started we have provided a baseline system that uses the HASPI metric to predict the speech intelligibility score. Details of this system are available on the Baseline page.  For full details of the task see the rules page.  ","version":"Next","tagName":"h3"},{"title":"Registering and submitting‚Äã","type":1,"pageTitle":"The 2nd Clarity Prediction Challenge","url":"/docs/cpc2/cpc2_intro#registering-and-submitting","content":" To take part in the challenge you will need to register your team and download the data. Entrants will have until 31st July to complete their submissions. Full instructions for submission are available on the Submission page. ","version":"Next","tagName":"h3"},{"title":"Prizes","type":0,"sectionRef":"#","url":"/docs/cpc2/taking_part/cpc2_prizes","content":"","keywords":"","version":"Next"},{"title":"The Team Prize‚Äã","type":1,"pageTitle":"Prizes","url":"/docs/cpc2/taking_part/cpc2_prizes#the-team-prize","content":" There will be a separate prize for the top two systems.Prizes have been made available by the generosity of the Hearing Industry Research Consortium.   1st Place ¬£700 (GBP) 2nd Place ¬£350 (GBP)    info The 2nd Clarity Prediction Challenge has now finished. For the details of the systems submitted, results and prize winners, please visit the Clarity-2023 Workshop website. ","version":"Next","tagName":"h2"},{"title":"CPC2 Registration","type":0,"sectionRef":"#","url":"/docs/cpc2/taking_part/cpc2_registration","content":"","keywords":"","version":"Next"},{"title":"Google group‚Äã","type":1,"pageTitle":"CPC2 Registration","url":"/docs/cpc2/taking_part/cpc2_registration#google-group","content":" If you haven't done so already, please sign up to Clarity's Google group to keep up to date with the challenges. ","version":"Next","tagName":"h2"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/docs/cpc1/taking_part/cpc1_faq","content":"","keywords":"","version":"Next"},{"title":"Speech Intelligibility‚Äã","type":1,"pageTitle":"FAQ","url":"/docs/cpc1/taking_part/cpc1_faq#speech-intelligibility","content":" ","version":"Next","tagName":"h2"},{"title":"What is Speech Intelligibility?‚Äã","type":1,"pageTitle":"FAQ","url":"/docs/cpc1/taking_part/cpc1_faq#what-is-speech-intelligibility","content":" The term Speech Intelligibility is generally used in two different ways. It can refer to how much speech is understood by a listener, or to the number of words correctly identified by a listener as a proportion or percentage of the total number of words. In the Clarity project, we are using the latter definition, i.e., the percentage of words in a sentence that a listener identified correctly. This percentage is the target for your prediction models.  Speech intelligibility captures how a listener's ability to participate in conversation is changed when the speech signal is degraded, e.g., by background noise and room reverberation, or is processed, e.g., by a hearing aid. Your prediction model will need to incorporate a model of the hearing abilities of each listener.  ","version":"Next","tagName":"h3"},{"title":"How is Speech Intelligibility measured with listeners?‚Äã","type":1,"pageTitle":"FAQ","url":"/docs/cpc1/taking_part/cpc1_faq#how-is-speech-intelligibility-measured-with-listeners","content":" In the Clarity project, a set of listeners listen to a sentence and then say what words they heard. In this project, speech intelligibility is measured as the number of words identified correctly as a percentage of the total number of words in a sentence.  You might consider looking at other metrics, such as Word Error Rate (WER), which picks up on, e.g., where listeners insert words not in the original sentence. You might do this if you think that an estimate of WER or other metrics would help your system to estimate speech intelligibility, as defined in the Clarity project.  ","version":"Next","tagName":"h3"},{"title":"How is Speech Intelligibility objectively measured by a computer?‚Äã","type":1,"pageTitle":"FAQ","url":"/docs/cpc1/taking_part/cpc1_faq#how-is-speech-intelligibility-objectively-measured-by-a-computer","content":" When fitting a hearing aid, it would be beneficial for an audiologist to be able to use an objective measure of speech intelligibility to determine what signal processing algorithm(s) should be used to compensate for the listener's hearing impairment. Objective measures are also useful when measured speech intelligibility scores are unavailable, such as when developing a machine learning-based hearing aid algorithm or some other speech enhancement method. Another advantage of non-intrusive measures is that they do not require time-alignment of processed and reference signals.  Objective measures - or metrics - of speech intelligibility are used to allow a computer to estimate the likely performance of humans in listening tests. The main goal of entries to the prediction challenge is to produce one of these measures that performs well for listeners with hearing loss. There are two broad classes of speech intelligibility models:  Intrusive metrics (also known as double-ended) are most common. This is where the intelligibility is estimated by comparing the degraded or processed speech signal with the original clean speech signal.Non-intrusive metrics (also known as single-ended or blind) are less well developed. This is where intelligibility is estimated from the degraded or processed speech signal alone.  In the Clarity project, both types of metrics are of interest. Intrusive metrics will be more accurate in many cases. However, there are hearing aid processes where the speech content is shifted in frequency, which will defeat most current intrusive speech intelligibility metrics. We also hypothesise that there might be issues with intrusive metrics and machine learning approaches in hearing aids that revoice the original speech.  ","version":"Next","tagName":"h3"},{"title":"What speech intelligibility models already exist and what are they used for?‚Äã","type":1,"pageTitle":"FAQ","url":"/docs/cpc1/taking_part/cpc1_faq#what-speech-intelligibility-models-already-exist-and-what-are-they-used-for","content":" There aren't many speech intelligibility models that consider hearing impairment, but one that does is HASPI by Kates and Arehart. In this seminar from the first Clarity workshop, James Kates discusses speech intelligibility models with a focus on the ones he has developed. He also discusses the speech quality metric HASQI. If you're interested in using HASPI or HASQI for the challenge, James Kates has kindly made the MATLAB code and user guide available for download.    Click arrow to see synopsis. Signal degradations, such as additive noise and nonlinear distortion, can reduce the intelligibility and quality of a speech signal. Predicting intelligibility and quality for hearing aids is especially difficult since these devices may contain intentional nonlinear distortion designed to make speech more audible to a hearing-impaired listener. This speech processing often takes the form of time-varying multichannel gain adjustments. Intelligibility and quality metrics used for hearing aids and hearing-impaired listeners must therefore consider the trade-offs between audibility and distortion introduced by hearing-aid speech envelope modifications. This presentation uses the Hearing Aid Speech Perception Index (HASPI) and the Hearing Aid Speech Quality Index (HASQI) to predict intelligibility and quality, respectively. These indices incorporate a model of the auditory periphery that can be adjusted to reflect hearing loss. They have been trained on intelligibility scores and quality ratings from both normal-hearing and hearing-impaired listeners for a wide variety of signal and processing conditions. The basics of the metrics are explained, and the metrics are then used to analyse the effects of additive noise on speech, to evaluate noise suppression algorithms, and to measure differences among commercial hearing aids.  ","version":"Next","tagName":"h3"},{"title":"Hearing Loss‚Äã","type":1,"pageTitle":"FAQ","url":"/docs/cpc1/taking_part/cpc1_faq#hearing-loss","content":" There are many types of hearing loss, but the focus of the Clarity project is the hearing loss that happens with ageing. This is a form of sensorineural hearing loss.  ","version":"Next","tagName":"h2"},{"title":"How does hearing loss affect the perception of audio signals, and how do modern hearing aids process sound to help with this?‚Äã","type":1,"pageTitle":"FAQ","url":"/docs/cpc1/taking_part/cpc1_faq#how-does-hearing-loss-affect-the-perception-of-audio-signals-and-how-do-modern-hearing-aids-process-sound-to-help-with-this","content":" In this seminar from the first Clarity workshop, Karolina Smeds from ORCA Europe and WS Audiology discusses the effects of hearing loss and the hearing aid processing strategies that are typically used to counter the sensory deficits.    Click arrow to see synopsis. Hearing loss leads to several unwanted effects. Loss of audibility for soft sounds is one effect, but also when amplification is used to create audibility for soft sounds, many suprathreshold deficits remain. The most common type of hearing loss is a cochlear hearing loss, where haircells or nerve synapses in the cochlea are damaged. Ageing and noise exposure are the most common causes of cochlear hearing loss. This type of hearing loss is associated with atypical loudness perception and difficulties in noisy situations. Background noise masks for instance speech to a higher degree than for a person with healthy hair cells. This explains why listening to speech-in-noise (SPIN) is such an important topic to work on. A brief introduction to signal processing in hearing aids will be presented. With the use of frequency-specific amplification and compression (automatic gain control, AGC), hearing aids are usually doing a good job in compensating for reduced audibility and for atypical suprathreshold loudness perception. However, it is more difficult to compensate for the increased masking effect. Some examples of strategies will be presented. Finally, natural conversations in noise will be discussed. The balance between being able to have a conversation with a specific communication partner in a group of people and being able to switch attention if someone else starts to talk will be touched upon.  ","version":"Next","tagName":"h3"},{"title":"Prediction model‚Äã","type":1,"pageTitle":"FAQ","url":"/docs/cpc1/taking_part/cpc1_faq#prediction-model","content":" ","version":"Next","tagName":"h2"},{"title":"Do I have to use a separate hearing loss model?‚Äã","type":1,"pageTitle":"FAQ","url":"/docs/cpc1/taking_part/cpc1_faq#do-i-have-to-use-a-separate-hearing-loss-model","content":" No is the short answer! In the baseline, we've used the Cambridge hearing loss model and a separate binaural speech intelligibility model. Another approach would be to create a single model that encapsulates the combined effects of hearing loss and speech perception.  ","version":"Next","tagName":"h3"},{"title":"What should the output of my prediction model be?‚Äã","type":1,"pageTitle":"FAQ","url":"/docs/cpc1/taking_part/cpc1_faq#what-should-the-output-of-my-prediction-model-be","content":" The output should include a predicted speech intelligibility score per input signal, specifically, an estimate of the number of words correct as a percentage of the total number of words in the signal.  ","version":"Next","tagName":"h3"},{"title":"Data‚Äã","type":1,"pageTitle":"FAQ","url":"/docs/cpc1/taking_part/cpc1_faq#data","content":" ","version":"Next","tagName":"h2"},{"title":"Do you have suggestions for expanding the training data?‚Äã","type":1,"pageTitle":"FAQ","url":"/docs/cpc1/taking_part/cpc1_faq#do-you-have-suggestions-for-expanding-the-training-data","content":" The prediction challenge data is limited by having to get the ground truth from listening tests on people with a hearing loss. We look forward to seeing what approaches teams use to help overcome this limitation, such as using unsurpervised models, data augmentation or generating additional ground truth data using a pre-existing model. The baseline model includes a hearing loss and speech intelligibility model that could be used for creating additional pre-training data. There are other models that you might consider where code is available. None has been checked by the Clarity team.  Katerina Zmolikova has made her Pytorch version of the baseline hearing impairment and speech intelligibility model available. Both model fit a neural network framework, are faster but more approximate (see graphs on github).HASQI and HASPI are quality and speech intelligibility metrics designed to work for people with a hearing impairment. James Kates explains more about these above. MATLAB code HASPI v2 and HASQI v2 are available, along with the user guide.STOI-Net: A Deep Learning based Non-Intrusive Speech Intelligibility Assessment Model by Ryandhimas Zezario et al. is monaural and non-intrusive using Python, Keras and TensorFlow. It doesn't model the effect of hearing loss. An alternative is Asger Heidemann Andersen's MATLAB code.  ","version":"Next","tagName":"h3"},{"title":"Missing data‚Äã","type":1,"pageTitle":"FAQ","url":"/docs/cpc1/taking_part/cpc1_faq#missing-data","content":" We have audiograms for all our listening panel. But for other characterisations of hearing, only some of the panel have provided data. Therefore there is missing data that has to be dealt with.  One approach to the missing data is to just ignore it and just use the audiograms. The problem with this approach is that audiograms only quantifies the hearing threshold, and our speech in noise audio samples were not played that quietly. Nevertheless, audiograms are the most common way of characterising hearing loss. Alternatively, a method to use the partial data could be developed, and we expect this would help with speech intelligibility prediction. You will find plenty of data science blog posts, e.g. towards data science discussing different approaches.  A key question is whether the missing data is 'missing at random' i.e. is the distribution of the missing data expected to be the same as that of the present data? For the prediction challenge, this would mean the missing triple-digit-test values are coming from some random sample of the listeners, who'd be no different from the listeners who did complete the triple-digital-test. Unfortunately, this might not be true, because the failure to complete the triple-digit-tests could well correlate with hearing loss (e.g. maybe older people with more hearing loss were less likely to do the test). The Clarity data is probably 'missing not at random'.  One simple solution is to delete examples with missing data, but the loss of so much data probably makes this undesirable.  A more sophisticated approach is to fill gaps in data via imputation i.e. first estimate values for the missing data and then treat the dataset as complete. A couple of simple approaches for imputation are: (i) use the mean value from the dataset for missing values, and (ii) create a model to estimate the missing data from the audiograms. There are other approaches in data science that could be exploited such as coding the missing values into a 'N/A' category within the input data. ","version":"Next","tagName":"h3"},{"title":"FAQ for CPC2","type":0,"sectionRef":"#","url":"/docs/cpc2/taking_part/cpc2_faq","content":"","keywords":"","version":"Next"},{"title":"Speech Intelligibility‚Äã","type":1,"pageTitle":"FAQ for CPC2","url":"/docs/cpc2/taking_part/cpc2_faq#speech-intelligibility","content":" ","version":"Next","tagName":"h2"},{"title":"What is Speech Intelligibility?‚Äã","type":1,"pageTitle":"FAQ for CPC2","url":"/docs/cpc2/taking_part/cpc2_faq#what-is-speech-intelligibility","content":" The term Speech Intelligibility is generally used in two different ways. It can refer to how much speech is understood by a listener, or to the number of words correctly identified by a listener as a proportion or percentage of the total number of words. In the Clarity project, we are using the latter definition, i.e., the percentage of words in a sentence that a listener identified correctly. This percentage is the target for your prediction models.  Speech intelligibility captures how a listener's ability to participate in conversation is changed when the speech signal is degraded, e.g., by background noise and room reverberation, or is processed, e.g., by a hearing aid. Your prediction model will need to incorporate a model of the hearing abilities of each listener.  ","version":"Next","tagName":"h3"},{"title":"How is Speech Intelligibility measured with listeners?‚Äã","type":1,"pageTitle":"FAQ for CPC2","url":"/docs/cpc2/taking_part/cpc2_faq#how-is-speech-intelligibility-measured-with-listeners","content":" In the Clarity project, a set of listeners listen to a sentence and then say what words they heard. In this project, speech intelligibility is measured as the number of words identified correctly as a percentage of the total number of words in a sentence.  You might consider looking at other metrics, such as Word Error Rate (WER), which picks up on, e.g., where listeners insert words not in the original sentence. You might do this if you think that an estimate of WER or other metrics would help your system to estimate speech intelligibility, as defined in the Clarity project.  ","version":"Next","tagName":"h3"},{"title":"How is Speech Intelligibility objectively measured by a computer?‚Äã","type":1,"pageTitle":"FAQ for CPC2","url":"/docs/cpc2/taking_part/cpc2_faq#how-is-speech-intelligibility-objectively-measured-by-a-computer","content":" When fitting a hearing aid, it would be beneficial for an audiologist to be able to use an objective measure of speech intelligibility to determine what signal processing algorithm(s) should be used to compensate for the listener's hearing impairment. Objective measures are also useful when measured speech intelligibility scores are unavailable, such as when developing a machine learning-based hearing aid algorithm or some other speech enhancement method. Another advantage of non-intrusive measures is that they do not require time-alignment of processed and reference signals.  Objective measures - or metrics - of speech intelligibility are used to allow a computer to estimate the likely performance of humans in listening tests. The main goal of entries to the prediction challenge is to produce one of these measures that performs well for listeners with hearing loss. There are two broad classes of speech intelligibility models:  Intrusive metrics (also known as double-ended) are most common. This is where the intelligibility is estimated by comparing the degraded or processed speech signal with the original clean speech signal.Non-intrusive metrics (also known as single-ended or blind) are less well developed. This is where intelligibility is estimated from the degraded or processed speech signal alone.  In the Clarity project, both types of metrics are of interest. Intrusive metrics will be more accurate in many cases. However, there are hearing aid processes where the speech content is shifted in frequency, which will defeat most current intrusive speech intelligibility metrics. We also hypothesise that there might be issues with intrusive metrics and machine learning approaches in hearing aids that revoice the original speech.  ","version":"Next","tagName":"h3"},{"title":"What speech intelligibility models already exist and what are they used for?‚Äã","type":1,"pageTitle":"FAQ for CPC2","url":"/docs/cpc2/taking_part/cpc2_faq#what-speech-intelligibility-models-already-exist-and-what-are-they-used-for","content":" There aren't many speech intelligibility models that consider hearing impairment, but one that does is HASPI by Kates and Arehart. In this seminar from the first Clarity workshop, James Kates discusses speech intelligibility models with a focus on the ones he has developed. He also discusses the speech quality metric HASQI. If you're interested in using HASPI or HASQI for the challenge, James Kates has kindly made the MATLAB code and user guide available for download.    Click arrow to see synopsis. Signal degradations, such as additive noise and nonlinear distortion, can reduce the intelligibility and quality of a speech signal. Predicting intelligibility and quality for hearing aids is especially difficult since these devices may contain intentional nonlinear distortion designed to make speech more audible to a hearing-impaired listener. This speech processing often takes the form of time-varying multichannel gain adjustments. Intelligibility and quality metrics used for hearing aids and hearing-impaired listeners must therefore consider the trade-offs between audibility and distortion introduced by hearing-aid speech envelope modifications. This presentation uses the Hearing Aid Speech Perception Index (HASPI) and the Hearing Aid Speech Quality Index (HASQI) to predict intelligibility and quality, respectively. These indices incorporate a model of the auditory periphery that can be adjusted to reflect hearing loss. They have been trained on intelligibility scores and quality ratings from both normal-hearing and hearing-impaired listeners for a wide variety of signal and processing conditions. The basics of the metrics are explained, and the metrics are then used to analyse the effects of additive noise on speech, to evaluate noise suppression algorithms, and to measure differences among commercial hearing aids.  ","version":"Next","tagName":"h3"},{"title":"Hearing Loss‚Äã","type":1,"pageTitle":"FAQ for CPC2","url":"/docs/cpc2/taking_part/cpc2_faq#hearing-loss","content":" There are many types of hearing loss, but the focus of the Clarity project is the hearing loss that happens with ageing. This is a form of sensorineural hearing loss.  ","version":"Next","tagName":"h2"},{"title":"How does hearing loss affect the perception of audio signals, and how do modern hearing aids process sound to help with this?‚Äã","type":1,"pageTitle":"FAQ for CPC2","url":"/docs/cpc2/taking_part/cpc2_faq#how-does-hearing-loss-affect-the-perception-of-audio-signals-and-how-do-modern-hearing-aids-process-sound-to-help-with-this","content":" In this seminar from the first Clarity workshop, Karolina Smeds from ORCA Europe and WS Audiology discusses the effects of hearing loss and the hearing aid processing strategies that are typically used to counter the sensory deficits.    Click arrow to see synopsis. Hearing loss leads to several unwanted effects. Loss of audibility for soft sounds is one effect, but also when amplification is used to create audibility for soft sounds, many suprathreshold deficits remain. The most common type of hearing loss is a cochlear hearing loss, where haircells or nerve synapses in the cochlea are damaged. Ageing and noise exposure are the most common causes of cochlear hearing loss. This type of hearing loss is associated with atypical loudness perception and difficulties in noisy situations. Background noise masks for instance speech to a higher degree than for a person with healthy hair cells. This explains why listening to speech-in-noise (SPIN) is such an important topic to work on. A brief introduction to signal processing in hearing aids will be presented. With the use of frequency-specific amplification and compression (automatic gain control, AGC), hearing aids are usually doing a good job in compensating for reduced audibility and for atypical suprathreshold loudness perception. However, it is more difficult to compensate for the increased masking effect. Some examples of strategies will be presented. Finally, natural conversations in noise will be discussed. The balance between being able to have a conversation with a specific communication partner in a group of people and being able to switch attention if someone else starts to talk will be touched upon.  ","version":"Next","tagName":"h3"},{"title":"Prediction model‚Äã","type":1,"pageTitle":"FAQ for CPC2","url":"/docs/cpc2/taking_part/cpc2_faq#prediction-model","content":" ","version":"Next","tagName":"h2"},{"title":"Do I have to use a separate hearing loss model?‚Äã","type":1,"pageTitle":"FAQ for CPC2","url":"/docs/cpc2/taking_part/cpc2_faq#do-i-have-to-use-a-separate-hearing-loss-model","content":" No is the short answer! In the baseline, we've used the Cambridge hearing loss model and a separate binaural speech intelligibility model. Another approach would be to create a single model that encapsulates the combined effects of hearing loss and speech perception.  ","version":"Next","tagName":"h3"},{"title":"What should the output of my prediction model be?‚Äã","type":1,"pageTitle":"FAQ for CPC2","url":"/docs/cpc2/taking_part/cpc2_faq#what-should-the-output-of-my-prediction-model-be","content":" The output should include a predicted speech intelligibility score per input signal, specifically, an estimate of the number of words correct as a percentage of the total number of words in the signal.  ","version":"Next","tagName":"h3"},{"title":"Data‚Äã","type":1,"pageTitle":"FAQ for CPC2","url":"/docs/cpc2/taking_part/cpc2_faq#data","content":" ","version":"Next","tagName":"h2"},{"title":"Do you have suggestions for expanding the training data?‚Äã","type":1,"pageTitle":"FAQ for CPC2","url":"/docs/cpc2/taking_part/cpc2_faq#do-you-have-suggestions-for-expanding-the-training-data","content":" The prediction challenge data is limited by having to get the ground truth from listening tests on people with a hearing loss. We look forward to seeing what approaches teams use to help overcome this limitation, such as using unsupervised models, data augmentation or generating additional ground truth data using a pre-existing model. The baseline model includes a hearing loss and speech intelligibility model that could be used for creating additional pre-training data. There are other models that you might consider where code is available. None has been checked by the Clarity team.  Katerina Zmolikova has made her Pytorch version of the baseline hearing impairment and speech intelligibility model available. Both model fit a neural network framework, are faster but more approximate (see graphs on github).HASQI and HASPI are quality and speech intelligibility metrics designed to work for people with a hearing impairment. James Kates explains more about these above. MATLAB code HASPI v2 and HASQI v2 are available, along with the user guide.STOI-Net: A Deep Learning based Non-Intrusive Speech Intelligibility Assessment Model by Ryandhimas Zezario et al. is monaural and non-intrusive using Python, Keras and TensorFlow. It doesn't model the effect of hearing loss. An alternative is Asger Heidemann Andersen's MATLAB code.  ","version":"Next","tagName":"h3"},{"title":"Missing data‚Äã","type":1,"pageTitle":"FAQ for CPC2","url":"/docs/cpc2/taking_part/cpc2_faq#missing-data","content":" We have audiograms for all our listening panel. But for other characterisations of hearing, only some of the panel have provided data. Therefore there is missing data that has to be dealt with.  One approach to the missing data is to just ignore it and just use the audiograms. The problem with this approach is that audiograms only quantify the hearing threshold, and our speech in noise audio samples were not played that quietly. Nevertheless, audiograms are the most common way of characterising hearing loss. Alternatively, a method to use the partial data could be developed, and we expect this would help with speech intelligibility prediction. You will find plenty of data science blog posts, e.g. towards data science discussing different approaches.  A key question is whether the missing data is 'missing at random' i.e. is the distribution of the missing data expected to be the same as that of the present data? For the prediction challenge, this would mean the missing triple-digit-test values are coming from some random sample of the listeners, who'd be no different from the listeners who did complete the triple-digital-test. Unfortunately, this might not be true, because the failure to complete the triple-digit-tests could well correlate with hearing loss (e.g. maybe older people with more hearing loss were less likely to do the test). The Clarity data is probably 'missing not at random'.  One simple solution is to delete examples with missing data, but the loss of so much data probably makes this undesirable.  A more sophisticated approach is to fill gaps in data via imputation i.e. first estimate values for the missing data and then treat the dataset as complete. A couple of simple approaches for imputation are: (i) use the mean value from the dataset for missing values, and (ii) create a model to estimate the missing data from the audiograms. There are other approaches in data science that could be exploited such as coding the missing values into a 'N/A' category within the input data. ","version":"Next","tagName":"h3"},{"title":"CPC2 Submission","type":0,"sectionRef":"#","url":"/docs/cpc2/taking_part/cpc2_submission","content":"","keywords":"","version":"Next"},{"title":"Registration‚Äã","type":1,"pageTitle":"CPC2 Submission","url":"/docs/cpc2/taking_part/cpc2_submission#registration","content":" Teams are required to register to help us organise the challenge. Registered teams will be assigned a unique team ID.  ","version":"Next","tagName":"h2"},{"title":"What evaluation data is provided?‚Äã","type":1,"pageTitle":"CPC2 Submission","url":"/docs/cpc2/taking_part/cpc2_submission#what-evaluation-data-is-provided","content":" The evaluation data consists of audio signals processed by hearing aid systems, clean reference signals, listener metadata, and a mapping of which listeners listened to which scenes/hearing aid systems.  The evaluation data is will be made available when the submission period opens. See the download page for more details.  There will be three evaluation sets (eval1, eval2 and eval3), corresponding to the three three training data partitions. i.e., predictions for the eval1 set should be made with systems trained on the train1 partition; eval2 with train2 and eval3 with train3.  Note, the evaluation data does not contain the listener responses. We will score your submission for you and return your score (we aim to do this within 24 hours of submission). We will then release the true listener responses the day after the submission deadline to allow teams to perform analysis of their results.  ","version":"Next","tagName":"h2"},{"title":"What do I need to submit?‚Äã","type":1,"pageTitle":"CPC2 Submission","url":"/docs/cpc2/taking_part/cpc2_submission#what-do-i-need-to-submit","content":" All teams must submit  Their predicted intelligibility scoresA two page technical report  ","version":"Next","tagName":"h2"},{"title":"The predicted intelligibility scores‚Äã","type":1,"pageTitle":"CPC2 Submission","url":"/docs/cpc2/taking_part/cpc2_submission#the-predicted-intelligibility-scores","content":" Scores for each evaluation set should be stored in a separate CSV file named as follows CPC2_&lt;TEAM_ID&gt;.&lt;SET&gt;.csv, where &lt;TEAM_ID&gt; is your individual team ID, e.g. 'E001' and &lt;SET&gt; is the evaluation set number, either 1, 2, or 3.  The CSV files should have two columns,  signal_ID, intelligibility_score   where the signal_ID is the unique signal identifier used for the wav file name (e.g., S08510_L0239_E001) and intelligibility_score is the predicted intelligibility given in terms of the percentage words recognised correctly for the signal (i.e., from 0 to 100).  The three CSV files should be sent as email attachments to the email address: claritychallengecontact@gmail.com  Please use &quot;CPC2 Submission &lt;TEAM_ID&gt;&quot; as the subject line.  We also encourage you to make your prediction model code available via an open-source license, but this is not a pre-requisite for entry (see challenge rules).  info All registered teams will be emailed with a reminder of their unique team ID shortly before the submission deadline. If you plan to submit please register before the submission deadline.  ","version":"Next","tagName":"h3"},{"title":"The technical report‚Äã","type":1,"pageTitle":"CPC2 Submission","url":"/docs/cpc2/taking_part/cpc2_submission#the-technical-report","content":" The two page technical report must be submitted in the format required for the Clarity-2023 Workshop. The author kit and link for submission can be found on the workshop website.  The report needs to be sufficiently complete for us to judge whether your system(s)/model(s) is compliant with the challenge rules. You can find a list of key challenge dates here.  Your report should include an abstract and introduction and sections on experimental setup/methodology including system/model information and model/network architecture, evaluation/results, discussion, conclusion and references. Please provide an estimation of the computational resources needed. You must describe any external data and pre-existing tools, software and models used. Please make it clear how your system(s)/model(s) meets the challenge rules.  Note, you will not have your final evaluation set scores when you submit your report. We will score your submission for you and return your score (we aim to do this within 24 hours of submission). We will then release the groundtruth listener responses the day after the submission deadline (i.e. 1st August) to allow teams to perform further analysis of their results. This extra information can then be included in a revised version of your report, which will be published on the workshop website in time for the workshop itself on 19th August.  ","version":"Next","tagName":"h3"},{"title":"How will intellectual property be handled?‚Äã","type":1,"pageTitle":"CPC2 Submission","url":"/docs/cpc2/taking_part/cpc2_submission#how-will-intellectual-property-be-handled","content":" See here under Intellectual Property. ","version":"Next","tagName":"h2"},{"title":"The Challenge Rules","type":0,"sectionRef":"#","url":"/docs/cpc2/taking_part/cpc2_rules","content":"","keywords":"","version":"Next"},{"title":"What information can I use?‚Äã","type":1,"pageTitle":"The Challenge Rules","url":"/docs/cpc2/taking_part/cpc2_rules#what-information-can-i-use","content":" ","version":"Next","tagName":"h2"},{"title":"Training and development‚Äã","type":1,"pageTitle":"The Challenge Rules","url":"/docs/cpc2/taking_part/cpc2_rules#training-and-development","content":" Teams should use the signals and listener responses provided in the CPC2.train.json file.  In addition, teams can use their own data for training or expand the training data through simple automated modifications. Additional pre-training data could be generated by existing speech intelligibility and hearing loss models. The FAQ gives links to some models that might be used for this.  Any audio or metadata can be used during training and development, but during evaluation the prediction model(s) will not have access to all of the data (see next section).  ","version":"Next","tagName":"h3"},{"title":"Evaluation‚Äã","type":1,"pageTitle":"The Challenge Rules","url":"/docs/cpc2/taking_part/cpc2_rules#evaluation","content":" The only data that can be used by the prediction model(s) during evaluation are described below.  For non-intrusive methods:  The output of the hearing aid processor/system.The IDs of the listeners assigned to the scene/hearing aid system in the metadata provided.The listener metadata.  Additionally, for intrusive methods:  The target reference signal, i.e. the target convolved with the anechoic BRIR (channel 1) for each ear (‚Äòtarget_anechoic‚Äô).The prompt for the utterances (the text the actors were given to read).  ","version":"Next","tagName":"h3"},{"title":"Baseline models and computational restrictions‚Äã","type":1,"pageTitle":"The Challenge Rules","url":"/docs/cpc2/taking_part/cpc2_rules#baseline-models-and-computational-restrictions","content":" Teams may choose to use all or some of the provided baseline models.There is no limit on computational cost.Models can be non-causal.  ","version":"Next","tagName":"h2"},{"title":"What sort of model do I create?‚Äã","type":1,"pageTitle":"The Challenge Rules","url":"/docs/cpc2/taking_part/cpc2_rules#what-sort-of-model-do-i-create","content":" You model should report the speech intelligibility for the whole sentence for each audio sample/listener combination, i.e. a single score that represents a prediction of the proportion of words that would be recognised correctlyThe model architecture is entirely up to you, e.g. you can create a model that attempts to recognise individual words and then reduces this down to a proportion, or you can estimate an intelligibility score directly from the audio. Models may have explicit hearing loss model stages or be trained directly to map signals and audiograms to predictions.  ","version":"Next","tagName":"h2"},{"title":"Submitting multiple entries‚Äã","type":1,"pageTitle":"The Challenge Rules","url":"/docs/cpc2/taking_part/cpc2_rules#submitting-multiple-entries","content":" If you wish to submit multiple entries,  Your systems must have significant differences in their approach.You must contact the organisers to discuss your plans.If accepted you will be issued with multiple Team IDs to distinguish your entries.In your documentation, you must make it clear how the submissions differ.  ","version":"Next","tagName":"h2"},{"title":"Evaluation of systems‚Äã","type":1,"pageTitle":"The Challenge Rules","url":"/docs/cpc2/taking_part/cpc2_rules#evaluation-of-systems","content":" Entries will be ranked according to their performance in predicting measured intelligibility scores.The system score will be taken to be the RMSE between the predicted and measured intelligibility scores across the complete test set.Separate rankings will be made for intrusive and non-intrusive methods.Systems will only be considered if the technical report has been submitted and the system is judged to be compliant with the challenge rules.  ","version":"Next","tagName":"h2"},{"title":"Teams‚Äã","type":1,"pageTitle":"The Challenge Rules","url":"/docs/cpc2/taking_part/cpc2_rules#teams","content":" Teams must have registered and nominated a contact person.Teams can be from one or more institutions.The organisers - and any person forming a team with one or more organisers - may enter the challenge themselves but will not be eligible to win the cash prizes.  ","version":"Next","tagName":"h2"},{"title":"Transparency‚Äã","type":1,"pageTitle":"The Challenge Rules","url":"/docs/cpc2/taking_part/cpc2_rules#transparency","content":" Teams must provide a technical document of up to 2 pages describing the system/model and any external data and pre-existing tools, software and models used.We will publish all technical documents on the challenge website (anonymous or otherwise).Teams are encouraged ‚Äì but not required ‚Äì to provide us with access to the system(s)/model(s) and to make their code open source.Anonymous entries are allowed but will not be eligible for cash prizes.If a group of people submits multiple entries, they cannot win more than one prize in a given category.All teams will be referred to using anonymous codenames if the rank ordering is published before the final results are announced.Teams are strongly encouraged to submit their report for presentation at the Clarity-2023 Interspeech Satellite Workshop.  ","version":"Next","tagName":"h2"},{"title":"Intellectual property‚Äã","type":1,"pageTitle":"The Challenge Rules","url":"/docs/cpc2/taking_part/cpc2_rules#intellectual-property","content":" The following terms apply to participation in this machine learning challenge (‚ÄúChallenge‚Äù). The entrants' ‚ÄúSubmission‚Äù will consist of a set of intelligibility predictions and an accompanying technical report. The Challenge is organised by the ‚ÄúChallenge Organiser‚Äù.  Entrants retain ownership of all intellectual and industrial property rights (including moral rights) in and to Submissions.  As a condition of submission, Entrant grants the Challenge Organiser, its subsidiaries, agents and partner companies, a perpetual, irrevocable, worldwide, royalty-free, and non-exclusive license to use, reproduce, adapt, modify, publish, distribute, publicly perform, create a derivative work from, and publicly display the Submission.  Entrants provide Submissions on an ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. ","version":"Next","tagName":"h2"},{"title":"ICASSP 2023 More ecologically-valid eval set","type":0,"sectionRef":"#","url":"/docs/icassp2023/data/icassp2023_new_evaluation","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"ICASSP 2023 More ecologically-valid eval set","url":"/docs/icassp2023/data/icassp2023_new_evaluation#overview","content":" This more ecologically-valid eval set (eval2) has been designed to answer the following research question: Can systems trained on simulated data generalise to more ecologically-valid measurement data?  Recordings were carried in a real room using live talkers.The talkers were recorded on both a close microphone and also a 1st-order ambisonic microphone at the listener position. Head rotations are done using the spherical harmonic representation of the sound.HRTFs are applied to get the hearing-aid microphone signals, as for the simulated datasets. The talkers were recorded in noise-free conditions.Noise, music and speech interferers were played from loudspeaker and recorded on the ambisonic microphone.The target talker and intereferer are then mixed to create a scene with a desired SNR.The random positions of the sources and receivers were achieved using the same limitations as applied to the simulated set (e.g. target talker and listener at least 1m apart)  Differences between simulated and ecologically-valid datasets:  Talkers speaking and behaving different when asked to talk to a distant microphone in a real room.Real room acoustic altering sound instead of simulation using geometric room acoustic model.Directivity of interferers not omni-directional.Transducer noise on the distant ambisonic microphone.Measurements had lower order Ambisonics than used in the simulations.  ","version":"Next","tagName":"h2"},{"title":"Environment‚Äã","type":1,"pageTitle":"ICASSP 2023 More ecologically-valid eval set","url":"/docs/icassp2023/data/icassp2023_new_evaluation#environment","content":" Recordings were done in the Acoustics Research Centre's listening room at the University of Salford.  Mid-frequency reverberation time: 0.27sRoom dimensions: 6.6m √ó 5.8m √ó 2.8mBackground noise: 5.7 dBA  Figure 1. The listening room (photo not from evaluation set recording).  ","version":"Next","tagName":"h2"},{"title":"Equipment‚Äã","type":1,"pageTitle":"ICASSP 2023 More ecologically-valid eval set","url":"/docs/icassp2023/data/icassp2023_new_evaluation#equipment","content":" Close microphone: Neumann KM184 cardioidClose microphone preamp: Alice mic.amp.pak1Ambisonic microphone: Sennheiser Ambeo VRInterface: RME Fireface UFXLoudspeaker for interferer: M-audio BX8a  ","version":"Next","tagName":"h2"},{"title":"Target speech‚Äã","type":1,"pageTitle":"ICASSP 2023 More ecologically-valid eval set","url":"/docs/icassp2023/data/icassp2023_new_evaluation#target-speech","content":" A new set of 1,600 sentences generated from the British National Corpus not previously used by Clarity. These were generated using the same process as before [1].  The sentences were read live by 10 actors: 5 male and 5 female. Ages ranged from 20 to 62.Actors were standing. The talker faced the ambisonic microphone. They were told to talk to that microphone and ignore the close microphone.Recorded in noise-free conditions.Each speaker recorded 160 unique sentences, in blocks of 10 talking positions.A cardioid microphone about 50 cm from the talker recorded the reference speech for HASPI and HASQI.  ","version":"Next","tagName":"h2"},{"title":"Interferers‚Äã","type":1,"pageTitle":"ICASSP 2023 More ecologically-valid eval set","url":"/docs/icassp2023/data/icassp2023_new_evaluation#interferers","content":" Recordings reproduced by loudspeakers.Recordings of speech, noise and muisc same sources as CEC2 evaluation set.Each interferer recorded separately on the ambisonics microphone.Loudspeaker facing ambisonic microphone  ","version":"Next","tagName":"h2"},{"title":"Listener‚Äã","type":1,"pageTitle":"ICASSP 2023 More ecologically-valid eval set","url":"/docs/icassp2023/data/icassp2023_new_evaluation#listener","content":" Recordings on a 1st order ambisonics microphone.Front of ambisonic room along x-axis of room.Head rotation done virtually via spherical harmonics with the same statistics as the training set.HRTFs applied to the ambisonic recordings using a virtual loudspeaker set-up to give the signals on the hearing aid microphones.  ","version":"Next","tagName":"h2"},{"title":"Talker, noise and listener position‚Äã","type":1,"pageTitle":"ICASSP 2023 More ecologically-valid eval set","url":"/docs/icassp2023/data/icassp2023_new_evaluation#talker-noise-and-listener-position","content":" 16 different room layouts (see Figure 2) with random talker, interferer and listener positions. These positions determined using the same protocol as used for the simulation. A block of 10 sentences read for each layout.Sources and receivers at the same height (but some variation in the talker z-coordinate because of height differences in the actors).  Figure 2. The 16 layouts. T talker; A ambisonic mic; N noise interferer; S speech interferer; M music interferer.  ","version":"Next","tagName":"h2"},{"title":"Publication‚Äã","type":1,"pageTitle":"ICASSP 2023 More ecologically-valid eval set","url":"/docs/icassp2023/data/icassp2023_new_evaluation#publication","content":" The target speech and interferers will be mixed to gain the desired signal to noise ratio using the same process as for the simulation set. The dataset will be available 1st February 2023.  ","version":"Next","tagName":"h2"},{"title":"Example recordings‚Äã","type":1,"pageTitle":"ICASSP 2023 More ecologically-valid eval set","url":"/docs/icassp2023/data/icassp2023_new_evaluation#example-recordings","content":" Recording of script reading by someone not used for the evaluation set. The audio starts 3-4 seconds into the recording.  Close microphone:  Your browser does not support the audio element.  Ambisonic microphone, A-format:  Front-left-up:  Your browser does not support the audio element.  Front-right-down:  Your browser does not support the audio element.  Back-left-down:  Your browser does not support the audio element.  Back-right-up:  Your browser does not support the audio element.  ","version":"Next","tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"ICASSP 2023 More ecologically-valid eval set","url":"/docs/icassp2023/data/icassp2023_new_evaluation#references","content":" [1] Graetzer, S., Akeroyd, M.A., Barker, J., Cox, T.J., Culling, J.F., Naylor, G., Porter, E. and Viveros-Mu√±oz, R., 2022. Dataset of British English speech recordings for psychoacoustics and speech processing research: The clarity speech corpus. Data in Brief, 41, p.107951. ","version":"Next","tagName":"h2"},{"title":"Scene Generation","type":0,"sectionRef":"#","url":"/docs/icassp2023/data/icassp2023_scene_generation","content":"","keywords":"","version":"Next"},{"title":"The scenario‚Äã","type":1,"pageTitle":"Scene Generation","url":"/docs/icassp2023/data/icassp2023_scene_generation#the-scenario","content":" The scenario is someone listening to a target speaker in a room with two or three interfering sound sources (Figure 1). The scenes are described by a large number of randomised parameters:  The room size and materials (which create moderate reverberation typical of a living room).The identity of the target talker (one of 40 possible speakers).The 7-10 word sentence being uttered by the target talker.The listener, target talker and noise interferer locations.The head orientation of the listener. Initially, the listener is not facing the target talker, but around the time the target speech starts, the listener rotates their head to face the target approximately.The interferer sound samples, which can be a: stream of competing speech; continuous domestic noise source (e.g., a washing machine); or music source.The speech onset and offset times.While scene generating software is provided, we anticipate most entrants would use our database of pre-mixed signals. The website will provide a full description of the scene generation.The main audio signals provided are for 3 microphones on two Behind-The-Ear (BTE) hearing aids (left and right ear).  While scene generating software is provided, we anticipate most entrants would use our database of pre-mixed signals. The website will provide a full description of the scene generation. The main audio signals provided are for 3 microphones on two Behind-The-Ear (BTE) hearing aids (left and right ear).  Figure 1. An example scenario with two noise interferers. ","version":"Next","tagName":"h2"},{"title":"ICASSP 2023 Clarity Challenge Schedule","type":0,"sectionRef":"#","url":"/docs/icassp2023/icassp2023_dates","content":"ICASSP 2023 Clarity Challenge Schedule Key dates are as follows 28th Nov 2022: Challenge launch: Release training/dev data; tools; baseline; rules &amp; documentation.2nd Feb 2023: Release of evaluation data.10th Feb 2023: Teams submit processed signals and technical reports.14th Feb 2023: Results released. Top 5 ranked teams invited to submit papers to ICASSP-202320th Feb 2023: Invited papers submitted to ICASSP-20234-9th June 2023: Overview paper and invited papers presented at dedicated ICASSP session","keywords":"","version":"Next"},{"title":"Modelling the scenario","type":0,"sectionRef":"#","url":"/docs/cpc1/data/cpc1_scenario","content":"","keywords":"","version":"Next"},{"title":"Overview‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cpc1/data/cpc1_scenario#overview","content":" This page details the scenario that we have simulated to create the speech-in-noise samples, which were processed by the (simulated) hearing aids. The processed signals were played to listeners in listening tests to obtain the measured speech intelligibility scores.  It is perfectly possible to compete in the prediction challenge without knowing the information provided on this page, it provides context.  You could just work with the processed signals from the hearing aids and the listening test scores. However, some will find this information useful, for example, because it might inform the sourcing or creation of additional data, for example to be used for unsupervised pre-training.  ","version":"Next","tagName":"h2"},{"title":"Simulating the audio signals that were processed by the hearing aids‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cpc1/data/cpc1_scenario#simulating-the-audio-signals-that-were-processed-by-the-hearing-aids","content":" A listener (or receiver) is in a small room that has low to moderate reverberation. They are listening to a target talker, who is selected from our set of 40 speakers. The target talker is producing one of our unique 7-10 word Clarity sentences. Simultaneously, an interferer sound is playing. This is either a competing talker or a continuous noise source (e.g., a washing machine). The target and interferer are at the same height as the listener. The room dimensions, boundary materials, and the locations of the listener, target and interferer are randomised (discussed below). An example of the scenario is shown in Figure 1. The room geometry showing origin location is defined in Figure 2.  Example SceneRoom Geometry Figure 1. Example scene.  Figure 3, below, shows the basic scene generator. The sound at the receiver is generated first by convolving the source signals with Binaural Room Impulse Responses (BRIRs). This generates reverberated speech and noise that includes the effects of the room and reflections from the listener's head. The reverberated speech and noise signals are then summed after appropriate gains are applied. The gains are set to achieve a Signal-to-Noise Ratio (SNR), which is chosen randomly between limits. The BRIRs are generated using the RAVEN Geometric Room Acoustic Model [1].  There are additional signal paths and outputs generated that have been omitted from Figure 3 for clarity. In addition to the reverberated signals associated with the hearing aid microphones, the signal close to the eardrum is also generated. You can also access the reverberated speech and noise signals before they are mixed.  Figure 3. Simplified diagram of the scene generator. RIR refers to Room Impulse Response, HRTFs refers to Head Related Transfer Functions, SNRs are signal-to-noise ratios, and gain calc. indicates gain calculation. Dry here means anechoic. The outputs are noisy speech signals.  ","version":"Next","tagName":"h2"},{"title":"Room Geometry‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cpc1/data/cpc1_scenario#room-geometry","content":" Cuboid rooms with dimensions length, LLL, by width, WWW, by height, HHH.Length LLL set using a uniform probability distribution random number generator with 3‚â§L(m)‚â§83 \\le L (m) \\le 83‚â§L(m)‚â§8.Height HHH set using a Gaussian distribution random number generator with a mean of 2.7m2.7 m2.7m and standard deviation of 0.8m0.8 m0.8m.Area L√óWL \\times WL√óW set using a Gaussian distribution random number generator with mean 17.7m217.7 m^217.7m2 and standard deviation of 5.5m25.5 m^25.5m2.  ","version":"Next","tagName":"h2"},{"title":"Room Materials‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cpc1/data/cpc1_scenario#room-materials","content":" One of the walls of the room is randomly selected for the location of the door. The door can be at any position with the constraint of being at least at 20 cm from the corner of the wall.  A window is placed on one of the other three walls. The window could be at any position of the wall but at 1.9 m height and at 0.4 m from any corner. The curtains are simulated to the side of the window. For larger rooms, a second window and curtains are simulated following a similar methodology.  A sofa is simulated at a random position as a layer on the wall and the floor. Finally, a rug is simulated at a random location on the floor.  ","version":"Next","tagName":"h2"},{"title":"The receiver‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cpc1/data/cpc1_scenario#the-receiver","content":" The receiver has position, r‚Éó=(xr,yr,zr)\\vec{r} = (x_r,y_r,z_r)r=(xr‚Äã,yr‚Äã,zr‚Äã)  This is positioned within the room using uniform probability distribution random number generators for the x and y coordinates (see Figure 2 for origin location). The reciver can have one of two heights (seated or standing height). There are constraints to ensure that the receiver is not too close to the wall:  ‚àíW/2+1‚â§xr‚â§W/2‚àí1-W/2+1 \\le x_r \\le W/2-1‚àíW/2+1‚â§xr‚Äã‚â§W/2‚àí11‚â§yr‚â§L‚àí11 \\le y_r \\le L-11‚â§yr‚Äã‚â§L‚àí1zrz_rzr‚Äã either 1.2m1.2 m1.2m (sitting) or 1.6m1.6 m1.6m (standing).  The receiver is positioned so as to be roughly facing the target talker. That is to say, within ¬±30\\pm 30¬±30 degrees of target. The angle = 7.5n7.5n7.5n where nnn is an integer and ‚à£n‚à£‚â§4|n| \\le 4‚à£n‚à£‚â§4.  ","version":"Next","tagName":"h2"},{"title":"The target talker‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cpc1/data/cpc1_scenario#the-target-talker","content":" ‚Äã‚ÄãThe target talker has position t‚Éó=(xt,yt,zt)\\vec{t} = (x_t,y_t,z_t)t=(xt‚Äã,yt‚Äã,zt‚Äã)  The target talker is positioned within the room using uniform probability distribution random number generators for the coordinates. Constraints ensure the target is not too close to the wall or receiver. It is set to have the same height as the receiver.  ‚àíW/2+1‚â§xt‚â§W/2‚àí1-W/2+1 \\le x_t \\le W/2-1‚àíW/2+1‚â§xt‚Äã‚â§W/2‚àí11‚â§yt‚â§L‚àí11 \\le y_t \\le L-11‚â§yt‚Äã‚â§L‚àí1‚à£r‚àít‚à£&gt;1|r-t| &gt; 1‚à£r‚àít‚à£&gt;1zt=zrz_t=z_rzt‚Äã=zr‚Äã  A speech directivity pattern is used, which is directed at the listener.  ","version":"Next","tagName":"h2"},{"title":"The interferer‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cpc1/data/cpc1_scenario#the-interferer","content":" The interferers have position i‚Éó1,2,3=(xi,yi,zi)\\vec{i}_{1,2,3} = (x_i,y_i,z_i)i1,2,3‚Äã=(xi‚Äã,yi‚Äã,zi‚Äã)  The interferer is a single point source radiating speech or non-speech noise omnidirectionally. It is placed within the room using uniform probability distribution random number generators for the coordinates. These constraints ensure the interferer is not too close to the wall or receiver. It is set to be at the same height as the receiver. Note, this means that the interferer can be at any angle relative to the receiver.  ‚àíW/2+1‚â§xi‚â§W/2‚àí1-W/2+1 \\le x_i \\le W/2-1‚àíW/2+1‚â§xi‚Äã‚â§W/2‚àí11‚â§yi‚â§L‚àí11 \\le y_i \\le L-11‚â§yi‚Äã‚â§L‚àí1‚à£r‚àíi‚à£&gt;1|r-i| \\gt 1‚à£r‚àíi‚à£&gt;1zi=zrz_i = z_rzi‚Äã=zr‚Äã  ","version":"Next","tagName":"h2"},{"title":"Timing‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cpc1/data/cpc1_scenario#timing","content":" The target sound starts 2 seconds after the start of the interferer. This is so the target is clear and unambiguously identifiable for listening tests. This also gives the hearing aid algorithms some time to adjust to the background noise.The interferer continues 1 second after the target has finished, so that all words in the target utterance can be masked.  ","version":"Next","tagName":"h2"},{"title":"Signal-to-Noise Ratio (SNR)‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cpc1/data/cpc1_scenario#signal-to-noise-ratio-snr","content":" The mixtures are engineered such that the target utterances are at an appropriate level of intelligibility when processed by the default hearing aid software. This is achieved by scaling the interferer. Pilot tests have been conducted to get this approximately correct. Scaling is done this way because it does not require recomputing the BRIRs. Note that the interferer can be at any azimuth from the point of view of the listener/receiver.  A desired signal-to-noise ratio, SNRD_DD‚Äã (dB), is chosen using a uniform probability distribution random number generator between the limits of ranges specified for the speech and non-speech interferers. The calculation is based on the ear that has the better signal to noise ratio, as this mimics the better ear effect in binaural listening, where listeners focus on the ear that has the best SNR. The better ear SNR (BE_SNR) is calculated for the reference channel (channel 1, which corresponds to the front microphone of the hearing aid). This value is used to scale all interferer channels. The procedure is described below.  For the reference channel,  The segment of the interferer that overlaps with the target (without padding) , i‚Äò, and the target (without padding), t‚Äò, are extractedSpeech-weighted SNRs are calculated for each ear, SNRL_LL‚Äã and SNRR_RR‚Äã: Signals i‚Äò and t‚Äô are separately convolved with a speech-weighting filter, h (specified below).The rms is calculated for each convolved signal.SNRL_LL‚Äã and SNRR_RR‚Äã are calculated as the ratio of these rms values. The BE_SNR is selected as the maximum of the two SNRs: BE_SNR = max(SNRL_LL‚Äã and SNRR_RR‚Äã).  Then per channel,  The whole interferer signal, i, is scaled by the BE_SNR i=i√ói = i \\timesi=i√ó BE_SNR Finally, i is scaled as follows: i=i√ó10‚àíSNRD/20i = i \\times 10^{-SNR_D/20}i=i√ó10‚àíSNRD‚Äã/20  The speech-weighting filter is an FIR designed using the host window method [2, 3]. The specification is:  Frequency (Hz) = [0, 150, 250, 350, 450, 4000, 4800, 5800, 7000, 8500, 9500, 22050];Magnitude of transfer function at each frequency = [0.0001, 0.0103, 0.0261, 0.0419, 0.0577, 0.0577, 0.046, 0.0343, 0.0226, 0.0110, 0.0001, 0.0001];  Figure 4, Speech weighting filter transfer function graph.  ","version":"Next","tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/cpc1/data/cpc1_scenario#references","content":" Schr√∂der, D. and Vorl√§nder, M., 2011, January. RAVEN: A real-time framework for the auralization of interactive virtual environments. In Proceedings of Forum Acusticum 2011 (pp. 1541-1546). Denmark: Aalborg.Abed, A.H.M. and Cain, G.D., 1978. Low-pass digital filtering with the host windowing design technique. Radio and Electronic Engineer, 48(6), pp.293-300.Abed, A.E. and Cain, G., 1984. The host windowing technique for FIR digital filter design. IEEE transactions on acoustics, speech, and signal processing, 32(4), pp.683-694. ","version":"Next","tagName":"h2"},{"title":"CPC1 Data","type":0,"sectionRef":"#","url":"/docs/cpc1/data/cpc1_data","content":"","keywords":"","version":"Next"},{"title":"A. Training, development, evaluation data‚Äã","type":1,"pageTitle":"CPC1 Data","url":"/docs/cpc1/data/cpc1_data#a-training-development-evaluation-data","content":" The dataset is split into these two subsets: training/development (train) and evaluation (eval).  You are responsible for splitting the training/development dataset into data for training and development, e.g., using k-fold cross validation.The final evaluation and ranking will be performed with the (held-out) evaluation set.For more information on supplementing the training data, please see the rules, and also the FAQ. The evaluation dataset will be made available one month before the challenge submission deadline.  ","version":"Next","tagName":"h2"},{"title":"B. The scene dataset‚Äã","type":1,"pageTitle":"CPC1 Data","url":"/docs/cpc1/data/cpc1_data#b-the-scene-dataset","content":" The complete dataset is composed of a large number of scenes associated with 6 talkers, 10 hearing aid systems and around 25 listeners.  Each scene corresponds to a unique target utterance and a unique segment of noise from an interferer. The training/development and evaluation sets are disjoint for system and listener.  Binaural Room Impulse Responses (BRIRs) are used to model how the sound is altered as it propagates through the room and interacts with the head. The audio signals for the scenes are generated by convolving source signals with the BRIRs and summing. See the page on modelling the scenario for more details. Randomised room dimensions, target and interferer locations are used. RAVEN is the geometric room acoustic model used to create the BRIR.  ","version":"Next","tagName":"h2"},{"title":"B.1 Training/development data‚Äã","type":1,"pageTitle":"CPC1 Data","url":"/docs/cpc1/data/cpc1_data#b1-trainingdevelopment-data","content":" This contains all the information about how the signals processed by the hearing aids were created. For the prediction challenge, some of the data can be ignored (but is included because some may find it useful).  Data and metadata most useful for the prediction challenge:  The output of the hearing aid processor.The target convolved with the anechoic Binaural Room Impulse Response (BRIR) (channel 1) for each ear (‚Äòtarget_anechoic‚Äô).The mono target and interferer signals (pre-convolution).Prompts of the utterances (what the actors were told to say)Metadata describing the scene: a JSON file containing, e.g., the filenames of the sources, the location of the sources, the viewvector of the target source, the location and viewvector of the receiver, the room dimensions (see specification below), and the room number, which corresponds to the RAVEN BRIR, rpf and ac files.  For evaluation not all of the data is available, see below.  Other information also provided, click me to expand Data used to create inputs to hearing aids, etc: The target and interferer BRIRs (4 pairs: front, mid, rear and eardrum for left and right ears).Head Related Impulse Responses (HRIRs) including those corresponding to the target azimuth.For each hearing aid microphone (channels 1-3 where channel 1 is front, channel 2 is mid and channel 3 is rear) and a position close to the eardrum (channel 0): The target convolved with the appropriate BRIR;The interferer convolved with the appropriate BRIR;The sum of the target and interferer convolved with the appropriate BRIRs. The BRIRs are generated for: A hearing aid with 3 microphone inputs (front, mid, rear). The hearing aid has a Behind-The-Ear (BTE) form factor; see Figure 1. The distance between microphones is approx. 7.6 mm. The properties of the tube and ear mould are not considered.Close to the eardrum.The anechoic target reference (front microphone; the premixed target signal convolved with the BRIR with the reflections ‚Äúturned off‚Äù). Figure 1. Front (Fr), Middle (Mid) and Rear microphones on a BTE hearing aid form. (HRIRs) are used to model how sound is altered as it propagates in a free-field and interacts with the head (i.e., no room is included). These are taken from the OlHeadHRTF database with permission. These include HRIRs for human heads and for three types of head-and-torso simulator/mannekin. The eardrum HRIRs (labelled ED) are for a position close to the eardrum of the open ear. The RAVEN project files - termed &quot;rpf&quot; - are specification files for the geometric room acoustic model that include a complete description of the room.  ","version":"Next","tagName":"h3"},{"title":"B.2 Evaluation scene data‚Äã","type":1,"pageTitle":"CPC1 Data","url":"/docs/cpc1/data/cpc1_data#b2-evaluation-scene-data","content":" For each scene in the evaluation data only the following will be available:  The output of the hearing aid processor.The target convolved with the anechoic BRIR (channel 1) for each ear (‚Äòtarget_anechoic‚Äô).The IDs of the listeners assigned to the scene/hearing aid system in the metadata provided.The listener metadata.The prompt for the utterances (the text the actors were given to read)  ","version":"Next","tagName":"h3"},{"title":"C Listener data‚Äã","type":1,"pageTitle":"CPC1 Data","url":"/docs/cpc1/data/cpc1_data#c-listener-data","content":" We will provide metadata characterising the hearing abilities of our listening panel. The listening panel data will be split, so that the listeners in the held back evaluation data are different from those provided in the training and development data. The listening panel are experienced bilateral hearing-aid users (they use two hearing aids but the hearing loss may be asymmetrical) with an averaged hearing loss as measured by pure tone air-conduction of between 25 and about 60 dB in the better ear, with fluent speaking of (and listening to) British English.  For every listener, you will be given the left and right pure tone air-conduction audiograms. These measure the threshold at which people can hear a pure-tone sound. For some listeners you will be provided with additional characterisation of their hearing. Consequently, if you wish to exploit this additional data, you will need to deal with the missing data. See the FAQ for more on missing data. Below is a description of each measure.  SSQ12 - Speech, Spatial, &amp; Qualities of Hearing questionnaire, 12-question version‚Äã  This is a popular self-assessment questionnaire of hearing disability. Each item asks about listening situations typical of real life and asks how well a listener would do in it. The SSQ assesses ability to make successful use of one‚Äôs hearing (i.e. hearing disability, or activity limitation). This is an intermediate link between the audiological measurement of someone's hearing loss (i.e. their impairment) and a patient's assessment of how that hearing loss impacts their wider life (i.e. their handicap, or participation restriction). The 12 questions are given in table 1 of this paper and FYI a recent paper that used it is here.  Responses to each question are on a scale from 0 to 10, with 0 representing &quot;not at all&quot; (or &quot;jumbled&quot;/&quot;concentrate hard&quot; for #11 &amp; #12), and 10 representing &quot;perfect&quot; (or &quot;not jumbled&quot;/&quot;no need to concentrate&quot;). We programmed this as a visual-analog slider, which the participant could set to any position from 0 to 10. The SSQ12 data supplied are the responses to each question, from 0-10 at 1 decimal place resolution, along with the mean of all 12 questions.  GHAPB - Glasgow hearing-aid benefit profile questionnaire‚Äã  This is designed to assess the efficacy and effectiveness of someone's hearing aid(s) in different scenarios. It asks respondents to consider four scenarios involving speech and to rate on a five-point scale their perceived initial (i.e. unaided) hearing disability, initial handicap, aided benefit, aided handicap, hearing aid use, and hearing aid satisfaction. The listening panel are experienced hearing-aid users, so some of the rating would be about their normal hearing aid. This paper describes the GHABP and provides some normative data.  For each scenario, the participant is asked a primary question about if a situation happens to them (relatable). If they answer  No, it moves onto to the next scenario.Yes, then a list of six secondary questions are asked (see figure below) If one of the secondary questions is not relatable to the participant, they're asked to respond &quot;N/A&quot; for not applicable.  Figure 2. The GHAPB questionnaire.  There are four scenarios:  listening to the television when the volume is adjusted for others.Having a conversation with one person in quiet.Having a conversation on a busy street or in a shop.Having a conversation with several people in a group.  In the datafile, the question numbers are coded as x.y where x is the scenario number and y the secondary question number.  If the answer to primary questions is No, then all the secondary questions are coded as 0.  If the answer to primary questions is Yes, then each subsequent question is scored as 0. = N/A  = first option in the list (eg &quot;no difficulty&quot;)= second= third= fourth= fifth (e.g. &quot;cannot manage at all&quot;)  There is no global score for the GHABP. The six secondary questions ask about different things and so should not be averaged across questions, though it is fairly common to average within-question across scenario.  DTT (digit-triplet test, also known as a triple digit test)‚Äã  This is an adaptive test of speech-in-noise ability. In each trial a listener hears three spoken digits (e.g. 3-6-1) against a background of noise at a given signal-to-noise-ratio (SNR). The task is to respond on a keypad with those three digits in the order they were presented. If the listener gets all three correct, then the SNR is reduced for the next trial so making it slightly harder. If the listener makes any mistake (i.e., any digit wrong, or the order wrong) then the SNR is increased, so making the next trial slightly easier. The test carries on trial-by-trial. The test asymptotes to the SNR at which the participant is equally likely to get all three correct or not, with a few tens of trials needed to get an acceptable result. DTT tests are now used world-wide to measure hearing as they are easy to make in any local language, to explain to participants and to do, and moreover can be done over the internet or telephone as they measure a relative threshold (signal-to-noise ratio), not an absolute threshold in dB SPL. Listeners are encouraged to set a volume that is comfortable and that does not distort or crackle, but is not too quiet.  This paper is a recent scoping review of the field. The particular version we used is Vlaming et al's high-frequency DTT, which uses a high-pass noise as the masker. Ours starts at -14 dB SNR, goes up/down at 2 dB steps per trial, and continues for 40 trials.  In the datafile, an average of the SNR for the last 30 trials is provided (labelled 'threshold'). For reference, the SNRs are supplied for each trial as well. The very first trial is practice and is not scored.  ","version":"Next","tagName":"h2"},{"title":"D Data file formats and naming conventions‚Äã","type":1,"pageTitle":"CPC1 Data","url":"/docs/cpc1/data/cpc1_data#d-data-file-formats-and-naming-conventions","content":" ","version":"Next","tagName":"h2"},{"title":"D.1 Abbreviations in Filenames‚Äã","type":1,"pageTitle":"CPC1 Data","url":"/docs/cpc1/data/cpc1_data#d1-abbreviations-in-filenames","content":" R ‚Äì ‚Äúroom‚Äù: e.g., ‚ÄúR02678‚Äù # Room ID linking to RAVEN rpf fileS ‚Äì ‚Äúscene‚Äù: e.g., S00121 # Scene ID for a particular setup in a room I.e., room + choice of target and interferer signalsBNC ‚Äì BNC sentence identifier e.g. BNC_A06_01702CH ‚Äì CH0 ‚Äì eardrum signalCH1 ‚Äì front signal, hearing aid channelCH2 ‚Äì middle signal, hearing aid channelCH3 ‚Äì rear signal, hearing aid channel I/i1 ‚Äì Interferer, i.e., noise or sentence ID for the interferer/maskerT ‚Äì talker who produced the target speech sentencesL ‚Äì listenerE ‚Äì entrant (identifying a team participating in the challenge)t ‚Äì target (used in BRIRs and RAVEN project ‚Äòrpf‚Äô files)  ","version":"Next","tagName":"h3"},{"title":"D.2 General‚Äã","type":1,"pageTitle":"CPC1 Data","url":"/docs/cpc1/data/cpc1_data#d2-general","content":" Audio and BRIRs will be 44.1 kHz 32 bit wav files in either mono or stereo as appropriate.Where stereo signals are provided, the two channels represent the left and right signals of the ear or hearing aid microphones.HRIRs have a sampling rate of 48 kHz.Metadata will be stored in JSON format wherever possible.Room descriptions are stored as RAVEN project ‚Äòrpf‚Äô configuration files.Signals are saved within the Python code as 32-bit floating point by default.  ","version":"Next","tagName":"h3"},{"title":"D.3 Prompt and transcription data‚Äã","type":1,"pageTitle":"CPC1 Data","url":"/docs/cpc1/data/cpc1_data#d3-prompt-and-transcription-data","content":" The following text is available for the target speech:  Prompts are the text that was supposed to be spoken as presented to the readers.‚ÄòDot‚Äô transcriptions contain the text as it was spoken in a form more suitable for scoring tools.These are stored in the master json metadata file.  ","version":"Next","tagName":"h3"},{"title":"D.4 Timing in audio files‚Äã","type":1,"pageTitle":"CPC1 Data","url":"/docs/cpc1/data/cpc1_data#d4-timing-in-audio-files","content":" The target sound starts 2 seconds after the start of the interferer. This is so the target is clear and unambiguously identifiable for listening tests. This also gives the hearing aid algorithms some time to adjust to the background noise.The interferer continues 1 second after the target has finished, so that all words in the target utterance can be masked.  ","version":"Next","tagName":"h3"},{"title":"D.5 Source audio files‚Äã","type":1,"pageTitle":"CPC1 Data","url":"/docs/cpc1/data/cpc1_data#d5-source-audio-files","content":" Wav files containing the original source materials.Could be used as the clean speech reference in an intrusive (double-ended) prediction modelOriginal target sentence recordings:   &lt;Talker ID&gt;_&lt;BNC sentence identifier&gt;.wav   ","version":"Next","tagName":"h3"},{"title":"D.6 Preprocessed scene signals‚Äã","type":1,"pageTitle":"CPC1 Data","url":"/docs/cpc1/data/cpc1_data#d6-preprocessed-scene-signals","content":" Audio files storing the signals picked up by the hearing aid microphone ready for processing.Target_anechoic could be used as the clean speech reference in an intrusive (double-ended) prediction model.Separate signals are generated for each hearing aid microphone pair or ‚Äòchannel‚Äô.  &lt;Scene ID&gt;_target_&lt;Channel ID&gt;.wav &lt;Scene ID&gt;_interferer_&lt;Channel ID&gt;.wav &lt;Scene ID&gt;_mixed_&lt;Channel ID&gt;.wav &lt;Scene ID&gt;_target_anechoic.wav   Scene ID ‚Äì S00001 to S10000  S followed by 5 digit integer with 0 pre-padding  Channel ID  CH0 ‚Äì Eardrum signalCH1 ‚Äì Hearing aid front microphoneCH2 ‚Äì Hearing aid middle microphoneCH3 ‚Äì Hearing aid rear microphone  ","version":"Next","tagName":"h3"},{"title":"D.7 Enhanced signals‚Äã","type":1,"pageTitle":"CPC1 Data","url":"/docs/cpc1/data/cpc1_data#d7-enhanced-signals","content":" These are the audio signals that the listeners heard during the speech intelligibility testing. The signals that are output by a given enhancement (hearing aid) model or system.  &lt;Entrant ID&gt;_&lt;Scene ID&gt;_&lt;Listener ID&gt;_HA-output.wav # HA output signal (i.e., as submitted by the challenge entrants)  Listener ID ‚Äì ID of the listener panel member, e.g., L200 to L244.    ","version":"Next","tagName":"h3"},{"title":"D.8 Scene metadata‚Äã","type":1,"pageTitle":"CPC1 Data","url":"/docs/cpc1/data/cpc1_data#d8-scene-metadata","content":" A JSON file called scenes.CPC1_train.json containing a description of each scene that is used in the listening experiments. It is a hierarchical dictionary, with the top level being scenes indexed by unique scene ID, and each scene described by a second-level dictionary. Here, viewvector indicates the direction vector or line of sight.  scenes.json [ { &quot;scene&quot;: &quot;S00001&quot;, &quot;room&quot;: { &quot;name&quot;: &quot;R00001&quot;, &quot;dimensions&quot;: &quot;5.9x3.4186x2.9&quot; // Room dimensions in metres }, &quot;SNR&quot;: 3.8356, &quot;hrirfilename&quot;: &quot;VP_N5-ED&quot;, // HRIR filename &quot;target&quot;: { // target positions (x,y,z) and view vectors (look directions, x,y,z) &quot;Positions&quot;: [ -0.5, 3.4, 1.2 ], &quot;ViewVectors&quot;: [ 0.291, -0.957, 0 ], &quot;name&quot;: &quot;T022_HCS_00002&quot;, // target speaker code and BNCid &quot;nsamples&quot;: 153468, // length of target speech in samples }, &quot;listener&quot;: { &quot;Positions&quot;: [ 0.2, 1.1, 1.2 ], &quot;ViewVectors&quot;: [ -0.414, 0.91, 0 ] }, &quot;interferer&quot;: { &quot;Positions&quot;: [ 0.4, 3.2, 1.2 ], &quot;name&quot;: &quot;CIN_dishwasher_012&quot;, // interferer name &quot;nsamples&quot;: 1190700, // interferer length in samples &quot;duration&quot;: 27, // interferer duration in seconds &quot;type&quot;: &quot;noise&quot;, // interferer type: noise or speech &quot;offset&quot;: 182115, // interferer segment starts at n samples from beginning of recording }, &quot;azimuth_target_listener&quot;: -7.55, // angle azimuth in degrees of target for receiver &quot;azimuth_interferer_listener&quot;: -29.92, // angle azimuth in degrees of interferer for receiver &quot;dataset&quot;: &quot;train&quot;, // dataset: train, dev or eval/test &quot;pre_samples&quot;: 88200, // number of samples of interferer before target onset &quot;post_samples&quot;: 44100 // number of samples of interferer after target offset }, { // ... etc. } ]   There are JSON files containing the scene specifications per dataset, e.g., scenes.train.json.Note that the scene ID and room ID might have a one-to-one mapping in the challenge, but are not necessarily the same.A scene is completely described by the room ID and target and interferer source IDs, as all other information, e.g., source + target geometry are already in the RAVEN project rpf files. Only the room ID is needed to identify the BRIR files.The listener ID is not stored in the scene metadata; this information is stored separately in a scenes_listeners.json file.Non-speech interferers are labelled CIN_&lt;noise type&gt;_XXX, while speech interferers are labelled &lt;three letter code including dialect and talker gender&gt;_XXXXX .  ","version":"Next","tagName":"h3"},{"title":"D.9 Listener metadata‚Äã","type":1,"pageTitle":"CPC1 Data","url":"/docs/cpc1/data/cpc1_data#d9-listener-metadata","content":" Listener audiogram data stored in a single JSON file called listeners.CPC1_train.json with the following format.  listeners.json { &quot;L0001&quot;: { &quot;name&quot;: &quot;L0001&quot;, &quot;audiogram_cfs&quot;: [250, 500, 1000, 2000, 3000, 4000, 6000, 8000], &quot;audiogram_levels_l&quot;: [10, 10, 20, 30, 40, 55, 55, 60], &quot;audiogram_levels_r&quot;: [10, 15, 25, 40, 50, 65, 65, 70 ], }, &quot;L0002&quot;: { // ... etc. }, // ... etc. }   A spreadsheet named listener_data.CPC1_train.xlsx containing the SSQ12, GHAPB, DTT data for each listener where it is available.  ","version":"Next","tagName":"h3"},{"title":"D.10 Listener intelligibility data‚Äã","type":1,"pageTitle":"CPC1 Data","url":"/docs/cpc1/data/cpc1_data#d10-listener-intelligibility-data","content":" JSON files CPC1.train.json (for Track 1) and CPC1.train_indep.json (for Track 2) which provides the responses made by the listeners when presented with a particular scene processed by a particular system. The data is a simple list of dictionaries with one entry for each listener response  CPC1.tran.json [ { &quot;scene&quot;:&quot;S08510&quot;, // The identity of the scene &quot;listener&quot;:&quot;L0239&quot;, // The identity of the listener &quot;system&quot;:&quot;E001&quot;, // The identify of the HA system &quot;prompt&quot;:&quot;i suppose you wouldn't be free for dinner this evening&quot;, // The target sentence (prompt) &quot;response&quot;:&quot;freeze evening&quot;, // The listeners response (transcript) &quot;n_words&quot;:10, // Number of words in the target sentence &quot;hits&quot;:1, // Number of words recognised correctly &quot;correctness&quot;:10.0, // The percentage of words recognised correctly &quot;signal&quot;:&quot;S08510_L0239_E001&quot; // The name of the file containing the signal listened to. }, { // ... etc. }, // ... etc. ]   ","version":"Next","tagName":"h3"},{"title":"E. Reproduction Levels‚Äã","type":1,"pageTitle":"CPC1 Data","url":"/docs/cpc1/data/cpc1_data#e-reproduction-levels","content":" The graph gives the SPL from one of our headsets based on the volume level of the tablet. The input signal was ICRA speech-shaped noise [1], unmodulated in time, and scaled to an RMS of 0.3.  Figure 3. Headset SPL by tablet volume level.  ","version":"Next","tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"CPC1 Data","url":"/docs/cpc1/data/cpc1_data#references","content":" [1] ICRA standard noises, https://icra-audiology.org/Repository/icra-noise. We used track #1. ","version":"Next","tagName":"h2"},{"title":"Baseline System","type":0,"sectionRef":"#","url":"/docs/icassp2023/software/icassp2023_baseline","content":"","keywords":"","version":"Next"},{"title":"Baseline performance‚Äã","type":1,"pageTitle":"Baseline System","url":"/docs/icassp2023/software/icassp2023_baseline#baseline-performance","content":" Baseline performance using amplification with no enhancement will appear shortly.    ","version":"Next","tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Baseline System","url":"/docs/icassp2023/software/icassp2023_baseline#references","content":"   Kates, J.M. and Arehart, K.H., 2021. The hearing-aid speech perception index (HASPI) version 2. Speech Communication, 131, pp.35-46.Kates, J.M. and Arehart, K.H., 2014. &quot;The hearing-aid speech quality index (HASQI) version 2&quot;. Journal of the Audio Engineering Society. 62 (3): 99‚Äì117. ","version":"Next","tagName":"h2"},{"title":"ICASSP 2023 Clarity Challenge Download","type":0,"sectionRef":"#","url":"/docs/icassp2023/icassp2023_download","content":"","keywords":"","version":"Next"},{"title":"Software‚Äã","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge Download","url":"/docs/icassp2023/icassp2023_download#software","content":" All the necessary software tools are available as a single GitHub repository.  A new version of the repository code, v0.2.0, has been released for use with the challenge. It contains a recipe for running the baseline and standard evaluation (recipes/icassp2023).  ","version":"Next","tagName":"h3"},{"title":"Data‚Äã","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge Download","url":"/docs/icassp2023/icassp2023_download#data","content":" The data is available for download here.  On the download site you will see four data packages are available,  clarity_CEC2_core.v1_1.tgz [28 GB] - metadata and dev set clarity_CEC2_train.v1_1.tgz [69 GB] - scenes for training systems clarity_CEC2_hoairs.v1_0.tgz [144 GB] - impulse responses for generating extended training data clarity_CEC2_icassp2023_eval.v1_0.tgz [6.5 GB] - the eval1 and eval2 evaluation sets (added on 2nd Feb 2023).  All participants will require the core data package. Participants using machine learning approaches will additionally require the train data package. Participants wishing to extend the training set by using our provided scene rendering tools will also require the high order ambisonic impulse responses (i.e., the hoairs package).  To unpack the data we recommend you follow the instructions in the Clarity Challenge GitHub repository. ","version":"Next","tagName":"h3"},{"title":"The ICASSP 2023 Clarity Challenge","type":0,"sectionRef":"#","url":"/docs/icassp2023/icassp2023_intro","content":"","keywords":"","version":"Next"},{"title":"Organisers‚Äã","type":1,"pageTitle":"The ICASSP 2023 Clarity Challenge","url":"/docs/icassp2023/icassp2023_intro#organisers","content":" Michael Akeroyd, Graham Naylor, University of NottinghamJon Barker, Will Bailey, Zehai Tu, University of SheffieldTrevor Cox, Simone Graetzer, University of SalfordJohn Culling, Cardiff University  ","version":"Next","tagName":"h2"},{"title":"Overview of challenge‚Äã","type":1,"pageTitle":"The ICASSP 2023 Clarity Challenge","url":"/docs/icassp2023/icassp2023_intro#overview-of-challenge","content":" Speech enhancement is a major research area with thousands of papers each year, yet only a tiny percentage of these explicitly consider improvements for listeners who have a hearing loss. Consequently, this signal processing challenge is designed to get the latest advancements in speech enhancement applied to hearing aids.  Entrants are tasked to enhance speech-in-noise for input into a hearing aid amplification stage. The hearing aid will be tuned to the hearing characteristics of particular people. Thus you can enter without in-depth knowledge of hearing aids, and just concentrate on the task of de-noising.  The scenario is listening to speech in the presence of typical domestic noise. We provide the signals captured by the microphones on a pair of behind-the-ear hearing aids and those captured at the eardrum. The target speech will be a short sentence. The interfering noises will be a mix of speech, domestic appliance noise and music. The audio includes the simulation of the acoustic of typical small living rooms.  The challenge is to improve the speech intelligibility without excessive loss of quality. To this end, entries will be evaluated using an objective metric that is an average of the Hearing Aid Speech Perception Index (HASPI) and Hearing Aid Speech Quality Index (HASQI).  ","version":"Next","tagName":"h2"},{"title":"What is be provided‚Äã","type":1,"pageTitle":"The ICASSP 2023 Clarity Challenge","url":"/docs/icassp2023/icassp2023_intro#what-is-be-provided","content":" Premixed speech + interferer scenes for training and evaluation.Databases of target sentences, along with speech, noise and music interferers.Listener characteristics, including audiograms and speech-in-noise testing.Software including tools for augmenting training data, a baseline enhancement system, a fixed hearing aid implementation and code for scoring signals using the HASPI and HASQI hearing aid metrics.  The scenario is similar to the second Clarity Enhancement Challenge but with the following key differences:  Participants are asked to focus on speech enhancement only. Hearing aid processing/simulation is not part of the challengeSpeech quality (HASQI) will be assessed in conjunction with speech intelligibility (HASPI)  This site provides access to all the software, data and information that you need to get started. ","version":"Next","tagName":"h2"},{"title":"Results","type":0,"sectionRef":"#","url":"/docs/icassp2023/icassp2023_results","content":"Results The Clarity ICASSP-2023 Enhancement Challenge is now complete. Results are shown below along with links to the system description papers. The table below reports the HASPI, HASQI and average (over HASPI and HASQI) scores for all the submitted systems. Results are shown for both Eval 1 (the simulated evaluation set) and Eval 2 (using the real recordings). Where a system ID ends with '_data', additional training data has been used. Those that end with '_hr' has exploitied the head rotation information. Links to the system reports are also provided. The Eval 1 average has been used to rank the systems. Rank\tTeam\tID\tPaper\tEval 1 (HASPI)\tEval 1 (HASQI)\tEval 1 (Average)\tEval 2 (HASPI)\tEval 2 (HASQI)\tEval 2 (Average)8\tBaseline\tBaseline 0.266\t0.128\t0.197\t0.176\t0.121\t0.149 9\tT001\tE002\tLink\t0.179\t0.093\t0.136\t0.101\t0.078\t0.09 7\tT002\tE009\tLink\t0.286\t0.161\t0.224\t0.126\t0.108\t0.117 5\tT003\tE014\tLink\t0.797\t0.414\t0.606\t0.291\t0.11\t0.201 10\tT004\tE023\tLink\t0.117\t0.047\t0.082\t0.026\t0.019\t0.018 2\tT005\tE028\tLink\t0.78\t0.526\t0.653\t0.026\t0.019\t0.022 1\tT005\tE028_data\tLink\t0.816\t0.57\t0.693\t0.249\t0.154\t0.199 4\tT006\tE029\tLink\t0.835\t0.393\t0.613\t0.256\t0.104\t0.18 3\tT006\tE029_hr\tLink\t0.838\t0.393\t0.616\t0.256\t0.103\t0.18 6\tT007\tE030\tLink\t0.729\t0.316\t0.522\t0.284\t0.132\t0.208","keywords":"","version":"Next"},{"title":"Additional Tools","type":0,"sectionRef":"#","url":"/docs/icassp2023/software/icassp2023_additional_tools","content":"","keywords":"","version":"Next"},{"title":"Hearing loss model‚Äã","type":1,"pageTitle":"Additional Tools","url":"/docs/icassp2023/software/icassp2023_additional_tools#hearing-loss-model","content":" This is an open-source python implementation of a hearing loss model developed by Brian Moore, Michael Stone and other members of the Auditory Perception Group, University of Cambridge [1, 2].  Inputs: A stereo wav audio signal, e.g., the output of the hearing aid model and audiograms for left and right ear.Outputs: The signal after simulating the hearing loss as specified by the set of audiograms (stereo wav file), &lt;scene&gt;_&lt;listener&gt;_HL-output.wav  ","version":"Next","tagName":"h2"},{"title":"Differentiable source separation and hearing aid amplification modules‚Äã","type":1,"pageTitle":"Additional Tools","url":"/docs/icassp2023/software/icassp2023_additional_tools#differentiable-source-separation-and-hearing-aid-amplification-modules","content":" The modules are from the Sheffield E009 system in CEC1. The source separation module is a multi-channel Conv-TasNet optimised with a SNR objective. The hearing aid amplification module is an FIR filter optimised with an objective, which is the combination of a differentiable approximation to the hearing loss model and a STOI loss.  Inputs: six channels of mixed signals, i.e., mixed_CH1.wav, mixed_CH2.wav, and mixed_CH3.wavOutputs: a single channel enhanced signal, therefore two source separation and amplification modules for left and right ears need to be optimised for the enhanced binaural signal.  ","version":"Next","tagName":"h2"},{"title":"Speech intelligibility model (MBSTOI)‚Äã","type":1,"pageTitle":"Additional Tools","url":"/docs/icassp2023/software/icassp2023_additional_tools#speech-intelligibility-model-mbstoi","content":" Python implementation of a binaural intelligibility model, Modified Binaural Short-Time Objective Intelligibility (MBSTOI) [3]. Note that MBSTOI requires signal time-alignment (and alignment within one-third octave bands).  Inputs: HL-model output signals, audiogram, reference target signal (i.e., the premixed target signal convolved with the BRIR with the reflections 'turned off', specified as 'target_anechoic'), (scene metadata)Outputs: predicted intelligibility score  ","version":"Next","tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Additional Tools","url":"/docs/icassp2023/software/icassp2023_additional_tools#references","content":"   Moore, B. C. J., Alcantara, J. I., Stone, M. and Glasberg, B. R., 1999. Use of a loudness model for hearing aid fitting: II. Hearing aids with multi-channel compression. British Journal of Audiology, 33(3), pp. 157-170.Nejime, Y. and Moore, B. C., 1997. Simulation of the effect of threshold elevation and loudness recruitment combined with reduced frequency selectivity on the intelligibility of speech in noise. Journal of the Acoustical Society of America, 102(1), pp. 603-615.Andersen, A. H., de Haan, J. M., Tan, Z. H. and Jensen, J., 2018. Refinement and validation of the binaural short-time objective intelligibility measure for spatially diverse conditions. Speech Communication, 102, pp. 1-13. ","version":"Next","tagName":"h2"},{"title":"ICASSP 2023 Clarity Grand Challenge Registration","type":0,"sectionRef":"#","url":"/docs/icassp2023/taking_part/icassp2023_registration","content":"ICASSP 2023 Clarity Grand Challenge Registration Teams are required to register using the form below. Please register as soon as possible. Please submit one form per team, providing a single contact email address. Once you have registered, you will receive an email confirmation with a team ID. When the submission date approaches, you will be sent an individualised link to a Google Drive for submitting materials. Loading‚Ä¶","keywords":"","version":"Next"},{"title":"Core Software","type":0,"sectionRef":"#","url":"/docs/icassp2023/software/icassp2023_core_software","content":"","keywords":"","version":"Next"},{"title":"A. Scene generator‚Äã","type":1,"pageTitle":"Core Software","url":"/docs/icassp2023/software/icassp2023_core_software#a-scene-generator","content":" Fully open-source Python code for generating hearing aid inputs for each scene  Inputs: target and interferer signals, HOA-IRs, RAVEN project (rpf) files, scene description JSON filesOutputs: Mixed target+interferer signals for each hearing aid channel, direct path (simulating a measurement close to the eardrum). Reverberated pre-mixed signals can also be optionally generated.  ","version":"Next","tagName":"h2"},{"title":"B. Hearing aid enhancement stage‚Äã","type":1,"pageTitle":"Core Software","url":"/docs/icassp2023/software/icassp2023_core_software#b-hearing-aid-enhancement-stage","content":" The hearing aid enhancement stage supplied simply reduces the six channel input to two channels by selection the 'front' microphone on each ear. This is the component that you are challenged to replace.  Inputs: 6 channel hearing aid input (3 microphones per for each ear)Outputs: An enhanced stereo signal that is passed to the amplification stage.  ","version":"Next","tagName":"h2"},{"title":"C. The hearing aid amplification stage‚Äã","type":1,"pageTitle":"Core Software","url":"/docs/icassp2023/software/icassp2023_core_software#c-the-hearing-aid-amplification-stage","content":" The hearing aid amplifier consists of a NAL-R fitting amplification stage [1] followed by a simple automatic gain compressor. It produces output signals in 16-bit wav format ready for HASPI and HASQI evaluation.  Inputs: Stereo output of the enhancement stage and audiograms to characterise the listeners.Outputs: Stereo hearing aid (HA) outputs signals.  ","version":"Next","tagName":"h2"},{"title":"D. HASPI Speech Intelligibility model‚Äã","type":1,"pageTitle":"Core Software","url":"/docs/icassp2023/software/icassp2023_core_software#d-haspi-speech-intelligibility-model","content":" Python implementation of the Hearing Aid Speech Perception Index (HASPI) [2] model which is used for objective intelligibility estimation. This will be one component of the evaluation metric.  Inputs: reference target signal (i.e., the premixed target signal convolved with the BRIR with the reflections ‚Äúturned off‚Äù, specified as ‚Äòtarget_anechoic‚Äô), HA output signals, audiogram, level reference (level in dB SPL which corresponds to 0 dB FS)Outputs: predicted intelligibility score It is important to remember that both reference target and HA output signals have to be calibrated to the same dB SPL level before calculating HASPI.  ","version":"Next","tagName":"h2"},{"title":"E. HASQI Speech Quality model‚Äã","type":1,"pageTitle":"Core Software","url":"/docs/icassp2023/software/icassp2023_core_software#e-hasqi-speech-quality-model","content":" Python implementation of the Hearing Aid Speech Quality Index (HASQI) [3] model which is used for objective quality estimation. This will be one component of the evaluation metric.  Inputs: reference target signal (i.e., the premixed target signal convolved with the BRIR with the reflections ‚Äúturned off‚Äù, specified as ‚Äòtarget_anechoic‚Äô), HA output signals, audiogram, level reference (level in dB SPL which corresponds to 0 dB FS)Outputs: predicted intelligibility score It is important to remember that both reference target and HA output signals have to be calibrated to the same dB SPL level before calculating HASPI.  ","version":"Next","tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Core Software","url":"/docs/icassp2023/software/icassp2023_core_software#references","content":"   Byrne, Denis, and Harvey Dillon. &quot;The National Acoustic Laboratories'(NAL) new procedure for selecting the gain and frequency response of a hearing aid.&quot; Ear and hearing 7.4 (1986): 257-265.Kates, J.M. and Arehart, K.H., 2021. &quot;The hearing-aid speech perception index (haspi) version 2&quot;. Speech Communication, 131, pp.35-46.Kates, J.M. and Arehart, K.H., 2014. &quot;The hearing-aid speech quality index (HASQI) version 2&quot;. Journal of the Audio Engineering Society. 62 (3): 99‚Äì117. ","version":"Next","tagName":"h2"},{"title":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","type":0,"sectionRef":"#","url":"/docs/icassp2023/taking_part/icassp2023_faq","content":"","keywords":"","version":"Next"},{"title":"Speech Intelligibility‚Äã","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"/docs/icassp2023/taking_part/icassp2023_faq#speech-intelligibility","content":" ","version":"Next","tagName":"h2"},{"title":"What is Speech Intelligibility?‚Äã","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"/docs/icassp2023/taking_part/icassp2023_faq#what-is-speech-intelligibility","content":" The term Speech Intelligibility is generally used in two different ways. It can refer to how much speech is understood by a listener, or to the number of words correctly identified by a listener as a proportion or percentage of the total number of words. In the Clarity project, we are using the latter definition, i.e., the percentage of words in a sentence that a listener identified correctly. This percentage is the target for your prediction models.  Speech intelligibility captures how a listener's ability to participate in conversation is changed when the speech signal is degraded, e.g., by background noise and room reverberation, or is processed, e.g., by a hearing aid. Your prediction model will need to incorporate a model of the hearing abilities of each listener.  ","version":"Next","tagName":"h3"},{"title":"How is Speech Intelligibility measured with listeners?‚Äã","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"/docs/icassp2023/taking_part/icassp2023_faq#how-is-speech-intelligibility-measured-with-listeners","content":" In the Clarity project, a set of listeners listen to a sentence and then say what words they heard. In this project, speech intelligibility is measured as the number of words identified correctly as a percentage of the total number of words in a sentence.  You might consider looking at other metrics, such as Word Error Rate (WER), which picks up on, e.g., where listeners insert words not in the original sentence. You might do this if you think that an estimate of WER or other metrics would help your system to estimate speech intelligibility, as defined in the Clarity project.  ","version":"Next","tagName":"h3"},{"title":"How is Speech Intelligibility objectively measured by a computer?‚Äã","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"/docs/icassp2023/taking_part/icassp2023_faq#how-is-speech-intelligibility-objectively-measured-by-a-computer","content":" When fitting a hearing aid, it would be beneficial for an audiologist to be able to use an objective measure of speech intelligibility to determine what signal processing algorithm(s) should be used to compensate for the listener's hearing impairment. Objective measures are also useful when measured speech intelligibility scores are unavailable, such as when developing a machine learning-based hearing aid algorithm or some other speech enhancement method. Another advantage of non-intrusive measures is that they do not require time-alignment of processed and reference signals.  Objective measures - or metrics - of speech intelligibility are used to allow a computer to estimate the likely performance of humans in listening tests. The main goal of entries to the prediction challenge is to produce one of these measures that performs well for listeners with hearing loss. There are two broad classes of speech intelligibility models:  Intrusive metrics (also known as double-ended) are most common. This is where the intelligibility is estimated by comparing the degraded or processed speech signal with the original clean speech signal.Non-intrusive metrics (also known as single-ended or blind) are less well developed. This is where intelligibility is estimated from the degraded or processed speech signal alone.  In the Clarity project, both types of metrics are of interest. Intrusive metrics will be more accurate in many cases. However, there are hearing aid processes where the speech content is shifted in frequency, which will defeat most current intrusive speech intelligibility metrics. We also hypothesise that there might be issues with intrusive metrics and machine learning approaches in hearing aids that revoice the original speech.  ","version":"Next","tagName":"h3"},{"title":"What speech intelligibility models already exist and what are they used for?‚Äã","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"/docs/icassp2023/taking_part/icassp2023_faq#what-speech-intelligibility-models-already-exist-and-what-are-they-used-for","content":" There aren't many speech intelligibility models that consider hearing impairment, but one that does is HASPI by Kates and Arehart. In this seminar from the first Clarity workshop, James Kates discusses speech intelligibility models with a focus on the ones he has developed. He also discusses the speech quality metric HASQI. If you're interested in using HASPI or HASQI for the challenge, James Kates has kindly made the MATLAB code and user guide available for download.    Click arrow to see synopsis. Signal degradations, such as additive noise and nonlinear distortion, can reduce the intelligibility and quality of a speech signal. Predicting intelligibility and quality for hearing aids is especially difficult since these devices may contain intentional nonlinear distortion designed to make speech more audible to a hearing-impaired listener. This speech processing often takes the form of time-varying multichannel gain adjustments. Intelligibility and quality metrics used for hearing aids and hearing-impaired listeners must therefore consider the trade-offs between audibility and distortion introduced by hearing-aid speech envelope modifications. This presentation uses the Hearing Aid Speech Perception Index (HASPI) and the Hearing Aid Speech Quality Index (HASQI) to predict intelligibility and quality, respectively. These indices incorporate a model of the auditory periphery that can be adjusted to reflect hearing loss. They have been trained on intelligibility scores and quality ratings from both normal-hearing and hearing-impaired listeners for a wide variety of signal and processing conditions. The basics of the metrics are explained, and the metrics are then used to analyse the effects of additive noise on speech, to evaluate noise suppression algorithms, and to measure differences among commercial hearing aids.  ","version":"Next","tagName":"h3"},{"title":"Hearing Loss‚Äã","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"/docs/icassp2023/taking_part/icassp2023_faq#hearing-loss","content":" There are many types of hearing loss, but the focus of the Clarity project is the hearing loss that happens with ageing. This is a form of sensorineural hearing loss.  ","version":"Next","tagName":"h2"},{"title":"How does hearing loss affect the perception of audio signals, and how do modern hearing aids process sound to help with this?‚Äã","type":1,"pageTitle":"ICASSP 2023 Clarity Challenge - Speech Enhancement for Hearing Aids FAQ","url":"/docs/icassp2023/taking_part/icassp2023_faq#how-does-hearing-loss-affect-the-perception-of-audio-signals-and-how-do-modern-hearing-aids-process-sound-to-help-with-this","content":" In this seminar from the first Clarity workshop, Karolina Smeds from ORCA Europe and WS Audiology discusses the effects of hearing loss and the hearing aid processing strategies that are typically used to counter the sensory deficits.    Click arrow to see synopsis. Hearing loss leads to several unwanted effects. Loss of audibility for soft sounds is one effect, but also when amplification is used to create audibility for soft sounds, many suprathreshold deficits remain. The most common type of hearing loss is a cochlear hearing loss, where haircells or nerve synapses in the cochlea are damaged. Ageing and noise exposure are the most common causes of cochlear hearing loss. This type of hearing loss is associated with atypical loudness perception and difficulties in noisy situations. Background noise masks for instance speech to a higher degree than for a person with healthy hair cells. This explains why listening to speech-in-noise (SPIN) is such an important topic to work on. A brief introduction to signal processing in hearing aids will be presented. With the use of frequency-specific amplification and compression (automatic gain control, AGC), hearing aids are usually doing a good job in compensating for reduced audibility and for atypical suprathreshold loudness perception. However, it is more difficult to compensate for the increased masking effect. Some examples of strategies will be presented. Finally, natural conversations in noise will be discussed. The balance between being able to have a conversation with a specific communication partner in a group of people and being able to switch attention if someone else starts to talk will be touched upon. ","version":"Next","tagName":"h3"},{"title":"Modelling the scenario","type":0,"sectionRef":"#","url":"/docs/icassp2023/data/cec2_scenario","content":"","keywords":"","version":"Next"},{"title":"The scenario‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/icassp2023/data/cec2_scenario#the-scenario","content":" We want entrants to improve speech in the presence of background noise; see Figure 1. On the left there is a person with a quantified hearing loss who is listening to speech from the target talker on the right. Both people are in a living room. There is interfering noise from a number of sources (a TV and washing machine in this case). The speech and noise are sensed by microphones on the hearing aids of the listener. The task is to take these microphone feeds and the listener‚Äôs hearing characteristics, and produce signals for the hearing aid processor that will make the speech more intelligible. We will evaluate the success of the processing using a combination of objective metrics for speech intelligibility and quality.  Figure 1. The scenario involves one talker, a listener who rotates their head, and at least two common sources of unwanted sound.  ","version":"Next","tagName":"h2"},{"title":"Baseline system and software tools‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/icassp2023/data/cec2_scenario#baseline-system-and-software-tools","content":" Challenge entrants are supplied with an end-to-end baseline system. Figure 2 shows a simplified schematic, which comprises:  Figure 2. Baseline schematic.  A scene generator (blue box) creates speech in noise (SPIN).A listener is chosen (green ellipse), so the processing can be individualised for each listener with quantified hearing characteristics.The speech is enhanced (pink box). The entrants are tasked to improve this.The hearing aid we provide then amplifies the improved speech (yellow box)The amplified and improved speech that is emitted by your hearing aid is then passed to the prediction stage (red boxes). A combination of HASPI and HASQI is the output of the objective metrics for intelligibility and quality respectively (Kates and Arehart, 2021, Kates and Arehart 2014).All software tools will be available as a single GitHub repository. The software is split into core components e.g. HASPI, HASQI, and additional tools e.g. a hearing loss model. All software is open-source and in Python.  ","version":"Next","tagName":"h2"},{"title":"Room geometry‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/icassp2023/data/cec2_scenario#room-geometry","content":" Cuboid rooms with dimensions length LLL by width WWW by height HHH.Length LLL set using a uniform probability distribution random number generator with 3&lt;L(m)‚â§83 &lt; L(m) \\le 83&lt;L(m)‚â§8.Height HHH set using a Gaussian distribution random number generator with a mean of 2.7 m and standard deviation of 0.8 m.Area L√óWL \\times WL√óW set using a Gaussian distribution random number generator with mean 17.7 m2^22 and standard deviation of 5.5 m2^22  ","version":"Next","tagName":"h2"},{"title":"Room materials‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/icassp2023/data/cec2_scenario#room-materials","content":" One of the walls of the room is randomly selected for the location of the door. The door can be at any position with the constraint of being at least 20 cm from the corner of the wall.  A window is placed on one of the other three walls. The window could be at any position of the wall but at 1.9 m height and at 0.4 m from any corner. The curtains are simulated to the side of the window. For larger rooms, a second window and curtains are simulated following a similar methodology.  A sofa is simulated at a random position as a layer on the wall and the floor. Finally, a rug is simulated at a random location on the floor.  ","version":"Next","tagName":"h2"},{"title":"The listener (receiver)‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/icassp2023/data/cec2_scenario#the-listener-receiver","content":" The listener has position, r‚Éó=(xr,yr,zr)\\vec{r} = (x_r,y_r,z_r)r=(xr‚Äã,yr‚Äã,zr‚Äã)  This is positioned within the room using uniform probability distribution random number generators for the x and y coordinates (see Figure 2 for origin location). There are constraints to ensure that the receiver is not too close to the wall:  ‚àíW/2+1‚â§xr‚â§W/2‚àí1-W/2+1 \\le x_r \\le W/2-1‚àíW/2+1‚â§xr‚Äã‚â§W/2‚àí11‚â§yr‚â§L‚àí11 \\le y_r \\le L-11‚â§yr‚Äã‚â§L‚àí1zrz_rzr‚Äã either 1.2 m (sitting) or 1.6 m (standing).  ","version":"Next","tagName":"h2"},{"title":"Head rotation‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/icassp2023/data/cec2_scenario#head-rotation","content":" The listener is initially oriented away from the target and will turn to be roughly facing the target talker around the time when the target speech starts  Orientation of listener at start of the sample ~25¬∞ from facing the target (standard deviation = 5¬∞), limited to +-2 standard deviations.Start of rotation is between -0.635 s to 0.865s (rectangular probability)The rotation lasts for 200 ms (standard deviation =10 ms)Orientation after rotation is 0-10¬∞ (random with rectangular probability distribution).  ","version":"Next","tagName":"h2"},{"title":"The target talker‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/icassp2023/data/cec2_scenario#the-target-talker","content":" ‚Äã‚ÄãThe target talker has position t‚Éó=(xt,yt,zt)\\vec{t} = (x_t,y_t,z_t)t=(xt‚Äã,yt‚Äã,zt‚Äã)  The target talker is positioned within the room using uniform probability distribution random number generators for the coordinates. Constraints ensure the target is not too close to the wall or receiver. It is set to have the same height as the receiver.  ‚àíW/2+1‚â§xt‚â§W/2‚àí1-W/2+1 \\le x_t \\le W/2-1‚àíW/2+1‚â§xt‚Äã‚â§W/2‚àí11‚â§yt‚â§L‚àí11 \\le y_t \\le L-11‚â§yt‚Äã‚â§L‚àí1‚à£r‚àít‚à£&gt;1|r-t| &gt; 1‚à£r‚àít‚à£&gt;1zt=zrz_t=z_rzt‚Äã=zr‚Äã  A speech directivity pattern is used, which is directed at the listener. The target speech starts between 1.0 and 1.5 seconds into the mixed sound files (rectangular probability distribution).  ","version":"Next","tagName":"h2"},{"title":"The interferers‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/icassp2023/data/cec2_scenario#the-interferers","content":" The interferers have position i‚Éó1,2,3=(xi,yi,zi)\\vec{i}_{1,2,3} = (x_i,y_i,z_i)i1,2,3‚Äã=(xi‚Äã,yi‚Äã,zi‚Äã)  Each interferer is modelled as an omnidirectional point source. They will be radiating: speech, noise or music. They are placed within the room using uniform probability distribution random number generators for the coordinates. The following constraints ensure the interferer is not too close to the wall or listener. However, interferers are independently positioned with no constraint on their position relative to each other. They are set to be at the same height as the listener. Note, this means that the interferers can be at any angle relative to the listener.  ‚àíW/2+1‚â§xi‚â§W/2‚àí1-W/2+1 \\le x_i \\le W/2-1‚àíW/2+1‚â§xi‚Äã‚â§W/2‚àí11‚â§yi‚â§L‚àí11 \\le y_i \\le L-11‚â§yi‚Äã‚â§L‚àí1‚à£r‚àíi‚à£&gt;1|r-i| \\gt 1‚à£r‚àíi‚à£&gt;1zi=zrz_i = z_rzi‚Äã=zr‚Äã  The interferers are present over the whole mixed sound file.  ","version":"Next","tagName":"h2"},{"title":"Signal-to-noise ratio (SNR)‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/icassp2023/data/cec2_scenario#signal-to-noise-ratio-snr","content":" The SNR of the mixtures are engineered to achieve a suitable range of speech intelligibility values. A desired signal-to-noise ratio, SNRD_DD‚Äã (dB), is chosen at random. This is generated with a uniform probability distribution between limits determined by pilot listening tests. The better ear SNR (BE_SNR) models the better ear effect in binaural listening. It is calculated for the reference channel (channel 1, which corresponds to the front microphone of the hearing aid). This value is used to scale all interferer channels. The procedure is described below.  For the reference channel,  The segment of the summed interferers that overlaps with the target (without padding), i‚Ä≤i'i‚Ä≤, and the target (without padding), t‚Ä≤t't‚Ä≤, are extractedSpeech-weighted SNRs are calculated for each ear, SNRL_LL‚Äã and SNRR_RR‚Äã: Signals i‚Ä≤i'i‚Ä≤ and t‚Ä≤t't‚Ä≤ are separately convolved with a speech-weighting filter, h (specified below).The rms is calculated for each convolved signal.SNRL_LL‚Äã and SNRR_RR‚Äã are calculated as the ratio of these rms values. The BE_SNR is selected as the maximum of the two SNRs: BE_SNR = max(SNRL_LL‚Äã and SNRR_RR‚Äã).  Then per channel,  The summed interferer signal, i, is scaled by the BE_SNR i=i√ói = i \\timesi=i√ó BE_SNR Finally, i is scaled as follows: i=i√ó10‚àíSNRD/20i = i \\times 10^{-SNR_D/20}i=i√ó10‚àíSNRD‚Äã/20  The speech-weighting filter is an FIR designed using the host window method [2, 3]. The frequency response is shown in Figure 2. The specification is:  Frequency (Hz) = [0, 150, 250, 350, 450, 4000, 4800, 5800, 7000, 8500, 9500, 22050]Magnitude of transfer function at each frequency = [0.0001, 0.0103, 0.0261, 0.0419, 0.0577, 0.0577, 0.046, 0.0343, 0.0226, 0.0110, 0.0001, 0.0001]  Figure 2, Speech weighting filter transfer function graph.  ","version":"Next","tagName":"h2"},{"title":"References‚Äã","type":1,"pageTitle":"Modelling the scenario","url":"/docs/icassp2023/data/cec2_scenario#references","content":"   Schr√∂der, D. and Vorl√§nder, M., 2011, January. RAVEN: A real-time framework for the auralization of interactive virtual environments. In Proceedings of Forum Acusticum 2011 (pp. 1541-1546). Denmark: Aalborg.Abed, A.H.M. and Cain, G.D., 1978. Low-pass digital filtering with the host windowing design technique. Radio and Electronic Engineer, 48(6), pp.293-300.Abed, A.E. and Cain, G., 1984. The host windowing technique for FIR digital filter design. IEEE transactions on acoustics, speech, and signal processing, 32(4), pp.683-694. ","version":"Next","tagName":"h2"},{"title":"Find collaborators","type":0,"sectionRef":"#","url":"/docs/icassp2023/taking_part/icassp2023_find_a_team","content":"Find collaborators If you'd like to team up with someone else to compete in the challenges, we can help. Please complete this Google form to let us know your own expertise, and what you're looking for in a collaborator. We'll then put people in contact with possible collaborators. We encourage everyone to join the Clarity Challenge‚Äôs Google group to stay updated with project news and announcements. We post in there when we have new people seeking team members (we don't share any personally-identifying details to the group). You are welcome to contact us if you have any questions about forming a team or participating in the challenge: Email the Clarity Team","keywords":"","version":"Next"},{"title":"ICASSP 2023 Grand Challenge Rules","type":0,"sectionRef":"#","url":"/docs/icassp2023/taking_part/icassp2023_rules","content":"","keywords":"","version":"Next"},{"title":"Teams‚Äã","type":1,"pageTitle":"ICASSP 2023 Grand Challenge Rules","url":"/docs/icassp2023/taking_part/icassp2023_rules#teams","content":" Teams must have pre-registered and nominated a contact person.Teams can be from one or more institutions.  ","version":"Next","tagName":"h2"},{"title":"Transparency‚Äã","type":1,"pageTitle":"ICASSP 2023 Grand Challenge Rules","url":"/docs/icassp2023/taking_part/icassp2023_rules#transparency","content":" Teams must provide a technical document of up to 2 pages describing the system/model and any external data and pre-existing tools, software and models used.We will publish all technical documents (anonymous or otherwise).Teams are encouraged ‚Äì but not required ‚Äì to provide us with access to the system/model and to make their code open source.Teams may reserve the right to be referred to using anonymous code names in the published rank ordering.  ","version":"Next","tagName":"h2"},{"title":"What information can I use?‚Äã","type":1,"pageTitle":"ICASSP 2023 Grand Challenge Rules","url":"/docs/icassp2023/taking_part/icassp2023_rules#what-information-can-i-use","content":" ","version":"Next","tagName":"h2"},{"title":"Training and development‚Äã","type":1,"pageTitle":"ICASSP 2023 Grand Challenge Rules","url":"/docs/icassp2023/taking_part/icassp2023_rules#training-and-development","content":" For training, teams can not use external data but can expand the official training data through automated modifications and remixing, i.e. data augmentation strategies. However, teams that do this must make a second submission using only the official audio files. Any audio or metadata can be used during training and development, but during evaluation, the enhancement algorithm will not have access to all of the data (see next section).  ","version":"Next","tagName":"h3"},{"title":"Evaluation‚Äã","type":1,"pageTitle":"ICASSP 2023 Grand Challenge Rules","url":"/docs/icassp2023/taking_part/icassp2023_rules#evaluation","content":" The only data that can be used by the Enhancement Processor during evaluation are  The audio input signals (the sum of the target and interferers for each hearing aid microphone).The listener characterisation (pure tone air-conduction audiograms and/or digit triple test results).The provided clean audio examples for the target talker (these will not be the same as any of the target utterances.)  ","version":"Next","tagName":"h3"},{"title":"Computational restrictions‚Äã","type":1,"pageTitle":"ICASSP 2023 Grand Challenge Rules","url":"/docs/icassp2023/taking_part/icassp2023_rules#computational-restrictions","content":" Teams may choose to use all, some or none of the parts of the baseline model.Systems must be causal; the output from the hearing aid at time t must not use any information from input samples more than 5 ms into the future (i.e., no information from input samples &gt;t+5 ms).There is no limit on computational cost.  Please see this blog post for further explanation of these last two rules about latency and computation time.  ","version":"Next","tagName":"h2"},{"title":"Submitting multiple entries‚Äã","type":1,"pageTitle":"ICASSP 2023 Grand Challenge Rules","url":"/docs/icassp2023/taking_part/icassp2023_rules#submitting-multiple-entries","content":" It is intended that there should be one submission per registered team. Submitting multiple entries is discouraged.  ","version":"Next","tagName":"h2"},{"title":"Evaluation of systems‚Äã","type":1,"pageTitle":"ICASSP 2023 Grand Challenge Rules","url":"/docs/icassp2023/taking_part/icassp2023_rules#evaluation-of-systems","content":" Each signal will be scored using the average of its HASPI and HASQI scores. A system score will then be computed by averaging over the evaluation set. Separate scores will be computed for the real and simulated evaluation sets, along with a summary score formed by their average. Systems will be ranked according to their summary scores with all three (real, simulated and summary) scores being reported.  ","version":"Next","tagName":"h2"},{"title":"Intellectual property‚Äã","type":1,"pageTitle":"ICASSP 2023 Grand Challenge Rules","url":"/docs/icassp2023/taking_part/icassp2023_rules#intellectual-property","content":" The following terms apply to participation in this machine learning challenge (‚ÄúChallenge‚Äù). Entrants may create original solutions, prototypes, datasets, scripts, or other content, materials, discoveries or inventions (a ‚ÄúSubmission‚Äù). The Challenge is organised by the Challenge Organiser.  Entrants retain ownership of all intellectual and industrial property rights (including moral rights) in and to Submissions.  As a condition of submission, Entrant grants the Challenge Organiser, its subsidiaries, agents and partner companies, a perpetual, irrevocable, worldwide, royalty-free, and non-exclusive licence to use, reproduce, adapt, modify, publish, distribute, publicly perform, create a derivative work from, and publicly display the Submission.  Entrants provide Submissions on an ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. ","version":"Next","tagName":"h2"},{"title":"ICASSP 2023 Submission","type":0,"sectionRef":"#","url":"/docs/icassp2023/taking_part/icassp2023_submission","content":"","keywords":"","version":"Next"},{"title":"What evaluation data is provided?‚Äã","type":1,"pageTitle":"ICASSP 2023 Submission","url":"/docs/icassp2023/taking_part/icassp2023_submission#what-evaluation-data-is-provided","content":" There will be two sets of evaluation data: i) the simulate set consisting of 1500 scenes generated in the same way as the training and development data (eval1), ii) the real data consisting of real acoustic mixtures (eval2). For details see the data description page.  For each scene, you are provided with the signals received at each of the three microphones on the left and right hearing aid device. You will also be provided with JSON or csv formatted metadata consisting of  the audiograms for a set of listeners anda mapping of which listeners will listen to which scenes.  There will also be some clean example utterances from the target talker, that are not the same as the target utterance, but which can be used to identify the target talker, i.e., to disambiguate scenes in which other speakers are present.  For HASPI/HASQI evaluation, there will be one listener per scene and the scene-listener mapping will be the same for all teams.  ","version":"Next","tagName":"h2"},{"title":"What audio do I need to submit?‚Äã","type":1,"pageTitle":"ICASSP 2023 Submission","url":"/docs/icassp2023/taking_part/icassp2023_submission#what-audio-do-i-need-to-submit","content":" You must submit the stereo audio signals produced at the output of your enhancement stage, which the organisers will process by the hearing aid amplification stage and the HASPI/HASQI evaluation metric. Signals should be submitted as stereo, floating point wav format signals, at the same sampling rate as the signals provided  ","version":"Next","tagName":"h2"},{"title":"Naming and packaging signals‚Äã","type":1,"pageTitle":"ICASSP 2023 Submission","url":"/docs/icassp2023/taking_part/icassp2023_submission#naming-and-packaging-signals","content":" Your processed signals should be named using the conventions used by the baseline system, i.e., &lt;Scene ID&gt;_&lt;Listener ID&gt;_enhanced.wav and explained on the data page.  Place the processed signals for the two sets into separate directories named eval1 and eval2.  These should be placed in a directory whose name is the unique team ID that you will be sent, e.g., ICASSP2023_E001 and then packaged using zip or tar or any standard packaging tool, e.g., to make a packaged file called &lt;TEAM_ID&gt;.zip  The packaged file will have the following structure,  ICASSP2023_E001 ‚îú‚îÄ‚îÄ eval1 (1500 processed signals) ‚îî‚îÄ‚îÄ eval2 (1500 processed signals)   The resulting file should be about 4 GB.  Upload the packaged data to the Google Drive link that you will have been sent.  ","version":"Next","tagName":"h2"},{"title":"Using head rotation data and/or extended training data‚Äã","type":1,"pageTitle":"ICASSP 2023 Submission","url":"/docs/icassp2023/taking_part/icassp2023_submission#using-head-rotation-data-andor-extended-training-data","content":" We would like to be able to separately evaluate the benefit of using the head rotation and extra training data, so in accordance with the challenge rules,  If you have trained on data which was not included in the core database, then please also provide outputs of a system trained only with the standard data. If you have made use of the head rotation data you should also provide outputs of an equivalent system that does not use the head rotation data.  If you have used extra training data and/or the head rotation data, then please package the outputs separately using the following naming convention,  ‚ÄπTEAM_ID‚Ä∫.zip - standard training data and no head rotation (all teams)  ‚ÄπTEAM_ID‚Ä∫_hr.zip - standard data and using head rotation  ‚ÄπTEAM_ID‚Ä∫_data.zip - extended training data without using head rotation  ‚ÄπTEAM_ID‚Ä∫_hr_data.zip - extended training data and using head rotation  ","version":"Next","tagName":"h2"},{"title":"Technical report‚Äã","type":1,"pageTitle":"ICASSP 2023 Submission","url":"/docs/icassp2023/taking_part/icassp2023_submission#technical-report","content":" For every entry, a technical report needs to be uploaded to the Google Drive along with your evaluation signals - see here for deadline. The draft needs to be sufficiently complete for us to judge whether your system is compliant with the challenge rules.Your report should include an abstract and introduction and sections on experimental setup/methodology including system information and model/network architecture, evaluation/results, discussion, conclusion and references. Please provide an estimation of the computational resources needed. You must describe any external data and pre-existing tools, software and models used.The report can be placed in the Google Drive alongside your data.The top five systems will be invited to submit a paper to the ICASSP 2023 special session.  ","version":"Next","tagName":"h2"},{"title":"How will intellectual property be handled?‚Äã","type":1,"pageTitle":"ICASSP 2023 Submission","url":"/docs/icassp2023/taking_part/icassp2023_submission#how-will-intellectual-property-be-handled","content":" See here under Intellectual Property.  ","version":"Next","tagName":"h2"},{"title":"Where do I submit the signals?‚Äã","type":1,"pageTitle":"ICASSP 2023 Submission","url":"/docs/icassp2023/taking_part/icassp2023_submission#where-do-i-submit-the-signals","content":" When you have registered you will receive a link to a Google Drive to which you will be able to securely upload your signals. We also encourage you to submit your enhancement code via this link.  Materials uploaded will be visible to the Clarity Team but not to other entrants.  warning Note, in order to use the Google Drive you will need to have a Google account. If you anticipate problems using Google then please make arrangements to send us the materials by other means, e.g., via a service such as WeTransfer or similar. ","version":"Next","tagName":"h2"},{"title":"Learning","type":0,"sectionRef":"#","url":"/docs/learning/learning_intro","content":"","keywords":"","version":"Next"},{"title":"Speech Intelligibility‚Äã","type":1,"pageTitle":"Learning","url":"/docs/learning/learning_intro#speech-intelligibility","content":" ","version":"Next","tagName":"h2"},{"title":"What is Speech Intelligibility?‚Äã","type":1,"pageTitle":"Learning","url":"/docs/learning/learning_intro#what-is-speech-intelligibility","content":" The term Speech Intelligibility is generally used in two different ways. It can refer to how much speech is understood by a listener, or to the number of words correctly identified by a listener as a proportion or percentage of the total number of words. In the Clarity project, we are using the latter definition, i.e., the percentage of words in a sentence that a listener identified correctly. This percentage is the target for your prediction models.  Speech intelligibility captures how a listener's ability to participate in conversation is changed when the speech signal is degraded, e.g., by background noise and room reverberation, or is processed, e.g., by a hearing aid. Your prediction model will need to incorporate a model of the hearing abilities of each listener.  ","version":"Next","tagName":"h3"},{"title":"How is Speech Intelligibility measured with listeners?‚Äã","type":1,"pageTitle":"Learning","url":"/docs/learning/learning_intro#how-is-speech-intelligibility-measured-with-listeners","content":" In the Clarity project, a set of listeners listen to a sentence and then say what words they heard. In this project, speech intelligibility is measured as the number of words identified correctly as a percentage of the total number of words in a sentence.  You might consider looking at other metrics, such as Word Error Rate (WER), which picks up on, e.g., where listeners insert words not in the original sentence. You might do this if you think that an estimate of WER or other metrics would help your system to estimate speech intelligibility, as defined in the Clarity project.  ","version":"Next","tagName":"h3"},{"title":"How is Speech Intelligibility objectively measured by a computer?‚Äã","type":1,"pageTitle":"Learning","url":"/docs/learning/learning_intro#how-is-speech-intelligibility-objectively-measured-by-a-computer","content":" When fitting a hearing aid, it would be beneficial for an audiologist to be able to use an objective measure of speech intelligibility to determine what signal processing algorithm(s) should be used to compensate for the listener's hearing impairment. Objective measures are also useful when measured speech intelligibility scores are unavailable, such as when developing a machine learning-based hearing aid algorithm or some other speech enhancement method. Another advantage of non-intrusive measures is that they do not require time-alignment of processed and reference signals.  Objective measures - or metrics - of speech intelligibility are used to allow a computer to estimate the likely performance of humans in listening tests. The main goal of entries to the prediction challenge is to produce one of these measures that performs well for listeners with hearing loss. There are two broad classes of speech intelligibility models:  Intrusive metrics (also known as double-ended) are most common. This is where the intelligibility is estimated by comparing the degraded or processed speech signal with the original clean speech signal.Non-intrusive metrics (also known as single-ended or blind) are less well developed. This is where intelligibility is estimated from the degraded or processed speech signal alone.  In the Clarity project, both types of metrics are of interest. Intrusive metrics will be more accurate in many cases. However, there are hearing aid processes where the speech content is shifted in frequency, which will defeat most current intrusive speech intelligibility metrics. We also hypothesise that there might be issues with intrusive metrics and machine learning approaches in hearing aids that revoice the original speech.  ","version":"Next","tagName":"h3"},{"title":"What speech intelligibility models already exist and what are they used for?‚Äã","type":1,"pageTitle":"Learning","url":"/docs/learning/learning_intro#what-speech-intelligibility-models-already-exist-and-what-are-they-used-for","content":" There aren't many speech intelligibility models that consider hearing impairment, but one that does is HASPI by Kates and Arehart. In this seminar from the first Clarity workshop, James Kates discusses speech intelligibility models with a focus on the ones he has developed. He also discusses the speech quality metric HASQI. If you're interested in using HASPI or HASQI for the challenge, James Kates has kindly made the MATLAB code and user guide available for download.    Click arrow to see synopsis. Signal degradations, such as additive noise and nonlinear distortion, can reduce the intelligibility and quality of a speech signal. Predicting intelligibility and quality for hearing aids is especially difficult since these devices may contain intentional nonlinear distortion designed to make speech more audible to a hearing-impaired listener. This speech processing often takes the form of time-varying multichannel gain adjustments. Intelligibility and quality metrics used for hearing aids and hearing-impaired listeners must therefore consider the trade-offs between audibility and distortion introduced by hearing-aid speech envelope modifications. This presentation uses the Hearing Aid Speech Perception Index (HASPI) and the Hearing Aid Speech Quality Index (HASQI) to predict intelligibility and quality, respectively. These indices incorporate a model of the auditory periphery that can be adjusted to reflect hearing loss. They have been trained on intelligibility scores and quality ratings from both normal-hearing and hearing-impaired listeners for a wide variety of signal and processing conditions. The basics of the metrics are explained, and the metrics are then used to analyse the effects of additive noise on speech, to evaluate noise suppression algorithms, and to measure differences among commercial hearing aids.  ","version":"Next","tagName":"h3"},{"title":"Hearing Loss‚Äã","type":1,"pageTitle":"Learning","url":"/docs/learning/learning_intro#hearing-loss","content":" There are many types of hearing loss, but the focus of the Clarity project is the hearing loss that happens with ageing. This is a form of sensorineural hearing loss.  ","version":"Next","tagName":"h2"},{"title":"How does hearing loss affect the perception of audio signals, and how do modern hearing aids process sound to help with this?‚Äã","type":1,"pageTitle":"Learning","url":"/docs/learning/learning_intro#how-does-hearing-loss-affect-the-perception-of-audio-signals-and-how-do-modern-hearing-aids-process-sound-to-help-with-this","content":" In this seminar from the first Clarity workshop, Karolina Smeds from ORCA Europe and WS Audiology discusses the effects of hearing loss and the hearing aid processing strategies that are typically used to counter the sensory deficits.    Details Click arrow to see synposis. Hearing loss leads to several unwanted effects. Loss of audibility for soft sounds is one effect, but also when amplification is used to create audibility for soft sounds, many suprathreshold deficits remain. The most common type of hearing loss is a cochlear hearing loss, where haircells or nerve synapses in the cochlea are damaged. Ageing and noise exposure are the most common causes of cochlear hearing loss. This type of hearing loss is associated with atypical loudness perception and difficulties in noisy situations. Background noise masks for instance speech to a higher degree than for a person with healthy hair cells. This explains why listening to speech-in-noise (SPIN) is such an important topic to work on. A brief introduction to signal processing in hearing aids will be presented. With the use of frequency-specific amplification and compression (automatic gain control, AGC), hearing aids are usually doing a good job in compensating for reduced audibility and for atypical suprathreshold loudness perception. However, it is more difficult to compensate for the increased masking effect. Some examples of strategies will be presented. Finally, natural conversations in noise will be discussed. The balance between being able to have a conversation with a specific communication partner in a group of people and being able to switch attention if someone else starts to talk will be touched upon.  ","version":"Next","tagName":"h3"},{"title":"Prediction model‚Äã","type":1,"pageTitle":"Learning","url":"/docs/learning/learning_intro#prediction-model","content":" ","version":"Next","tagName":"h2"},{"title":"Do I have to use a separate hearing loss model?‚Äã","type":1,"pageTitle":"Learning","url":"/docs/learning/learning_intro#do-i-have-to-use-a-separate-hearing-loss-model","content":" No is the short answer! In the baseline, we've used the Cambridge hearing loss model and a separate binaural speech intelligibility model. Another approach would be to create a single model that encapsulates the combined effects of hearing loss and speech perception.  ","version":"Next","tagName":"h3"},{"title":"What should the output of my prediction model be?‚Äã","type":1,"pageTitle":"Learning","url":"/docs/learning/learning_intro#what-should-the-output-of-my-prediction-model-be","content":" The output should include a predicted speech intelligibility score per input signal, specifically, an estimate of the number of words correct as a percentage of the total number of words in the signal.  ","version":"Next","tagName":"h3"},{"title":"Data‚Äã","type":1,"pageTitle":"Learning","url":"/docs/learning/learning_intro#data","content":" ","version":"Next","tagName":"h2"},{"title":"Do you have suggestions for expanding the training data?‚Äã","type":1,"pageTitle":"Learning","url":"/docs/learning/learning_intro#do-you-have-suggestions-for-expanding-the-training-data","content":" The prediction challenge data is limited by having to get the ground truth from listening tests on people with a hearing loss. We look forward to seeing what approaches teams use to help overcome this limitation, such as using unsurpervised models, data augmentation or generating additional ground truth data using a pre-existing model. The baseline model includes a hearing loss and speech intelligibility model that could be used for creating additional pre-training data. There are other models that you might consider where code is available. None has been checked by the Clarity team.  Katerina Zmolikova has made her Pytorch version of the baseline hearing impairment and speech intelligibility model available. Both model fit a neural network framework, are faster but more approximate (see graphs on github).HASQI and HASPI are quality and speech intelligibility metrics designed to work for people with a hearing impairment. James Kates explains more about these above. MATLAB code HASPI v2 and HASQI v2 are available, along with the user guide.STOI-Net: A Deep Learning based Non-Intrusive Speech Intelligibility Assessment Model by Ryandhimas Zezario et al. is monaural and non-intrusive using Python, Keras and TensorFlow. It doesn't model the effect of hearing loss. An alternative is Asger Heidemann Andersen's MATLAB code.  ","version":"Next","tagName":"h3"},{"title":"Missing data‚Äã","type":1,"pageTitle":"Learning","url":"/docs/learning/learning_intro#missing-data","content":" We have audiograms for all our listening panel. But for other characterisations of hearing, only some of the panel have provided data. Therefore there is missing data that has to be dealt with.  One approach to the missing data is to just ignore it and just use the audiograms. The problem with this approach is that audiograms only quantifies the hearing threshold, and our speech in noise audio samples were not played that quietly. Nevertheless, audiograms are the most common way of characterising hearing loss. Alternatively, a method to use the partial data could be developed, and we expect this would help with speech intelligibility prediction. You will find plenty of data science blog posts, e.g. towards data science discussing different approaches.  A key question is whether the missing data is 'missing at random' i.e. is the distribution of the missing data expected to be the same as that of the present data? For the prediction challenge, this would mean the missing triple-digit-test values are coming from some random sample of the listeners, who'd be no different from the listeners who did complete the triple-digital-test. Unfortunately, this might not be true, because the failure to complete the triple-digit-tests could well correlate with hearing loss (e.g. maybe older people with more hearing loss were less likely to do the test). The Clarity data is probably 'missing not at random'.  One simple solution is to delete examples with missing data, but the loss of so much data probably makes this undesirable.  A more sophisticated approach is to fill gaps in data via imputation i.e. first estimate values for the missing data and then treat the dataset as complete. A couple of simple approaches for imputation are: (i) use the mean value from the dataset for missing values, and (ii) create a model to estimate the missing data from the audiograms. There are other approaches in data science that could be exploited such as coding the missing values into a 'N/A' category within the input data. ","version":"Next","tagName":"h3"},{"title":"ICASSP 2023 Data","type":0,"sectionRef":"#","url":"/docs/icassp2023/data/icassp2023_data","content":"","keywords":"","version":"Next"},{"title":"A. Training, development and evaluation data‚Äã","type":1,"pageTitle":"ICASSP 2023 Data","url":"/docs/icassp2023/data/icassp2023_data#a-training-development-and-evaluation-data","content":" The dataset of 10,000 simulated scenes is split into three sets:  6000 training scenes (available now)2500 development scenes (available now)1500 evaluation scenes (released 1st Feb. 2023)  In addition there will be:  A secondary 'real data' evaluation set that will be based on real ecologically-valid recordings and so can highlight the generalizability of the entrants‚Äô approaches beyond the simulations (released 1st February 2023). More information.  ","version":"Next","tagName":"h2"},{"title":"B. The scene dataset‚Äã","type":1,"pageTitle":"ICASSP 2023 Data","url":"/docs/icassp2023/data/icassp2023_data#b-the-scene-dataset","content":" For the dataset of 10,000 simulated scenes  Each scene corresponds to a unique target utterance and unique segment(s) of noise from the interferers.The training, development and evaluation sets are disjoint with respect to the target speakers.Sets are balanced for the gender of the target talker.Entrants must not use the development or evaluation data sets for training.The system submitted should be chosen on the evidence provided by the development set.  For evaluation  The final ranking will be performed with the (held-out) evaluation sets.Neither evaluation datasets (simulation nor real) have been used in previous Clarity challenges.    The secondary 'real data' evaluation set will be made using real acoustic mixtures but using loudspeaker playback of target talkers so that the reference speech can be extracted as needed by the objective metrics.  For the training and development set, entrants have access to a diverse range of signals and metadata, with the most important being:  The hearing aid microphone signalsThe hearing characteristics of the listener (e.g. audiogram)The anechoic target reference and interferer signals.  For training, teams can not use external data but can expand the official training data through automated modifications and remixing, i.e. data augmentation strategies. However, teams that do this must make a second submission using only the official audio files.  For evaluation, the data available is more limited, i.e.,  The hearing aid microphone signalsThe hearing characteristics of the listener (e.g. audiogram)The anechoic target reference signal which will be used by the organisers but not released to entrants.  High-Order Ambisonic Impulse Responses (HOA-IRs) and Head-Related Impulse Response (HRIRs) are used to model how the sound is altered as it propagates through the room and interacts with the head. See the page on scene generation for more details.  Time-domain acoustic signals are generated for:  A hearing aid with 3 microphone inputs (front, mid, rear). The hearing aid has a Behind-The-Ear (BTE) form factor; see Figure 1. The distance between microphones is approx. 7.6 mm. The properties of the tube and ear mould are not considered.Close to the eardrum.The anechoic target reference (front microphone).  Figure 1. Front (Fr), Middle (Mid) and Rear microphones on a BTE hearing aid form.  Head Related Impulse Responses (HRIRs) are used to model how sound is altered as it propagates in a free-field and interacts with the head (i.e., no room is included). These are taken from the OlHeadHRTF database with permission. These include HRIRs for human heads and for three types of head-and-torso simulator/mannekin. The eardrum HRIRs (labelled ED) are for a position close to the eardrum of the open ear.  rpf files and ac files are specification files for the geometric room acoustic model that include a complete description of the room, both in terms of geometry and room materials.  ","version":"Next","tagName":"h2"},{"title":"B.1 Training data‚Äã","type":1,"pageTitle":"ICASSP 2023 Data","url":"/docs/icassp2023/data/icassp2023_data#b1-training-data","content":" For each scene in the training data the following signals and metadata are available:  The target and interferer HOA-IRs (4 pairs: front, mid, rear and eardrum for left and right ears).The mono target and interferer signals (pre-convolution).For each hearing aid microphone (channels 1-3 where channel 1 is front, channel 2 is mid and channel 3 is rear) and a position close to the eardrum (channel 0): The target convolved with the appropriate HOA-IRs and downmixed;The interferers convolved with the appropriate HOA-IRs and downmixed;The sum of the target and interferer convolved with the appropriate HOA-IRs and downmixed; (i.e. the noisy signals that would be received by the hearing aid) The target convolved with the anechoic HOA-IRs and downmixed for channel 1 for each ear (‚Äòtarget_anechoic‚Äô). For use as a reference when computing HASPI scores.Metadata describing the scene: a JSON file containing, e.g., the filenames of the sources, the location of the sources, the viewvector of the target source, the location and viewvector of the receiver, the room dimensions (see specification below), and the room number, which corresponds to the RAVEN BRIR, rpf and ac files.A signal describing the head rotation (i.e. azimuthal angle at each sample)  ","version":"Next","tagName":"h3"},{"title":"B.2 Development data‚Äã","type":1,"pageTitle":"ICASSP 2023 Data","url":"/docs/icassp2023/data/icassp2023_data#b2-development-data","content":" This is made available to allow you to fully examine the performance of your system. Ground truth data (i.e., the premixed target and interferers are available in the development set)  Development data also contains target speaker adaptation sentences, i.e., four utterances from each of the target speakers. These will also be available in the evaluation data. i.e., systems can use these utterances in conjunction with the known target ID to inform their system of the which speaker in the scene should be attended.  Note, that the data available for the evaluation will be much more limited, e.g. it will not contain premixed ground truth signals or scene metadata, (see Section B.3).  When using the development data for evaluation, your hearing aid enhancement model should only be using the types of data available in the evaluation data set (see below).  ","version":"Next","tagName":"h3"},{"title":"B.3 Simulated Evaluation data (eval1)‚Äã","type":1,"pageTitle":"ICASSP 2023 Data","url":"/docs/icassp2023/data/icassp2023_data#b3-simulated-evaluation-data--eval1","content":" The following data will only be available:  Audio: the sum of the target and interferers for each hearing aid microphone.The ID of the listener who will be auditioning the processed scene.The listener characterisation data for these listeners.ID of target talker and a few examples of clean audio that are not the same as the target utterance.The head rotation signal, i.e. as might be recovered from hearing aid motion sensors. (Systems can use this signal but should also be evaluated without using it.)Speaker adaptation sentence - 4 clean utterances for each target speaker.  One challenge will be identifying the target talker from the hearing aid microphone signals. There are two possibilities:  The ID of the target talker is given with examples of clean audio. This would allow an algorithm to learn characteristics of the target talker to then help it identify the voice in the mixture.The azimuth of the target and the starting time of the utterance are both roughly known from the scene generation metadata statistics.  These two approaches mimic what is available to human listeners. They might focus on a known voice or they might use visual cues to know roughly where and when someone is talking.  ","version":"Next","tagName":"h3"},{"title":"B.4 Real Evaluation data (eval2)‚Äã","type":1,"pageTitle":"ICASSP 2023 Data","url":"/docs/icassp2023/data/icassp2023_data#b4-real-evaluation-data-eval2","content":" The following data will only be available:  Audio: the sum of the target and interferers for each hearing aid microphone.The ID of the listener who will be auditioning the processed scene.The listener characterisation data for these listeners.ID of target talker and a few examples of clean audio that are not the same as the target utterance.Speaker adaptation sentence - 4 clean utterances for each target speaker.Further details to be confirmed.  ","version":"Next","tagName":"h3"},{"title":"C Listener data‚Äã","type":1,"pageTitle":"ICASSP 2023 Data","url":"/docs/icassp2023/data/icassp2023_data#c-listener-data","content":" We will provide metadata characterising the hearing abilities of the listeners so the audio signals you generate for evaluation can be individualised to the specific listeners who will be hearing them.  The same types of data are available for training, development and evaluation.  A panel of hearing-aided listeners will be recruited for evaluation. They will be experienced bilateral hearing-aid users: they use two hearing aids but the hearing loss may be asymmetrical. The average pure tone air-conduction hearing loss will be between 25 and about 60 dB in the better ear. They will be fluent in British English.  The quantification of the listeners‚Äô hearing is done with:  Left and right pure tone air-conduction audiograms. These measure the threshold at which people can hear a pure-tone sound.Results from the DTT (digit-triplet test, also known as a triple digit test)‚Äã  The audiogram is the standard clinical measurement of hearing ability. It‚Äôs the pure-tone threshold of hearing in each ear, measured in quiet in a sound booth. The procedure is standardized e.g., British Society of Audiology Recommended Procedure. Typically it‚Äôs measured at octave frequencies and important intermediate frequencies.The values of the audiogram defines how much gain the hearing aid needs to apply, with the calculation typically done by one of a group of &quot;prescription rules&quot;, e.g. CAMFIT, NAL-NL2 or DSL .  Note that the scale of an audiogram is in ‚ÄúdB HL‚Äù = ‚ÄúdB Hearing Level‚Äù. This is not dB SPL; instead, it‚Äôs relative to an international standard such that 0-dB is ‚Äúnormal hearing‚Äù at every frequency. For background see Why the Audiogram Is Upside-down | The Hearing Review and The Quest for Audiometric Zero | The Hearing Review  The DTT is an adaptive test of speech-in-noise ability. In each trial a listener hears three spoken digits (e.g. 3-6-1) against a background of noise at a given signal-to-noise-ratio (SNR). The task is to respond on a keypad with those three digits in the order they were presented. If the listener gets all three correct, then the SNR is reduced for the next trial so making it slightly harder. If the listener makes any mistake (i.e., any digit wrong, or the order wrong) then the SNR is increased, so making the next trial slightly easier. The test carries on trial-by-trial. The test asymptotes to the SNR at which the participant is equally likely to get all three correct or not, with a few tens of trials needed to get an acceptable result. DTT tests are now used world-wide to measure hearing as they are easy to make in any local language, to explain to participants and to do, and moreover can be done over the internet or telephone as they measure a relative threshold (signal-to-noise ratio), not an absolute threshold in dB SPL. Listeners are encouraged to set a volume that is comfortable and that does not distort or crackle, but is not too quiet.  This paper is a recent scoping review of the field. The particular version we used is Vlaming et al.'s high-frequency DTT, which uses a high-pass noise as the masker. Ours starts at -14 dB SNR, goes up/down at 2 dB steps per trial, and continues for 40 trials.  In the datafile, an average of the SNR for the last 30 trials is provided (labelled 'threshold'). For reference, the SNRs are supplied for each trial as well. The very first trial is practice and is not scored.  ","version":"Next","tagName":"h2"},{"title":"D Data file formats and naming conventions‚Äã","type":1,"pageTitle":"ICASSP 2023 Data","url":"/docs/icassp2023/data/icassp2023_data#d-data-file-formats-and-naming-conventions","content":" ","version":"Next","tagName":"h2"},{"title":"D.1 Abbreviations used in filenames‚Äã","type":1,"pageTitle":"ICASSP 2023 Data","url":"/docs/icassp2023/data/icassp2023_data#d1-abbreviations-used-in-filenames","content":" The following abbreviations are used consistently throughout the filenames and references in the metadata.  R ‚Äì ‚Äúroom‚Äù: e.g., ‚ÄúR02678‚Äù # Room ID linking to RAVEN rpf fileS ‚Äì ‚Äúscene‚Äù: e.g., S00121 # Scene ID for a particular setup in a room I.e., room + choice of target and interferer signalsBNC ‚Äì BNC sentence identifier e.g. BNC_A06_01702CH ‚Äì CH0 ‚Äì eardrum signalCH1 ‚Äì front signal, hearing aid channelCH2 ‚Äì middle signal, hearing aid channelCH3 ‚Äì rear signal, hearing aid channel I/i1 ‚Äì Interferer, i.e., noise or sentence ID for the interferer/maskerT ‚Äì talker who produced the target speech sentencesL ‚Äì listenerE ‚Äì entrant (identifying a team participating in the challenge)t ‚Äì target (used in BRIRs and RAVEN project ‚Äòrpf‚Äô files)  ","version":"Next","tagName":"h3"},{"title":"D.2 General‚Äã","type":1,"pageTitle":"ICASSP 2023 Data","url":"/docs/icassp2023/data/icassp2023_data#d2-general","content":" Audio and HOA-IRs will be 44.1 kHz 32-bit wav files in either mono or stereo as appropriate.Where stereo signals are provided the two channels represent the left (0) and right (1) signals of the ear or hearing aid microphones.0 dB FS in the audio signals corresponds to 100 dB SPL.Metadata will be stored in JSON or csv format as appropriate with the exception of Room descriptions are stored as RAVEN project ‚Äòrpf‚Äô configuration files and ‚Äòac‚Äô files. (However, key details are reflected in the scene.json files) Signals are saved within the Python code as 32-bit floating point by default.Output signals for the listening tests will be required to be in 16-bit format.  ","version":"Next","tagName":"h3"},{"title":"D.3 Prompt and transcription data‚Äã","type":1,"pageTitle":"ICASSP 2023 Data","url":"/docs/icassp2023/data/icassp2023_data#d3-prompt-and-transcription-data","content":" The following text is available for the target speech:  Prompts are the text that was given to the talkers to say.‚ÄòDot‚Äô transcriptions contain the text as it was spoken in a form more suitable for scoring tools.These are stored in the master json metadata file.  ","version":"Next","tagName":"h3"},{"title":"D.4 Source audio files‚Äã","type":1,"pageTitle":"ICASSP 2023 Data","url":"/docs/icassp2023/data/icassp2023_data#d4-source-audio-files","content":" Wav files containing the original source materials. Original target sentence recordings:  &lt;Talker ID&gt;_&lt;BNC sentence identifier&gt;.wav  ","version":"Next","tagName":"h3"},{"title":"D.5 Preprocessed scene signals‚Äã","type":1,"pageTitle":"ICASSP 2023 Data","url":"/docs/icassp2023/data/icassp2023_data#d5-preprocessed-scene-signals","content":" Audio files storing the signals picked up by the hearing aid microphone that are ready for processing. Separate signals are generated for each hearing aid microphone pair or ‚Äòchannel‚Äô.  &lt;Scene ID&gt;_target_&lt;Channel ID&gt;.wav&lt;Scene ID&gt;_interferer_&lt;Channel ID&gt;.wav&lt;Scene ID&gt;_mixed_&lt;Channel ID&gt;.wav&lt;Scene ID&gt;_target_anechoic.wav - at hearing device front microphone&lt;Scene ID&gt;_hr.wav - head rotation signal  Scene ID ‚Äì S00001 to S10000  S followed by 5 digit integer with 0 pre-padding  Channel ID  CH0 ‚Äì Eardrum signalCH1 ‚Äì Hearing aid front microphoneCH2 ‚Äì Hearing aid middle microphoneCH3 ‚Äì Hearing aid rear microphone  The anechoic signal is the signal that will be used as the referernce in the HASPI evaluation.  The head rotation signal indicates the precise azimuthal angle of the head at each sample. It is stored as a floating point wav file with values between -1 and +1 where the range maps linearly from -180 degrees to +180 degrees. Teams are free to use this signal in their hearing aid algorithms, but if you do so we will ask you to also submit a version of your system that does not use it, so that the benefit of known head motion can be measured.  ","version":"Next","tagName":"h3"},{"title":"D.6 Enhanced signals‚Äã","type":1,"pageTitle":"ICASSP 2023 Data","url":"/docs/icassp2023/data/icassp2023_data#d6-enhanced-signals","content":" The signals that are output by the baseline enhancement algorithm.  &lt;Scene ID&gt;_&lt;Listener ID&gt;_enhanced.wav # Enhancement output signal (i.e., as submitted by the challenge entrants)  Listener ID ‚Äì ID of the listener panel member, e.g., L001 to L100 for initial ‚Äòpseudo-listeners‚Äô, etc.  ","version":"Next","tagName":"h3"},{"title":"D.7 Hearing-aid output signals‚Äã","type":1,"pageTitle":"ICASSP 2023 Data","url":"/docs/icassp2023/data/icassp2023_data#d7-hearing-aid-output-signals","content":" &lt;Scene ID&gt;_&lt;Listener ID&gt;_HA-output.wav # i.e., the enhanced signals after processing with the supplied hearing aid amplification.  Listener ID ‚Äì ID of the listener panel member, e.g., L001 to L100 for initial ‚Äòpseudo-listeners‚Äô, etc.  ","version":"Next","tagName":"h3"},{"title":"D.8 Room metadata‚Äã","type":1,"pageTitle":"ICASSP 2023 Data","url":"/docs/icassp2023/data/icassp2023_data#d8-room-metadata","content":" JSON file containing the description of a room. This is the data from which the ambisonic room impulse response are generated. It stores the fixed room, listener, target and interferer geometry but does not specify the dynamic factors (e.g. signals, SNRs, head movements etc) that are needed to fully define a scene.  rooms.json [ { &quot;name&quot;: &quot;R00001&quot;, // ID of room linking to RAVEN rpf and ac files &quot;dimensions&quot;: &quot;6.9933x3x3&quot;, // Room dimensions in metres &quot;target&quot;: { // target positions (x,y,z) and view vectors (look directions, x,y,z) &quot;position&quot;: [-0.3, 2.4, 1.2], &quot;view_vector&quot;: [0.071, 0.997, 0.0], }, &quot;listener&quot;: { &quot;position&quot;: [-0.1, 5.2, 1.2], &quot;view_vector&quot;: [0.071, 0.997, 0.0], }, &quot;interferers&quot;: [ { &quot;position&quot;: [0.4, 4.0, 1.2], }, { // etc, up to three interferers } ], }, ... ]   ","version":"Next","tagName":"h3"},{"title":"D.9 Scene metadata‚Äã","type":1,"pageTitle":"ICASSP 2023 Data","url":"/docs/icassp2023/data/icassp2023_data#d9-scene-metadata","content":" JSON file containing a description of the scene. It is a list of dictionaries with each entry representing a unique scene. A scene can be considered to be a room (see Section D.7) plus the full set of listener, target and interferer details. Note, many scenes can be generated from a single room, i.e. each using different listener, target and interferer settings.  scenes.json [ { &quot;scene&quot;: &quot;S00001&quot;, // the unique scene ID &quot;room&quot;:: &quot;R00001&quot;, // ID of room linking to rooms.json &quot;target&quot;: { &quot;name&quot;: &quot;T005_JYD_04274&quot;, // target speaker code and BNCid &quot;time_start&quot;: 107210, // start time of target in samples &quot;time_end&quot;: 217019 // end time of target in samples }, &quot;listener&quot;: { &quot;rotation&quot;: [ // Defines the head motion - list of time, direction pairs { &quot;sample&quot;: 88200, &quot;angle&quot;: 30 // Azimuth angle in degrees }, { &quot;sample&quot;: 176400, &quot;angle‚Äù: 50 } ], &quot;hrir_filename&quot;: [&quot;VP_N4-ED&quot;, &quot;VP_N4-BTE_fr&quot;, &quot;VP_N4-BTE_mid&quot;, &quot;VP_N4-BTE_rear&quot;] // HRIR filename for each channel to generate }, &quot;interferers&quot;: [ { &quot;position&quot;: 1, // Index of interferer position (See rooms.json) &quot;time_start&quot;: 0, // time of interferer onset in samples &quot;time_end&quot;: 261119, // time of interferer offset in samples &quot;name&quot;: &quot;track_1353255&quot;, // interferer name &quot;type&quot;: &quot;music&quot;, // interferer type: speech, noise or music &quot;offset&quot;: 4076256 // index into interferer file at which to extract sample }, { // etc, up to three interferers } ], &quot;dataset&quot;: &quot;train&quot;, // the dataset to which the scene belongs: train, dev or eval &quot;duration&quot;: 261119, // total duration of scene in samples &quot;SNR&quot;: 6.89 // targe SNR for the scene }, ... ]   There are JSON files containing the scene specifications per dataset, e.g., scenes.train.json.- Note, that the scene ID and room ID might have a one-to-one mapping in the challenge, but are not necessarily the same. Multiple scenes can be made by changing the target and masker choices for a given room. E.g., participants wanting to expand the training data could remix multiple scenes from the same room.  The listener ID is not stored in the scene metadata; this information is stored separately in a scenes_listeners.json file which maps scenes to listeners, ie. telling you which listener (or listeners) will be listening to which scenes in the evaluation (see Section D.9).  Noise interferers are labelled with a type ‚Äúmusic‚Äù, ‚Äúnoise‚Äù or ‚Äúspeech‚Äù and then have a unique name identifying the file.  For speech: &lt;ACCENT_CODE&gt;_&lt;SPEAKER_ID&gt; where ACCENT_CODE is a three letter code identify the accent region and gender of the speaker and SPEAKER_ID is a 5-digit ID specific to an individual speaker. E.g. &quot;mif_02484&quot; is a UK midlands accented female, speaker 02484. The speech comes from Demirshan et al. [1] which provides more details.For noise: CIN_&lt;NOISE_TYPE&gt;_&lt;NOISE_ID&gt; where NOISE_TYPE is one of dishwasher, fan, hairdryer, kettle, microwave, vacuum (vacuum cleaner) or washing (washing machine) and NOISE_ID is a unique 3-digit code for the sample.For music: track_&lt;TRACK_ID&gt; where TRACK_ID is unique 7-digit track identifier taken from the MTG Jamendo database. [2]  Given the type and name, further interferer metadata can be found in the files masker_speech_list.json, masker_noise_list.json and masker_music_list.json which are distributed with the challenge.  ","version":"Next","tagName":"h3"},{"title":"D.10 Listener metadata‚Äã","type":1,"pageTitle":"ICASSP 2023 Data","url":"/docs/icassp2023/data/icassp2023_data#d10-listener-metadata","content":" Audiogram data is stored in a single JSON file with the following format.  listeners.json { &quot;L0001&quot;: { &quot;name&quot;: &quot;L0001&quot;, &quot;audiogram_cfs&quot;: [250, 500, 1000, 2000, 3000, 4000, 6000, 8000], &quot;audiogram_levels_l&quot;: [10, 10, 20, 30, 40, 55, 55, 60], &quot;audiogram_levels_r&quot;: [ ‚Ä¶ ], }, &quot;L0002&quot;: { // ... etc }, // ... etc }   Additional metadata (e.g. digit triple test results) are stored in a csv file. DETAILS  ","version":"Next","tagName":"h3"},{"title":"D.11 Scene-Listener map‚Äã","type":1,"pageTitle":"ICASSP 2023 Data","url":"/docs/icassp2023/data/icassp2023_data#d11-scene-listener-map","content":" JSON file named scenes_listeners.json dictates which scenes are to be processed by which listeners.  scenes_listeners.json { &quot;S00001&quot;: [&quot;L0001&quot;, &quot;L0002&quot;, &quot;L0003&quot;], &quot;S00002&quot;: [&quot;L0003&quot;, &quot;L0005&quot;, &quot;L0007&quot;], // ... etc }   ","version":"Next","tagName":"h3"},{"title":"References‚Äã","type":1,"pageTitle":"ICASSP 2023 Data","url":"/docs/icassp2023/data/icassp2023_data#references","content":" Demirsahin, Isin and Kjartansson, Oddur and Gutkin, Alexander and Rivera, Clara, &quot;Open-source Multi-speaker Corpora of the English Accents in the British Isles&quot;, Proceedings of The 12th Language Resources and Evaluation Conference (LREC), 6532--6541, 2020, Avialable OnlineBogdanov, Dmitry and Won, Minz and Tovstogan, Philip and Porter, Alastair and Serra, Xavier, &quot;The MTG-Jamendo Dataset for Automatic Music Tagging&quot;, In Proc. Machine Learning for Music Discovery Workshop, International Conference on Machine Learning (ICML 2019), 2019, Long Beach, CA, United States&quot;, Available Online ","version":"Next","tagName":"h2"}],"options":{"indexBaseUrl":true,"id":"default"}}