---
id: cpc3_intro
title: The 3rd Clarity Prediction Challenge
sidebar_label: CPC3 Introduction
sidebar_position: 1
---

To advance hearing enhancement technologies, such as hearing aids and hearable devices, it is essential to establish reliable methods for automatically evaluating the speech intelligibility of audio signals. This involves creating a predictive model that takes both the audio produced by a hearing aid and listener characteristics (e.g., their [audiogram](https://www.hear-it.org/Audiogram)) to estimate the speech intelligibility score that listeners would achieve in a listening test. In recent years, we have conducted the [CPC1 Challenge](../cpc1/cpc1_intro) and [CPC2 Challenge](../cpc2/cpc2_intro) to improve these models. These challenges were featured at a special session of Interspeech 2022 and at a satellite workshop during Interspeech 2023, respectively.

We are now excited to announce the launch of the third round of the Clarity Prediction Challenge, which builds on previous editions by incorporating a larger and more diverse set of listener data for training and evaluation. The results of this new challenge will be showcased at an [ISCA workshop](URL_2), which will be a satellite event at Interspeech 2025 in Rotterdam on August 22, 2025. The challenge was officially launched on March 17, 2025, and this site provides all necessary resources for participants.

### Short Description

The task involves estimating the intelligibility of speech-in-noise signals processed by hearing aid algorithms and presented to listeners with hearing loss. Each signal contains a short sentence that listeners were asked to repeat. Your system will need to predict how many words were correctly recognised by the listeners.

Systems will be ranked based on overall performance across a large evaluation set. Specifically, we will compute the RMSE between the predictions and the true values of the listener's percentage word correctness scores per sentence.

The quality of the hearing aid signals being evaluated varies widely. Below are examples of good, fair, and poor signals. Your prediction algorithm must be robust enough to handle this variation.

<table>
<tr>
<th>Good</th>
<th>Fair</th>
<th>Poor</th>
</tr>
<tr>
<td>
<audio controls  style={{width: "250px"}} >
<source src="/audio/CEC2_samples/CEC2_E009/S08501_L0104_HA-output.wav" type="audio/wav"/>
Your browser does not support the audio element.
</audio>
</td>
<td>
<audio controls style={{width: "250px"}}>
<source src="/audio/CEC2_samples/CEC2_E022/S08501_L0104_HA-output.wav" type="audio/wav" />
Your browser does not support the audio element.
</audio>
</td>
<td>
<audio controls style={{width: "250px"}}>
<source src="/audio/CEC2_samples/CEC2_E032/S08501_L0104_HA-output.wav" type="audio/wav" />
Your browser does not support the audio element.
</audio>
</td>
</tr>
<tr>
<td>
<audio controls style={{width: "250px"}}>
<source src="/audio/CEC2_samples/CEC2_E009/S08502_L0106_HA-output.wav" type="audio/wav" />
Your browser does not support the audio element.
</audio>
</td>
<td>
<audio controls style={{width: "250px"}}>
<source src="/audio/CEC2_samples/CEC2_E022/S08502_L0106_HA-output.wav" type="audio/wav" />
Your browser does not support the audio element.
</audio>
</td>
<td>
<audio controls style={{width: "250px"}}>
<source src="/audio/CEC2_samples/CEC2_E032/S08502_L0106_HA-output.wav" type="audio/wav" />
Your browser does not support the audio element.
</audio>
</td>
</tr></table>

### The Data

Participants will receive a training dataset to develop their systems. This dataset includes:

- Audio signals processed by a variety of (simulated) hearing aids for speech-in-noise;
- The corresponding clean reference signals (the original speech);
- A measurement of the severity of each listener's hearing impairment;
- Measured speech intelligibility scores from listening tests, in which listeners repeated what they heard after hearing aid processing.

See the [Data](./cpc3_data) page for full data details.

### The Task

Participants will be provided with an evaluation set containing:

- Audio produced by a variety of (simulated) hearing aids for speech-in-noise;
- The hearing impairment severity of the listeners;
- The clean reference signal (the original speech).

We will evaluate two types of systems: intrusive and non-intrusive.

- Intrusive systems (double-ended) require a clean speech reference.
- Non-intrusive systems (single-ended) rely solely on the hearing aid output.

Systems will be ranked based on the Root Mean Squared Error (RMSE) between predicted and actual intelligibility scores.

To help participants get started, we provide a baseline system that uses the HASPI metric to predict speech intelligibility. Details of this system are available on the [Baseline](./cpc3_baseline) page.

Refer to the [rules](./taking_part/cpc3_rules) page for complete task details.

### Registering and Submitting

To participate in the challenge, you will need to [register your team](./taking_part/cpc3_registration) and [download the data](./cpc3_download). Entrants will have until July 31st to complete their submissions. Full instructions for submission are available on the [Submission](./taking_part/cpc3_submission) page.
