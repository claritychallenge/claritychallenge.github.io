"use strict";(self.webpackChunkclarity=self.webpackChunkclarity||[]).push([[8494],{10908:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>c,contentTitle:()=>l,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"icassp2023/data/icassp2023_new_evaluation","title":"ICASSP 2023 More ecologically-valid eval set","description":"Overview","source":"@site/docs/icassp2023/data/icassp2023_new_evaluation.mdx","sourceDirName":"icassp2023/data","slug":"/icassp2023/data/icassp2023_new_evaluation","permalink":"/docs/icassp2023/data/icassp2023_new_evaluation","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":6.5,"frontMatter":{"id":"icassp2023_new_evaluation","title":"ICASSP 2023 More ecologically-valid eval set","sidebar_label":"New evaluation set","sidebar_position":6.5},"sidebar":"tutorialSidebar_icassp2023","previous":{"title":"Data Specification","permalink":"/docs/icassp2023/data/icassp2023_data"},"next":{"title":"Software","permalink":"/docs/category/software-3"}}');var r=n(74848),o=n(28453),t=n(86025);const a={id:"icassp2023_new_evaluation",title:"ICASSP 2023 More ecologically-valid eval set",sidebar_label:"New evaluation set",sidebar_position:6.5},l=void 0,c={},d=[{value:"Overview",id:"overview",level:2},{value:"Environment",id:"environment",level:2},{value:"Equipment",id:"equipment",level:2},{value:"Target speech",id:"target-speech",level:2},{value:"Interferers",id:"interferers",level:2},{value:"Listener",id:"listener",level:2},{value:"Talker, noise and listener position",id:"talker-noise-and-listener-position",level:2},{value:"Publication",id:"publication",level:2},{value:"Example recordings",id:"example-recordings",level:2},{value:"References",id:"references",level:2}];function h(e){const i={a:"a",code:"code",h2:"h2",li:"li",p:"p",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsxs)(i.p,{children:["This more ecologically-valid eval set (",(0,r.jsx)(i.code,{children:"eval2"}),") has been designed to answer the following research question: Can systems trained on simulated data generalise to more ecologically-valid measurement data?"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Recordings were carried in a real room using live talkers."}),"\n",(0,r.jsxs)(i.li,{children:["The talkers were recorded on both a close microphone and also a 1st-order ambisonic microphone at the listener position.","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Head rotations are done using the spherical harmonic representation of the sound."}),"\n",(0,r.jsx)(i.li,{children:"HRTFs are applied to get the hearing-aid microphone signals, as for the simulated datasets."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(i.li,{children:"The talkers were recorded in noise-free conditions."}),"\n",(0,r.jsx)(i.li,{children:"Noise, music and speech interferers were played from loudspeaker and recorded on the ambisonic microphone."}),"\n",(0,r.jsx)(i.li,{children:"The target talker and intereferer are then mixed to create a scene with a desired SNR."}),"\n",(0,r.jsx)(i.li,{children:"The random positions of the sources and receivers were achieved using the same limitations as applied to the simulated set (e.g. target talker and listener at least 1m apart)"}),"\n"]}),"\n",(0,r.jsx)(i.p,{children:"Differences between simulated and ecologically-valid datasets:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Talkers speaking and behaving different when asked to talk to a distant microphone in a real room."}),"\n",(0,r.jsx)(i.li,{children:"Real room acoustic altering sound instead of simulation using geometric room acoustic model."}),"\n",(0,r.jsx)(i.li,{children:"Directivity of interferers not omni-directional."}),"\n",(0,r.jsx)(i.li,{children:"Transducer noise on the distant ambisonic microphone."}),"\n",(0,r.jsx)(i.li,{children:"Measurements had lower order Ambisonics than used in the simulations."}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"environment",children:"Environment"}),"\n",(0,r.jsxs)(i.p,{children:["Recordings were done in the ",(0,r.jsx)(i.a,{href:"https://acoustictesting.salford.ac.uk/acoustic-laboratories/listening-room/",children:"Acoustics Research Centre's listening room at the University of Salford"}),"."]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Mid-frequency reverberation time: 0.27s"}),"\n",(0,r.jsx)(i.li,{children:"Room dimensions: 6.6m \xd7 5.8m \xd7 2.8m"}),"\n",(0,r.jsx)(i.li,{children:"Background noise: 5.7 dBA"}),"\n"]}),"\n",(0,r.jsxs)("figure",{id:"fig1",children:[(0,r.jsx)("img",{width:"500",src:(0,t.Ay)("/img/ICASSP2023/binaural_with_head_tracking_in_salford_university_listening_room.jpg")}),(0,r.jsx)("figcaption",{children:"Figure 1.  The listening room (photo not from evaluation set recording)."})]}),"\n",(0,r.jsx)(i.h2,{id:"equipment",children:"Equipment"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Close microphone: Neumann KM184 cardioid"}),"\n",(0,r.jsx)(i.li,{children:"Close microphone preamp: Alice mic.amp.pak1"}),"\n",(0,r.jsx)(i.li,{children:"Ambisonic microphone: Sennheiser Ambeo VR"}),"\n",(0,r.jsx)(i.li,{children:"Interface: RME Fireface UFX"}),"\n",(0,r.jsx)(i.li,{children:"Loudspeaker for interferer: M-audio BX8a"}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"target-speech",children:"Target speech"}),"\n",(0,r.jsx)(i.p,{children:"A new set of 1,600 sentences generated from the British National Corpus not previously used by Clarity. These were generated using the same process as before [1]."}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:["The sentences were read live by 10 actors: 5 male and 5 female.","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Ages ranged from 20 to 62."}),"\n",(0,r.jsx)(i.li,{children:"Actors were standing."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(i.li,{children:"The talker faced the ambisonic microphone. They were told to talk to that microphone and ignore the close microphone."}),"\n",(0,r.jsx)(i.li,{children:"Recorded in noise-free conditions."}),"\n",(0,r.jsx)(i.li,{children:"Each speaker recorded 160 unique sentences, in blocks of 10 talking positions."}),"\n",(0,r.jsx)(i.li,{children:"A cardioid microphone about 50 cm from the talker recorded the reference speech for HASPI and HASQI."}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"interferers",children:"Interferers"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Recordings reproduced by loudspeakers."}),"\n",(0,r.jsx)(i.li,{children:"Recordings of speech, noise and muisc same sources as CEC2 evaluation set."}),"\n",(0,r.jsx)(i.li,{children:"Each interferer recorded separately on the ambisonics microphone."}),"\n",(0,r.jsx)(i.li,{children:"Loudspeaker facing ambisonic microphone"}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"listener",children:"Listener"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Recordings on a 1st order ambisonics microphone."}),"\n",(0,r.jsx)(i.li,{children:"Front of ambisonic room along x-axis of room."}),"\n",(0,r.jsx)(i.li,{children:"Head rotation done virtually via spherical harmonics with the same statistics as the training set."}),"\n",(0,r.jsx)(i.li,{children:"HRTFs applied to the ambisonic recordings using a virtual loudspeaker set-up to give the signals on the hearing aid microphones."}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"talker-noise-and-listener-position",children:"Talker, noise and listener position"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:["16 different room layouts (see Figure 2) with random talker, interferer and listener positions.","\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"These positions determined using the same protocol as used for the simulation."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(i.li,{children:"A block of 10 sentences read for each layout."}),"\n",(0,r.jsx)(i.li,{children:"Sources and receivers at the same height (but some variation in the talker z-coordinate because of height differences in the actors)."}),"\n"]}),"\n",(0,r.jsxs)("figure",{id:"fig2",children:[(0,r.jsx)("img",{width:"100%",src:(0,t.Ay)("/img/ICASSP2023/layout_icassp_evaluation_set.png")}),(0,r.jsx)("figcaption",{children:"Figure 2.  The 16 layouts. T talker; A ambisonic mic; N noise interferer; S speech interferer; M music interferer."})]}),"\n",(0,r.jsx)(i.h2,{id:"publication",children:"Publication"}),"\n",(0,r.jsx)(i.p,{children:"The target speech and interferers will be mixed to gain the desired signal to noise ratio using the same process as for the simulation set. The dataset will be available 1st February 2023."}),"\n",(0,r.jsx)(i.h2,{id:"example-recordings",children:"Example recordings"}),"\n",(0,r.jsx)(i.p,{children:"Recording of script reading by someone not used for the evaluation set. The audio starts 3-4 seconds into the recording."}),"\n",(0,r.jsx)(i.p,{children:"Close microphone:"}),"\n",(0,r.jsxs)("audio",{controls:!0,children:[(0,r.jsx)("source",{src:"/audio/example_sentences_05-Neumann-230106_1501.wav",type:"audio/wav"}),(0,r.jsx)(i.p,{children:"Your browser does not support the audio element."})]}),"\n",(0,r.jsx)(i.p,{children:"Ambisonic microphone, A-format:"}),"\n",(0,r.jsx)(i.p,{children:"Front-left-up:"}),"\n",(0,r.jsxs)("audio",{controls:!0,children:[(0,r.jsx)("source",{src:"/audio/example_sentences_01-A-Format_FLU-230106_1501.wav",type:"audio/wav"}),(0,r.jsx)(i.p,{children:"Your browser does not support the audio element."})]}),"\n",(0,r.jsx)(i.p,{children:"Front-right-down:"}),"\n",(0,r.jsxs)("audio",{controls:!0,children:[(0,r.jsx)("source",{src:"/audio/example_sentences_02-A-Format_FRD-230106_1501.wav",type:"audio/wav"}),(0,r.jsx)(i.p,{children:"Your browser does not support the audio element."})]}),"\n",(0,r.jsx)(i.p,{children:"Back-left-down:"}),"\n",(0,r.jsxs)("audio",{controls:!0,children:[(0,r.jsx)("source",{src:"/audio/example_sentences_03-A-Format_BLD-230106_1501.wav",type:"audio/wav"}),(0,r.jsx)(i.p,{children:"Your browser does not support the audio element."})]}),"\n",(0,r.jsx)(i.p,{children:"Back-right-up:"}),"\n",(0,r.jsxs)("audio",{controls:!0,children:[(0,r.jsx)("source",{src:"/audio/example_sentences_04-A-Format_BRU-230106_1501.wav",type:"audio/wav"}),(0,r.jsx)(i.p,{children:"Your browser does not support the audio element."})]}),"\n",(0,r.jsx)(i.h2,{id:"references",children:"References"}),"\n",(0,r.jsx)(i.p,{children:"[1] Graetzer, S., Akeroyd, M.A., Barker, J., Cox, T.J., Culling, J.F., Naylor, G., Porter, E. and Viveros-Mu\xf1oz, R., 2022. Dataset of British English speech recordings for psychoacoustics and speech processing research: The clarity speech corpus. Data in Brief, 41, p.107951."})]})}function u(e={}){const{wrapper:i}={...(0,o.R)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},28453:(e,i,n)=>{n.d(i,{R:()=>t,x:()=>a});var s=n(96540);const r={},o=s.createContext(r);function t(e){const i=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function a(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(o.Provider,{value:i},e.children)}}}]);