"use strict";(self.webpackChunkclarity=self.webpackChunkclarity||[]).push([[1477],{30010:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"CPC2 eval data released","metadata":{"permalink":"/blog/CPC2 eval data released","source":"@site/blog/2023-07-03-CPC2-eval-data-release.mdx","title":"CPC2 eval data released","description":"The CPC2 evaluation data has now been released.","date":"2023-07-03T00:00:00.000Z","formattedDate":"July 3, 2023","tags":[{"label":"clarity","permalink":"/blog/tags/clarity"},{"label":"announcement","permalink":"/blog/tags/announcement"},{"label":"CPC2","permalink":"/blog/tags/cpc-2"}],"readingTime":0.62,"hasTruncateMarker":false,"authors":[{"name":"Jon Barker","title":"Clarity Team Member","url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","imageURL":"https://avatars.githubusercontent.com/jonbarker68"}],"frontMatter":{"slug":"CPC2 eval data released","title":"CPC2 eval data released","author":"Jon Barker","author_title":"Clarity Team Member","author_url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","author_image_url":"https://avatars.githubusercontent.com/jonbarker68","tags":["clarity","announcement","CPC2"]},"unlisted":false,"nextItem":{"title":"Clarity-2023 Workshop @ Interspeech, Dublin","permalink":"/blog/Clarity-2023 Workshop @ Interspeech, Dublin"}},"content":"The CPC2 evaluation data has now been released.\\n\\nThe data is available for download as a single 478 MB file, [clarity_CPC2_data.test.v1_0.tgz](https://mab.to/138typ3cpVBmK). The evaluation data should be untarred into the same root as the training data. Further details can be found on the [challenge website](https://claritychallenge.org/docs/cpc2/cpc2_intro).\\n\\nThe data consists of the hearing aid algorithm output signals, clean reference signals, listener audiograms, and head rotation information. Listener responses are not provided for the evaluation data but will be made available after the submission window has closed.\\n\\nFor details on how to prepare your submission [please see the instructions on the website](https://claritychallenge.org/docs/cpc2/taking_part/cpc2_submission).\\n\\nIf you have any questions please feel free to post them on this forum.\\n\\nThe submission window will close on the 31st of July.\\n\\nGood luck!"},{"id":"Clarity-2023 Workshop @ Interspeech, Dublin","metadata":{"permalink":"/blog/Clarity-2023 Workshop @ Interspeech, Dublin","source":"@site/blog/2023-06-21-clarity-2023-workshop.mdx","title":"Clarity-2023 Workshop @ Interspeech, Dublin","description":"We are pleased to announce the 4th ISCA Clarity Workshop on Machine Learning Challenges for Hearing Aids (Clarity-2023).","date":"2023-06-21T00:00:00.000Z","formattedDate":"June 21, 2023","tags":[{"label":"clarity","permalink":"/blog/tags/clarity"},{"label":"announcement","permalink":"/blog/tags/announcement"},{"label":"CPC2","permalink":"/blog/tags/cpc-2"}],"readingTime":2.52,"hasTruncateMarker":false,"authors":[{"name":"Jon Barker","title":"Clarity Team Member","url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","imageURL":"https://avatars.githubusercontent.com/jonbarker68"}],"frontMatter":{"slug":"Clarity-2023 Workshop @ Interspeech, Dublin","title":"Clarity-2023 Workshop @ Interspeech, Dublin","author":"Jon Barker","author_title":"Clarity Team Member","author_url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","author_image_url":"https://avatars.githubusercontent.com/jonbarker68","tags":["clarity","announcement","CPC2"]},"unlisted":false,"prevItem":{"title":"CPC2 eval data released","permalink":"/blog/CPC2 eval data released"},"nextItem":{"title":"Announcing the 2nd Clarity Prediction Challenge (CPC2)","permalink":"/blog/Announcing CPC2"}},"content":"We are pleased to announce the **4th ISCA Clarity Workshop on Machine Learning Challenges for Hearing Aids (Clarity-2023)**.\\n\\nThe event will be a one-day workshop held as an ISCA satellite event to Interspeech 2023 in Dublin, Ireland.\\n\\nFor registration and programme details please visit the workshop website\\n\\n[https://claritychallenge.github.io/clarity2023-workshop/](https://claritychallenge.github.io/clarity2023-workshop/\\n)\\n\\n**IMPORTANT DATES**\\n\\n- 2nd June 2023 - Workshop Submission Deadline (Regular Papers)\\n- 31st July 2023 - Workshop Submission Deadline (Clarity Challenge Papers)\\n- 5th August 2023 - Registration closes\\n- 19th August - Workshop / Clarity Challenge results announced\\n\\n**About**\\n\\nOne of the biggest challenges for hearing-impaired listeners is understanding speech in the presence of background noise. Everyday social noise levels can have a devastating impact on speech intelligibility. The inability to communicate effectively can lead to social withdrawal and isolation. Disabling hearing impairment affects 360 million people worldwide, with that number increasing because of the ageing population. Unfortunately, current hearing aid technology is often ineffective in noisy situations. Although amplification can restore audibility, it does not compensate fully for the effects of hearing loss.\\n\\nThe Clarity workshops are designed to stimulate a two-way conversation between the speech research community and hearing aid developers. Hearing aid developers, who are not typically represented at Interspeech, will have an opportunity to present the challenges of their industry to the speech community; the speech community will be able to present and discuss potentially transformative approaches to speech in noise processing in the presence of hearing researchers and industry experts.\\n\\n**Topics**\\n\\nAny work related to the challenges of hearing aid signal processing will be considered relevant topics include,\\n\\n- Binaural technology for speech enhancement and source separation\\n- Multi-microphone processing technology\\n- Real-time approaches to speech enhancement\\n- Statistical model-driven approaches to hearing aid processing\\n- Audio quality & intelligibility assessment hearing aid and cochlear implant users\\n- Efficient and effective integration of psychoacoustic testing in machine learning\\n- Machine learning for diverse target listeners\\n- Machine learning models of hearing impairment\\n\\n**The 2nd Clarity Prediction Challenge**\\n\\nThe Clarity-2023 will also host the 2nd Clarity Prediction Challenge, that is addressing the problem of developing new intrusive and non-intrusive approaches to hearing-aid speech intelligibility prediction. The Challenge will be launching on 1st March, is you may be interested in participating please sign up to our Google group for further announcements.\\n\\n**Keynote Talks**\\n\\n- Prof Fei Chen, SUSTech, China,\\n- Prof DeLiang Wang, Ohio State University, US\\n\\n**Organisers**\\n\\n- Michael Akeroyd, University of Nottingham\\n- Jon Barker,  University of Sheffield\\n- Trevor Cox, University of Salford\\n- Fei Chen, Southern University of Science and Technology, China\\n- John Culling,  University of Cardiff\\n- Simone Graetzer, University of Salford\\n- Andrew Hines, University College Dublin\\n\\n**For further information**\\n\\nTo be kept up to date please join our [Clarity Challenge Google group](https://groups.google.com/g/clarity-challenge). If you have questions, please contact us directly using the [contact details found here](https://claritychallenge.org/contact).\\n\\n**Funded by** the Engineering and Physical Sciences Research Council (EPSRC), UK\\n\\n**Supported by** RNID (formerly Action on Hearing Loss), Hearing Industry Research Consortium, Amazon TTS Research"},{"id":"Announcing CPC2","metadata":{"permalink":"/blog/Announcing CPC2","source":"@site/blog/2023-03-17-announcing-CPC2.mdx","title":"Announcing the 2nd Clarity Prediction Challenge (CPC2)","description":"The 2nd Clarity Prediction Challenge - Register Now","date":"2023-03-17T00:00:00.000Z","formattedDate":"March 17, 2023","tags":[{"label":"clarity","permalink":"/blog/tags/clarity"},{"label":"announcement","permalink":"/blog/tags/announcement"},{"label":"CPC2","permalink":"/blog/tags/cpc-2"}],"readingTime":1.72,"hasTruncateMarker":false,"authors":[{"name":"Jon Barker","title":"Clarity Team Member","url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","image_url":"https://avatars.githubusercontent.com/jonbarker68","imageURL":"https://avatars.githubusercontent.com/jonbarker68"},{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","image_url":"https://avatars.githubusercontent.com/trevorjcox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"}],"frontMatter":{"slug":"Announcing CPC2","title":"Announcing the 2nd Clarity Prediction Challenge (CPC2)","authors":[{"name":"Jon Barker","title":"Clarity Team Member","url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","image_url":"https://avatars.githubusercontent.com/jonbarker68","imageURL":"https://avatars.githubusercontent.com/jonbarker68"},{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","image_url":"https://avatars.githubusercontent.com/trevorjcox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"}],"tags":["clarity","announcement","CPC2"]},"unlisted":false,"prevItem":{"title":"Clarity-2023 Workshop @ Interspeech, Dublin","permalink":"/blog/Clarity-2023 Workshop @ Interspeech, Dublin"},"nextItem":{"title":"ICASSP 2023 evaluation data released","permalink":"/blog/ICASSP 2023 evaluation data released"}},"content":"The 2nd Clarity Prediction Challenge - [Register Now](https://claritychallenge.org/docs/cpc2/taking_part/cpc2_registration)\\n\\nTo allow the development of better hearing aids, we need ways to evaluate the speech intelligibility of audio signals automatically. We need a prediction model that takes the audio produced by a hearing aid and the listener\'s characteristics (e.g. audiogram) and estimates the speech intelligibility score that the listener would achieve in a listening test.\\n\\nLast year we ran the [CPC1 Challenge](https://claritychallenge.org/docs/cpc1/cpc1_intro) to develop such models. The challenge was presented at an online workshop and a special session of Interspeech 2022. We are now running the 2nd round of this challenge (CPC2), which builds on the first by using more complex signals and a larger set of listening test data for training and evaluating the prediction systems.\\n\\nThe outputs of the new challenge will be presented at an [ISCA workshop](https://claritychallenge.org/clarity2023-workshop/) that is being run as a satellite event to Interspeech 2023 in Dublin on 19th August 2023.\\n\\nFull details can be found on the Challenge Website.\\n\\n### Register now to take part\\n\\nIf you are interested in participating please register now via the [online registration form](https://claritychallenge.org/docs/cpc2/taking_part/cpc2_registration).\\n\\n### Important Dates\\n\\n- March - Launch of challenge, release of training data + baseline system.\\n- 1st July - Release of evaluation data and opening of submission window.\\n- 31st July - Submission deadline.\\n- 19th August - ISCA Clarity 2023 workshop @ Interspeech\\n- 19th September - Deadline for submission of finalised Workshop papers\\n\\n### What will be provided\\n\\n- Audio produced by a variety of (simulated) hearing aids for speech-in-noise;\\n- The corresponding clean reference signals (the original speech);\\n- Characteristics of the listeners (pure tone audiograms, etc);\\n- The measured speech intelligibility scores from listening tests, where hearing-impaired listeners were asked to say what they heard after listening to the hearing aid processed signals.\\n- Software tools including a baseline system based on HASPI scores.\\n\\n### For further information\\n\\nTo be kept up to date please join our [Clarity Challenge Google group](https://groups.google.com/g/clarity-challenge). If you have questions, please contact us directly using the contact details found [here](https://claritychallenge.org/contact)."},{"id":"ICASSP 2023 evaluation data released","metadata":{"permalink":"/blog/ICASSP 2023 evaluation data released","source":"@site/blog/2023-02-02-ICASSP-eval-data.mdx","title":"ICASSP 2023 evaluation data released","description":"We are pleased to announce that the evaluation dataset for the ICASSP Clarity Challenge is now available for download.","date":"2023-02-02T00:00:00.000Z","formattedDate":"February 2, 2023","tags":[{"label":"clarity","permalink":"/blog/tags/clarity"},{"label":"announcement","permalink":"/blog/tags/announcement"},{"label":"CEC2","permalink":"/blog/tags/cec-2"},{"label":"ICASSP2023","permalink":"/blog/tags/icassp-2023"}],"readingTime":0.8,"hasTruncateMarker":false,"authors":[{"name":"Jon Barker","title":"Clarity Team Member","url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","image_url":"https://avatars.githubusercontent.com/jonbarker68","imageURL":"https://avatars.githubusercontent.com/jonbarker68"},{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","image_url":"https://avatars.githubusercontent.com/trevorjcox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"}],"frontMatter":{"slug":"ICASSP 2023 evaluation data released","title":"ICASSP 2023 evaluation data released","authors":[{"name":"Jon Barker","title":"Clarity Team Member","url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","image_url":"https://avatars.githubusercontent.com/jonbarker68","imageURL":"https://avatars.githubusercontent.com/jonbarker68"},{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","image_url":"https://avatars.githubusercontent.com/trevorjcox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"}],"tags":["clarity","announcement","CEC2","ICASSP2023"]},"unlisted":false,"prevItem":{"title":"Announcing the 2nd Clarity Prediction Challenge (CPC2)","permalink":"/blog/Announcing CPC2"},"nextItem":{"title":"Announcement of ICASSP 2023 Grand Challenge","permalink":"/blog/Announcement of ICASSP 2023 Grand Challenge"}},"content":"import useBaseUrl from \'@docusaurus/useBaseUrl\';\\n\\nWe are pleased to announce that the evaluation dataset for the ICASSP Clarity Challenge is now available for download.\\n\\n[https://www.myairbridge.com/en/#!/folder/EkthOZZeBW33aaDBWSDadTgpOkbgaFxO](https://www.myairbridge.com/en/#!/folder/EkthOZZeBW33aaDBWSDadTgpOkbgaFxO)\\n\\nFor instructions on preparing your submission please visit:\\n\\n[https://claritychallenge.org/docs/icassp2023/taking_part/icassp2023_submission](https://claritychallenge.org/docs/icassp2023/taking_part/icassp2023_submission)\\n\\n**If you have not yet registered** it is not too late to do so. Please use the form at the link below and we will then send you a Team ID and a personalised upload link for your submission.\\n\\n[https://claritychallenge.org/docs/icassp2023/taking_part/icassp2023_registration](https://claritychallenge.org/docs/icassp2023/taking_part/icassp2023_registration)\\n\\nNote, we have extended the deadline for submission until Friday 10th February so that teams have a full week to process the signals.\\n\\nThe remaining schedule is as follows,\\n\\n- **2nd Feb 2023**: Release of evaluation data.\\n- **10th Feb 2023**: Teams submit processed signals and technical reports.\\n- **14th Feb 2023**: Results released. Top 5 ranked teams invited to submit papers to ICASSP-2023\\n- **20th Feb 2023**: Invited papers submitted to ICASSP-2023\\n- **4-9th June 2023**: Overview paper and invited papers presented at dedicated ICASSP session"},{"id":"Announcement of ICASSP 2023 Grand Challenge","metadata":{"permalink":"/blog/Announcement of ICASSP 2023 Grand Challenge","source":"@site/blog/2022-11-15-ICASSP-registration.mdx","title":"Announcement of ICASSP 2023 Grand Challenge","description":"We are pleased to announce that registration for the ICASSP 2023 Clarity Grand Challenge is now open.","date":"2022-11-15T00:00:00.000Z","formattedDate":"November 15, 2022","tags":[{"label":"clarity","permalink":"/blog/tags/clarity"},{"label":"announcement","permalink":"/blog/tags/announcement"},{"label":"CEC2","permalink":"/blog/tags/cec-2"}],"readingTime":0.725,"hasTruncateMarker":false,"authors":[{"name":"Will Bailey","title":"Clarity Team Member","url":"https://www.sheffield.ac.uk/dcs/people/research-staff/will-bailey"}],"frontMatter":{"slug":"Announcement of ICASSP 2023 Grand Challenge","title":"Announcement of ICASSP 2023 Grand Challenge","author":"Will Bailey","author_title":"Clarity Team Member","author_url":"https://www.sheffield.ac.uk/dcs/people/research-staff/will-bailey","tags":["clarity","announcement","CEC2"]},"unlisted":false,"prevItem":{"title":"ICASSP 2023 evaluation data released","permalink":"/blog/ICASSP 2023 evaluation data released"},"nextItem":{"title":"CPC1 results and prizes","permalink":"/blog/CPC1 results and prizes"}},"content":"import useBaseUrl from \'@docusaurus/useBaseUrl\';\\n\\nWe are pleased to announce that registration for the **ICASSP 2023 Clarity Grand Challenge** is now open.\\n\\nTo register please complete the simple Google form found on the [registration page](/docs/icassp2023/taking_part/icassp2023_registration).\\n\\nThe remaining important dates for the challenge are as follows:\\n\\n- **28th Nov 2022**: Challenge launch: Release training/dev data; tools; baseline; rules & documentation.\\n- **2nd Feb 2023**: Release of evaluation data.\\n- **10th Feb 2023**: Teams submit processed signals and technical reports.\\n- **14th Feb 2023**: Results released. Top 5 ranked teams invited to submit papers to ICASSP-2023\\n- **20th Feb 2023**: Invited papers submitted to ICASSP-2023\\n- **4-9th** June 2023: Overview paper and invited papers presented at dedicated ICASSP session\\n\\nThe challenge training, dev data and initial tools are now fully from the [Github repository](https://github.com/claritychallenge/clarity).\\n\\nIf you have any questions please do not hesitate to contact us at [claritychallengecontact@gmail.com](mailto:claritychallengecontact@gmail.com)."},{"id":"CPC1 results and prizes","metadata":{"permalink":"/blog/CPC1 results and prizes","source":"@site/blog/2022-07-10-CPC1-results.mdx","title":"CPC1 results and prizes","description":"The 1st Clarity Prediction Challenge is now complete. Thank you to all who took part!","date":"2022-07-10T00:00:00.000Z","formattedDate":"July 10, 2022","tags":[{"label":"clarity","permalink":"/blog/tags/clarity"},{"label":"registration","permalink":"/blog/tags/registration"},{"label":"CEC2","permalink":"/blog/tags/cec-2"}],"readingTime":0.625,"hasTruncateMarker":false,"authors":[{"name":"Jon Barker","title":"Clarity Team Member","url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","imageURL":"https://avatars.githubusercontent.com/jonbarker68"}],"frontMatter":{"slug":"CPC1 results and prizes","title":"CPC1 results and prizes","author":"Jon Barker","author_title":"Clarity Team Member","author_url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","author_image_url":"https://avatars.githubusercontent.com/jonbarker68","tags":["clarity","registration","CEC2"]},"unlisted":false,"prevItem":{"title":"Announcement of ICASSP 2023 Grand Challenge","permalink":"/blog/Announcement of ICASSP 2023 Grand Challenge"},"nextItem":{"title":"CEC2 registration open","permalink":"/blog/CEC2 registration open"}},"content":"import useBaseUrl from \'@docusaurus/useBaseUrl\';\\n\\nThe 1st Clarity Prediction Challenge is now complete. Thank you to all who took part!\\n\\nThe full results can be found on the <a href=\\"https://claritychallenge.org/clarity2022-workshop/\\">Clarity-2022 workshop website</a> where you will also find links to system papers and the overview presentation.\\n\\nMany of the systems have led to successful Interspeech 2022 papers and will be contributing to the Interspeech 2022 special session on <a href=\\"https://claritychallenge.org/interspeech2022_siphil/\\">Speech Intelligibility Prediction for Hearing-Impaired Listeners</a>. We hope to see many of you in Korea!\\n\\nIn the meantime, please be sure to check out the onging <a href=\\"https://claritychallenge.org/docs/cec2/cec2_intro\\">2nd Clarity Enhancement Challenge</a>. The deadline for submitting enhanced signals is 1st September 2022, so there is still time to participate. To register a team please use the form <a href=\\"https://claritychallenge.org/docs/cec2/taking_part/cec2_registration\\">here</a>."},{"id":"CEC2 registration open","metadata":{"permalink":"/blog/CEC2 registration open","source":"@site/blog/2022-05-03-CEC2-registration.mdx","title":"CEC2 registration open","description":"We are pleased to announce that registration for the 2nd Clarity Enhancement Challenge (CEC2) is now open.","date":"2022-05-03T00:00:00.000Z","formattedDate":"May 3, 2022","tags":[{"label":"clarity","permalink":"/blog/tags/clarity"},{"label":"registration","permalink":"/blog/tags/registration"},{"label":"CEC2","permalink":"/blog/tags/cec-2"}],"readingTime":0.62,"hasTruncateMarker":false,"authors":[{"name":"Jon Barker","title":"Clarity Team Member","url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","imageURL":"https://avatars.githubusercontent.com/jonbarker68"}],"frontMatter":{"slug":"CEC2 registration open","title":"CEC2 registration open","author":"Jon Barker","author_title":"Clarity Team Member","author_url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","author_image_url":"https://avatars.githubusercontent.com/jonbarker68","tags":["clarity","registration","CEC2"]},"unlisted":false,"prevItem":{"title":"CPC1 results and prizes","permalink":"/blog/CPC1 results and prizes"},"nextItem":{"title":"Release of CEC2 baseline","permalink":"/blog/release of CEC2 baseline"}},"content":"import useBaseUrl from \'@docusaurus/useBaseUrl\';\\n\\nWe are pleased to announce that registration for the **2nd Clarity Enhancement Challenge (CEC2)** is now open.\\n\\nTo register please complete the simple Google form found on the [registration page](/docs/cec2/taking_part/cec2_registration).\\n\\nThe remaining important dates for the challenge are as follows:\\n\\n- **25th July 2022**: Evaluation data released\\n- **1st Sept 2022**: 1st round [submission](/docs/cec2/taking_part/cec2_submission) deadline for evaluation by objective measure\\n- **15th Sept 2022**: 2nd round [submission](/docs/cec2/taking_part/cec2_submission) deadline for listening tests\\n- **Sept-Nov 2022**: Listening test evaluation period.\\n- **2nd Dec 2022**:  Results announced at a Clarity Challenge Workshop; prizes awarded.\\n\\nThe challenge training, dev data and initial tools are now fully from the [Github repository](https://github.com/claritychallenge/clarity).\\n\\nIf you have any questions please do not hesitate to contact us at [claritychallengecontact@gmail.com](mailto:claritychallengecontact@gmail.com)."},{"id":"release of CEC2 baseline","metadata":{"permalink":"/blog/release of CEC2 baseline","source":"@site/blog/2022-05-02-CEC2-baseline.mdx","title":"Release of CEC2 baseline","description":"We are pleased to announce the release of the 2nd Clarity Enhancement Challenge (CEC2) baseline system code.","date":"2022-05-02T00:00:00.000Z","formattedDate":"May 2, 2022","tags":[{"label":"clarity","permalink":"/blog/tags/clarity"},{"label":"baseline","permalink":"/blog/tags/baseline"},{"label":"HASPI","permalink":"/blog/tags/haspi"},{"label":"CEC2","permalink":"/blog/tags/cec-2"}],"readingTime":0.72,"hasTruncateMarker":false,"authors":[{"name":"Jon Barker","title":"Clarity Team Member","url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","imageURL":"https://avatars.githubusercontent.com/jonbarker68"}],"frontMatter":{"slug":"release of CEC2 baseline","title":"Release of CEC2 baseline","author":"Jon Barker","author_title":"Clarity Team Member","author_url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","author_image_url":"https://avatars.githubusercontent.com/jonbarker68","tags":["clarity","baseline","HASPI","CEC2"]},"unlisted":false,"prevItem":{"title":"CEC2 registration open","permalink":"/blog/CEC2 registration open"},"nextItem":{"title":"Launch of CEC2","permalink":"/blog/launch of CEC2"}},"content":"We are pleased to announce the release of the **2nd Clarity Enhancement Challenge (CEC2)** baseline system code.\\n\\nThe baseline code has been released in the latest commit to the [Clarity GitHub repository](https://github.com/claritychallenge/clarity). \\n\\nThe baseline system perform NAL-R amplification according to the audiogram of the target listener, followed by a simple gain control and output of the signals to 16-bit stereo wav format. The system has been kept deliberately simple with no microphone array processing or attempt at noise cancellation.\\n\\nHASPI scores for the dev set have been measured. The scores are as follows.\\n\\n| System | HASPI |\\n| --- | ---- |\\n| Unprocessed | 0.1615 |\\n| NAL-R baseline | 0.2493 |\\n\\nSee [here](/docs/cec2/software/cec2_baseline) for further details.\\n\\nIf you have any problems using the baseline code please do not hesitate to contact us at [claritychallengecontact@gmail.com](mailto:claritychallengecontact@gmail.com), or post questions on the [Google group](https://groups.google.com/g/clarity-challenge)."},{"id":"launch of CEC2","metadata":{"permalink":"/blog/launch of CEC2","source":"@site/blog/2022-03-30-CEC2-launch.mdx","title":"Launch of CEC2","description":"We are pleased to announce the launch of the 2nd Clarity Enhancement Challenge (CEC2).","date":"2022-03-30T00:00:00.000Z","formattedDate":"March 30, 2022","tags":[{"label":"clarity","permalink":"/blog/tags/clarity"},{"label":"launch","permalink":"/blog/tags/launch"},{"label":"CEC2","permalink":"/blog/tags/cec-2"}],"readingTime":0.845,"hasTruncateMarker":false,"authors":[{"name":"Jon Barker","title":"Clarity Team Member","url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","imageURL":"https://avatars.githubusercontent.com/jonbarker68"}],"frontMatter":{"slug":"launch of CEC2","title":"Launch of CEC2","author":"Jon Barker","author_title":"Clarity Team Member","author_url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","author_image_url":"https://avatars.githubusercontent.com/jonbarker68","tags":["clarity","launch","CEC2"]},"unlisted":false,"prevItem":{"title":"Release of CEC2 baseline","permalink":"/blog/release of CEC2 baseline"},"nextItem":{"title":"Live events in January","permalink":"/blog/Jan-2-live-events"}},"content":"We are pleased to announce the launch of the **2nd Clarity Enhancement Challenge (CEC2)**.\\n\\nThe website has been fully updated to provide you with all the information you will need to participate in the challenge.\\n\\nThe schedule for the challenge is as follows:\\n\\n- **13th April 2022**: [Release](/docs/cec2/cec2_download) of training and development data; initial tools.\\n- **30th April 2022**: [Release](/docs/cec2/cec2_download) of full toolset and baseline system.\\n- **1st May 2022**: [Registration](/docs/cec2/taking_part/cec2_registration) for challenge entrants opens.\\n- **25th July 2022**: Evaluation data released\\n- **1st Sept 2022**: 1st round [submission](/docs/cec2/taking_part/cec2_submission) deadline for evaluation by objective measure\\n- **15th Sept 2022**: 2nd round [submission](/docs/cec2/taking_part/cec2_submission) deadline for listening tests\\n- **Sept-Nov 2022**: Listening test evaluation period.\\n- **2nd Dec 2022**:  Results announced at a Clarity Challenge Workshop; prizes awarded.\\n\\nThe challenge training, dev data and initial tools will be available from 13th April. In the meantime, please visit the [CEC2 Intro page](/docs/cec2/cec2_intro) to learn more about the task.\\n\\nIf you have any questions please do not hesitate to contact us at [claritychallengecontact@gmail.com](mailto:claritychallengecontact@gmail.com)."},{"id":"Jan-2-live-events","metadata":{"permalink":"/blog/Jan-2-live-events","source":"@site/blog/2022-01-07_Jan_live_events.mdx","title":"Live events in January","description":"The Clarity team are hosting two live sessions this month related to the Prediction Challenge. Everyone is welcome to attend, whether or not you have registered to participate in the challenge or are still considering signing up.","date":"2022-01-07T00:00:00.000Z","formattedDate":"January 7, 2022","tags":[{"label":"clarity","permalink":"/blog/tags/clarity"},{"label":"webinar","permalink":"/blog/tags/webinar"},{"label":"CPC1","permalink":"/blog/tags/cpc-1"}],"readingTime":1.61,"hasTruncateMarker":false,"authors":[{"name":"Lara Harris","title":"Clarity Team Member","url":"https://acoustictesting.salford.ac.uk/","imageURL":"https://avatars.githubusercontent.com/LaraHarris-Salford"}],"frontMatter":{"slug":"Jan-2-live-events","title":"Live events in January","author":"Lara Harris","author_title":"Clarity Team Member","author_url":"https://acoustictesting.salford.ac.uk/","author_image_url":"https://avatars.githubusercontent.com/LaraHarris-Salford","tags":["clarity","webinar","CPC1"]},"unlisted":false,"prevItem":{"title":"Launch of CEC2","permalink":"/blog/launch of CEC2"},"nextItem":{"title":"Introduction Webinar - Recording Available","permalink":"/blog/webinar-1-link"}},"content":"The Clarity team are hosting two live sessions this month related to the Prediction Challenge. Everyone is welcome to attend, whether or not you have registered to participate in the challenge or are still considering signing up.\\n\\n**The presentations will be very similar to the webinar in November.** These events are intended as a chance for people in different time zones to attend live and ask the team questions. \\n\\nHosting is via Microsoft Teams. You can join from your browser without needing to install Teams, but if you join from a mobile device you may need to install the Teams app.\\n\\n## Webinar - Challenge Overview\\n\\n### Friday 14th January \\n__9:00 GMT | 17:00 CST (GMT+8)__\\n\\n### [Click here to join the webinar](https://teams.microsoft.com/l/meetup-join/19%3ameeting_ZjFhNjJhMzYtOWEyMC00NjNiLThjOTEtYTIwMTk2YTczZGRh%40thread.v2/0?context=%7b%22Tid%22%3a%2265b52940-f4b6-41bd-833d-3033ecbcf6e1%22%2c%22Oid%22%3a%223f282402-9a2d-4e68-b2d4-3c1f86585a45%22%7d)\\n\\n\\nAn introduction to the aims of the challenge and some background to the problem of speech intelligibility prediction for hearing aids:\\n\\n- Welcome, introduction to Clarity.\\n- Speech intelligibility models: Overview and why are they needed.\\n- Hearing impairment speech intelligibility prediction.\\n- The prediction challenge - details and how you can sign up to participate.\\n- Audience questions / discussion.\\n\\nThe presentations will be recorded and made available online shortly after the event. The Q&A discussion will not be recorded.\\n\\nYou are welcome to join slightly later if you are only interested in joining for the Q&A section (presentations should finish around 9:40 GMT).\\n\\n\\n\\n## Live Q&A session\\n\\n### Monday 17th January\\n\\n__17:00 GMT | 12:00 EST (GMT-5) | 9:00 PST (GMT-8)__\\n\\n### [Click here to join the Q&A](https://teams.microsoft.com/l/meetup-join/19%3ameeting_YTJhOGRmZGMtMjRiZS00MGY0LTliNjctZmZhMzhmNDI5M2I0%40thread.v2/0?context=%7b%22Tid%22%3a%2265b52940-f4b6-41bd-833d-3033ecbcf6e1%22%2c%22Oid%22%3a%223f282402-9a2d-4e68-b2d4-3c1f86585a45%22%7d)\\n\\nA chance to ask the team questions about the Clarity Prediction Challenge - for anyone that could not attend the webinar on Friday 14th due to time zone differences.\\n\\n__Please note there will be no presentations in this session.__ The talks from Friday\u2019s webinar will be uploaded to the [Clarity project YouTube channel](https://www.youtube.com/channel/UCIc8FCHUA3Il9PUPt-sW1qw/videos) later in the day so you are invited to watch those before joining this live Q&A."},{"id":"webinar-1-link","metadata":{"permalink":"/blog/webinar-1-link","source":"@site/blog/2021-12-13_Webinar_1_link.mdx","title":"Introduction Webinar - Recording Available","description":"The Clarity team recently hosted a webinar to introduce the Prediction Challenge. The recording is now available to view online:","date":"2021-12-13T00:00:00.000Z","formattedDate":"December 13, 2021","tags":[{"label":"clarity","permalink":"/blog/tags/clarity"},{"label":"webinar","permalink":"/blog/tags/webinar"},{"label":"CPC1","permalink":"/blog/tags/cpc-1"}],"readingTime":0.455,"hasTruncateMarker":false,"authors":[{"name":"Lara Harris","title":"Clarity Team Member","url":"https://acoustictesting.salford.ac.uk/","imageURL":"https://avatars.githubusercontent.com/LaraHarris-Salford"}],"frontMatter":{"slug":"webinar-1-link","title":"Introduction Webinar - Recording Available","author":"Lara Harris","author_title":"Clarity Team Member","author_url":"https://acoustictesting.salford.ac.uk/","author_image_url":"https://avatars.githubusercontent.com/LaraHarris-Salford","tags":["clarity","webinar","CPC1"]},"unlisted":false,"prevItem":{"title":"Live events in January","permalink":"/blog/Jan-2-live-events"},"nextItem":{"title":"Welcome to CPC1","permalink":"/blog/welcome to CPC1"}},"content":"The Clarity team recently hosted a webinar to introduce the Prediction Challenge. The recording is now available to view online: \\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/G_9KczaoZY4\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n\\n\\n\\n\\n# Slides\\nThe slides are available to download:\\n\\n[1 Welcome and Overview](https://drive.google.com/file/d/1pycRrMv5OF9R6948Cda0QsJ4jh_Bek22/view?usp=sharing)\\n\\n[2 Speech Intelligibility Models](https://drive.google.com/file/d/1rScmDEUrtjBHG14VhWrKQu66-PiesU85/view?usp=sharing)\\n\\n[3 Hearing Impariment and SI Prediction](https://drive.google.com/file/d/1CQptm9sSIC8o2qHf_mtzQstLVhBnBzM_/view?usp=sharing)\\n\\n[4 Clarity Prediction Challenge Details](https://drive.google.com/file/d/1BVeqMbygIWyiIo61HEMjwjyEALxHipzP/view?usp=sharing)\\n\\n\\n\\nNote that we did not record the Q&A session at the end, but if you have questions about taking part in the challenge you can contact us at [claritychallengecontact@gmail.com](mailto:claritychallengecontact@gmail.com)"},{"id":"welcome to CPC1","metadata":{"permalink":"/blog/welcome to CPC1","source":"@site/blog/2021-07-08-CPC1_welcome.mdx","title":"Welcome to CPC1","description":"Welcome to the new Clarity CPC1 site for the first prediction challenge launching in autumn 2021. Feel free to look around. At the moment we\'re still doing listening tests and preparing the data, so the download links don\'t work. If anything is unclear or you\'ve got questions, please contact us through the Google group.","date":"2021-07-08T00:00:00.000Z","formattedDate":"July 8, 2021","tags":[{"label":"clarity","permalink":"/blog/tags/clarity"},{"label":"hello","permalink":"/blog/tags/hello"},{"label":"CPC1","permalink":"/blog/tags/cpc-1"}],"readingTime":0.27,"hasTruncateMarker":false,"authors":[{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"}],"frontMatter":{"slug":"welcome to CPC1","title":"Welcome to CPC1","author":"Trevor Cox","author_title":"Clarity Team Member","author_url":"http://trevorcox.me/trevor-cox","author_image_url":"https://avatars.githubusercontent.com/trevorjcox","tags":["clarity","hello","CPC1"]},"unlisted":false,"prevItem":{"title":"Introduction Webinar - Recording Available","permalink":"/blog/webinar-1-link"},"nextItem":{"title":"CEC1 submissions received","permalink":"/blog/CEC1 submissions received"}},"content":"Welcome to the new Clarity CPC1 site for the first prediction challenge launching in autumn 2021. Feel free to look around. At the moment we\'re still doing listening tests and preparing the data, so the download links don\'t work. If anything is unclear or you\'ve got questions, please contact us through the Google group."},{"id":"CEC1 submissions received","metadata":{"permalink":"/blog/CEC1 submissions received","source":"@site/blog/2021-06-16-submission-deadline.mdx","title":"CEC1 submissions received","description":"The CEC1 submission deadline has now passed. Thank you to all the teams who sent us signals.","date":"2021-06-16T00:00:00.000Z","formattedDate":"June 16, 2021","tags":[{"label":"clarity","permalink":"/blog/tags/clarity"},{"label":"submission","permalink":"/blog/tags/submission"},{"label":"CEC1","permalink":"/blog/tags/cec-1"}],"readingTime":0.79,"hasTruncateMarker":false,"authors":[{"name":"Jon Barker","title":"Clarity Team Member","url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","imageURL":"https://avatars.githubusercontent.com/jonbarker68"}],"frontMatter":{"slug":"CEC1 submissions received","title":"CEC1 submissions received","author":"Jon Barker","author_title":"Clarity Team Member","author_url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","author_image_url":"https://avatars.githubusercontent.com/jonbarker68","tags":["clarity","submission","CEC1"]},"unlisted":false,"prevItem":{"title":"Welcome to CPC1","permalink":"/blog/welcome to CPC1"},"nextItem":{"title":"CEC1 eval data released","permalink":"/blog/CEC1 eval data released"}},"content":"The CEC1 submission deadline has now passed. Thank you to all the teams who sent us signals. \\n\\nPlease remember to submit your finalised system descriptions by June 22nd to the Clarity workshop following the [instructions provided on the workshop website](https://claritychallenge.github.io/clarity2021-workshop/submissions.html).\\n\\nWe are currently busy evaluating the submissions using the MBSTOI metric. We will be contacting teams on the 22nd with details of how to prepare signals for the listening panel evaluation.\\n\\n**If you have been working on the challenge but missed the submission deadline** then [please do get in contact](mailto:claritychallengecontact@gmail.com). We will still be happy to receive your signals and system descriptions. Although late entries will not be eligible for the official challenge ranking, we will be happy to compute the eval set MBSTOI score for you and may even be able to arrange listening test evaluation through our panel.\\n\\nFor any questions please contact us at [claritychallengecontact@gmail.com ](mailto:claritychallengecontact@gmail.com) or by posting to the [Clarity challenge google group](https://groups.google.com/g/clarity-challenge?pli=1)."},{"id":"CEC1 eval data released","metadata":{"permalink":"/blog/CEC1 eval data released","source":"@site/blog/2021-06-02-eval-data.mdx","title":"CEC1 eval data released","description":"The evaluation dataset is now available to download from the myairbridge download site. The evaluation data filename is clarityCEC1data.sceneseval.v11.tgz.","date":"2021-06-02T00:00:00.000Z","formattedDate":"June 2, 2021","tags":[{"label":"clarity","permalink":"/blog/tags/clarity"},{"label":"evaluation","permalink":"/blog/tags/evaluation"},{"label":"CEC1","permalink":"/blog/tags/cec-1"}],"readingTime":1.24,"hasTruncateMarker":false,"authors":[{"name":"Jon Barker","title":"Clarity Team Member","url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","imageURL":"https://avatars.githubusercontent.com/jonbarker68"}],"frontMatter":{"slug":"CEC1 eval data released","title":"CEC1 eval data released","author":"Jon Barker","author_title":"Clarity Team Member","author_url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","author_image_url":"https://avatars.githubusercontent.com/jonbarker68","tags":["clarity","evaluation","CEC1"]},"unlisted":false,"prevItem":{"title":"CEC1 submissions received","permalink":"/blog/CEC1 submissions received"},"nextItem":{"title":"Baseline speech intelligibility model in round one","permalink":"/blog/baseline"}},"content":"The evaluation dataset is now available to download from the myairbridge [download site](https://mab.to/I9mkGx4wsiiaX). The evaluation data filename is `clarity_CEC1_data.scenes_eval.v1_1.tgz`.\\n\\nFull details of how to prepare your submission are now available on this site. Please read them carefully. \\n\\n**Registration**: Teams must register via the Google form on the [How To Submit](/docs/cec1/cec1_submission) page of this site. (Please complete this even if you have already completed a pre-registration form). Only one person from each team should register. Only those who have registered will be eligible to proceed to the evaluation. Once you have registered you will receive a confirmation email, a team ID and a link to a Google Drive to which you can upload your signals.\\n\\n**Submission deadline**: The deadline for submission is the **15th June**. \\n\\nThe submission consists of two components:\\n\\ni) a technical document of up to 2 pages describing the system/model and any external data and pre-existing tools, software and models used. This should be prepared as a Clarity-2021 workshop abstract and [submitted to the workshop](https://claritychallenge.github.io/clarity2021-workshop/).\\n\\nii) the set of processed signals that we will evaluate using the MBSTOI metric. Details of how to name and package your signals for upload can be found on the [How To Submit](/docs/cec1/cec1_submission) page.\\n\\n**Listening Tests:** Teams that do well in the MBSTOI evaluation will be notified on **22nd June** and invited to submit further signals for the second stage Listening Test evaluation.\\n\\nFor any questions please contact us at [claritychallengecontact@gmail.com ](mailto:claritychallengecontact@gmail.com) or by posting to the [Clarity challenge google group](https://groups.google.com/g/clarity-challenge?pli=1)."},{"id":"baseline","metadata":{"permalink":"/blog/baseline","source":"@site/blog/2021-04-22-baseline-speech-intelligiblity-model.mdx","title":"Baseline speech intelligibility model in round one","description":"Some comments on signal alignment and level-insensitivity","date":"2021-04-22T00:00:00.000Z","formattedDate":"April 22, 2021","tags":[{"label":"audibility","permalink":"/blog/tags/audibility"},{"label":"intelligibility","permalink":"/blog/tags/intelligibility"},{"label":"MBSTOI","permalink":"/blog/tags/mbstoi"},{"label":"baseline","permalink":"/blog/tags/baseline"},{"label":"CEC1","permalink":"/blog/tags/cec-1"}],"readingTime":3.755,"hasTruncateMarker":true,"authors":[{"name":"Simone Graetzer","title":"Clarity Team Member","url":"https://www.salford.ac.uk/our-staff/simone-graetzer","imageURL":"https://avatars.githubusercontent.com/sgraetzer"}],"frontMatter":{"slug":"baseline","title":"Baseline speech intelligibility model in round one","author":"Simone Graetzer","author_title":"Clarity Team Member","author_url":"https://www.salford.ac.uk/our-staff/simone-graetzer","author_image_url":"https://avatars.githubusercontent.com/sgraetzer","tags":["audibility","intelligibility","MBSTOI","baseline","CEC1"]},"unlisted":false,"prevItem":{"title":"CEC1 eval data released","permalink":"/blog/CEC1 eval data released"},"nextItem":{"title":"Latency, computation time and real-time operation","permalink":"/blog/Latency, computation time and real-time operation"}},"content":"### Some comments on signal alignment and level-insensitivity\\n\\nOur baseline binaural speech intelligibility measure in round one is the Modified Binaural Short-Time Objective Intelligibility measure, or MBSTOI. This short post outlines the importance of correcting for delays that your hearing aid processing algorithm introduces into the audio signals to allow MBSTOI to estimate the speech intelligibility accurately. It also discusses the importance of considering the audibility of signals before evaluation with MBSTOI.\\n\\n## Evaluation\\n\\nIn stage one, entries will be ranked according to the average MBSTOI score across all samples in the evaluation test set. In the second stage, entries will be evaluated by the listening panel. There will be prizes for both stages. See this [page](https://claritychallenge.github.io/clarity_CEC1_doc/docs/cec1_rules) for more information.\\n\\n\x3c!--truncate--\x3e\\n## Signal alignment in time and frequency\\n\\nIf the signal processed by the hearing aid introduces a significant delay, you should correct for this delay before submitting your entry. This is necessary because MBSTOI requires alignment of the clean speech \u201creference\u201d with the processed signal in time and frequency. This needs to be done for both ear signals.\\n\\nMBSTOI downsamples signals to 10 kHz, uses a Discrete Fourier Transform to decompose the signal into one-third octave bands, and performs envelope extraction and short-time segmentation into 386 ms regions. Each region consists of 30 frames. These approaches are motivated by what is know about which frequencies and modulation frequencies are most important for intelligibility. For each frequency band and frame (over the region of which it is the last frame), an intermediate correlation coefficient is calculated between the clean reference and processed power envelopes for each ear. These are averaged to obtain the MBSTOI index. Thus is usually between 0 and 1, and rises monotonically with measured intelligibility scores, such that higher values indicate greater speech intelligibility. Alignment is therefore required at the level of the one-third octave bands and short-time regions.\\n\\nOur baseline corrects for broadband delay per ear due to the hearing loss model. (The delay is measured by running a kronnecker delta function through the model for each ear.) However, the baseline software will not correct for delays created by your hearing aid processing.\\n\\nConsequently, when submitting your hearing aid output signals, you are responsible for correcting for any delays introduced by your hearing aid. Note that this must be done blindly; the clean reference signals will not be supplied for the test/evaluation set.\\n\\n\x3c!--truncate--\x3e\\n\\n## Level insensitivity\\n\\nMBSTOI is level-independent, i.e., MBSTOI is broadly insensitive to the level of the processed signal because it is calculated using a cross-correlation method. This could be a problem because sounds that are below the auditory thresholds of the hearing impaired listener may appear to MBSTOI to be highly intelligible.\\n\\nTo overcome this, the baseline experimental code mbstoi_beta, in conjunction with the baseline hearing loss model, can be used to approximate hearing-impaired auditory thresholds. Specifically, mbstoi_beta adds internal noise that can be used to approximate normal hearing auditory thresholds. This noise, in combination with the attenuation of signals by the hearing loss model to simulate raised auditory thresholds, makes MBSTOI level-sensitive.\\n\\nThe noise is created by filtering white noise using pure tone threshold filter coefficients with one-third octave weighting, approximating the shape of a typical auditory filter (from Moore 2012, based on Patterson\u2019s method, 1976). This noise is added to the processed signal. Note, the standard MBSTOI in the equalisation-cancellation stage adds internal noise to parameters, but this is an independent process.\\n\\n## MBSTOI\\n\\nThe method was developed by Asger Heidemann Andersen, Jan Mark de Haan, Zheng-Hua Tan and Jesper Jensen (Andersen et al., 2018). It builds on the Short-Time Objective Intelligibility (STOI) metric created by Cees H. Taal, Richard C. Hendriks, Richard Heusdens, and Jesper Jensen (Taal et al., 2011). MBSTOI includes a better ear stage and an equalisation-cancellation stage. For simplicity, the latter stage is not discussed here; see Andersen et al. (2018) for details.\\n\\n## References\\n\\n- Andersen, A. H., de Haan, J. M., Tan, Z. H., & Jensen, J. (2018). Refinement and validation of the binaural short time objective intelligibility measure for spatially diverse conditions. *Speech Communication*, 102, 1-13.\\n- Moore, B. C. (2012). *An introduction to the psychology of hearing*. Brill.\\n- Patterson, R. D. (1976). Auditory filter shapes derived with noise stimuli. *The Journal of the Acoustical Society of America*, 59(3), 640-654.\\n- Taal, C. H., Hendriks, R. C., Heusdens, R., & Jensen, J. (2011). An algorithm for intelligibility prediction of time\u2013frequency weighted noisy speech. *IEEE Transactions on Audio, Speech, and Language Processing*, 19(7), 2125-2136."},{"id":"Latency, computation time and real-time operation","metadata":{"permalink":"/blog/Latency, computation time and real-time operation","source":"@site/blog/2021-03-05-latency-computation-time-and-real-time-operation.mdx","title":"Latency, computation time and real-time operation","description":"An explanation of the time and computational limits for the first round of the enhancement challenge.","date":"2021-03-05T00:00:00.000Z","formattedDate":"March 5, 2021","tags":[{"label":"challenge","permalink":"/blog/tags/challenge"},{"label":"computation","permalink":"/blog/tags/computation"},{"label":"enhancement","permalink":"/blog/tags/enhancement"},{"label":"latency","permalink":"/blog/tags/latency"},{"label":"real-time","permalink":"/blog/tags/real-time"}],"readingTime":2.125,"hasTruncateMarker":true,"authors":[{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","image_url":"https://avatars.githubusercontent.com/trevorjcox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"}],"frontMatter":{"slug":"Latency, computation time and real-time operation","title":"Latency, computation time and real-time operation","authors":[{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","image_url":"https://avatars.githubusercontent.com/trevorjcox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"}],"tags":["challenge","computation","enhancement","latency","real-time"]},"unlisted":false,"prevItem":{"title":"Baseline speech intelligibility model in round one","permalink":"/blog/baseline"},"nextItem":{"title":"Clarity Challenge pre-announcement","permalink":"/blog/Clarity Challenge pre-announcement"}},"content":"import rehypeKatex from \'rehype-katex\'\\nimport {remarkMath} from \\"remark-math\\";\\n\\nAn explanation of the time and computational limits for the first round of the enhancement challenge.\\n\\n## The 1st Clarity Enhancement Challenge\\n\\nFor a hearing aid to work well for users, the processing needs to be quick. The output of the hearing aid should be produced with a delay of less than about 10 ms. Many audio processing techniques are non-causal, i.e., the output of the system depends on samples from the future. Such processing is useless for hearing aids and therefore our rules include a restriction on the use of future samples.\\n\\nThe rules state the following:\\n\\n- Systems must be causal; the output at time t must not use any information from input samples more than 5 ms into the future (i.e., no information from input samples >t+5ms).\\n- There is no limit on computational cost.\\n\\n\x3c!--truncate--\x3e\\n\\nMathematically this is:\\n\\ny<sub>n</sub>=f(x<sub>m</sub> , x<sub>m+1</sub> ... x<sub>n+N-1</sub> , x<sub>n+N</sub> , L )\\n\\n- where y<sub>n</sub> is the output from your hearing aid for sample $n$.\\n- $x$ is the audio input signal from a hearing aid microphone.\\n- $N = 0.005 fs$ where $fs$ is the sampling frequency.\\n- $m$ is a sample number where $m \\\\le n$.\\n- $L$ is the listener characteristics.\\n- $f()$ is the hearing aid function. There is no limitation on how long this takes to compute.\\n- You can use multiple microphones; only a single input signal $x$ is shown here just for simplicity.\\n\\nHere it is illustrated as a diagram.\\n\\n![latency diagram](/img/latency_diagram-1.png)\\n\\nFigure. Example of how the limit of 5 ms is applied to a hearing aid input and output signal.\\nWe have a chosen a limit of 5 ms because in a real hearing aid there will be other sources of delay (e.g., analogue-to-digital, digital-to-analogue conversion).\\n\\n## Why is there no limitation of how long f() takes to compute?\\n\\nWe\u2019re trying to foster new approaches to hearing aid processing and decided that at this stage we will drive more innovation if we don\u2019t restrict computation time for round one. Such restrictions will be considered in future rounds.\\n\\n## Why haven\u2019t you talked about latency?\\n\\nIn discussions, it is apparent that this term is used in different ways by different people, so to avoid confusion we\u2019re not using it!\\n\\n## Do algorithms have to be real-time?\\n\\nThe above limitations mean that the algorithms could in theory be made real-time if a powerful enough computer was available, but your entry can take as long as it needs to process the signals."},{"id":"Clarity Challenge pre-announcement","metadata":{"permalink":"/blog/Clarity Challenge pre-announcement","source":"@site/blog/2020-11-24-clarity-challenge-pre-announcement.mdx","title":"Clarity Challenge pre-announcement","description":"Although age-related hearing loss affects 40% of 55 to 74 year-olds, the majority of adults who would benefit from hearing aids don\u2019t use them. A key reason is simply that hearing aids don\u2019t provide enough benefit.","date":"2020-11-24T00:00:00.000Z","formattedDate":"November 24, 2020","tags":[{"label":"CEC1","permalink":"/blog/tags/cec-1"}],"readingTime":2.975,"hasTruncateMarker":true,"authors":[{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"}],"frontMatter":{"slug":"Clarity Challenge pre-announcement","title":"Clarity Challenge pre-announcement","author":"Trevor Cox","author_title":"Clarity Team Member","author_url":"http://trevorcox.me/trevor-cox","author_image_url":"https://avatars.githubusercontent.com/trevorjcox","tags":["CEC1"]},"unlisted":false,"prevItem":{"title":"Latency, computation time and real-time operation","permalink":"/blog/Latency, computation time and real-time operation"},"nextItem":{"title":"One approach to our enhancement challenge","permalink":"/blog/One approach to our enhancement challenge"}},"content":"Although age-related hearing loss affects 40% of 55 to 74 year-olds, the majority of adults who would benefit from hearing aids don\u2019t use them. A key reason is simply that hearing aids don\u2019t provide enough benefit.\\n\\nPicking out speech from background noise is a critical problem even for the most sophisticated devices. The purpose of the Clarity Challenges is to catalyse new work to radically improve the speech intelligibility provided by hearing aids.\\n\\nThe series of challenges will consider increasingly complex listening scenarios. The first round, launching in January 2021, will focus on speech in indoor environments in the presence of a single interferer. It will begin with a challenge involving improving hearing aid processing. Future challenges on how to model speech-in-noise perception will be launched at a later date.\\n\\n![Person using tablet](/img/UoN_HS-08207-1536x1024.jpeg)\\n\\n\x3c!--truncate--\x3e\\n\\n## The Task\\n\\nYou will be provided with simulated scenes, each including a target speaker and interfering noise. For each scene, there will be signals that simulate those captured by a behind-the-ear hearing aid with three channels at each ear and those captured at the eardrum without a hearing aid present.  The target speech will be a short sentence and the interfering noise will be either speech or domestic appliance noise.\\n\\nThe task will be to deliver a hearing aid signal processing algorithm that can improve the intelligibility of the target speaker for a specified hearing-impaired listener. Initially, entries will be evaluated using an objective speech intelligibility measure we will provide. Subsequently, up to twenty of the most promising systems will be evaluated by a panel of listeners.\\n\\nWe will provide a baseline system so that teams can choose to focus on individual components or to develop their own complete pipelines.\\n\\n\\n## What will be provided\\n\\n- Evaluation of the best entries by a panel of hearing-impaired listeners.\\n- Speech + interferer scenes for training and evaluation.\\n- An entirely new database of 10,000 spoken sentences\\n- Listener characterisations including audiograms and speech-in-noise testing.\\n- Software including tools for generating training data, a baseline hearing aid algorithm, a baseline model of hearing impairment, and a binaural objective intelligibility measure.\\n\\n## Important Dates\\n\\n- January 2021 \u2013 Challenge launch and release of software and data\\n- April 2021 \u2013  Evaluation data released\\n- May 2021 \u2013 Submission deadline\\n- June-August 2021  \u2013 Listening test evaluation period\\n- September 2021 \u2013 Results announced at a Clarity Challenge Workshop in conjunction with Interspeech 2021\\n\\nChallenge and workshop participants will be invited to contribute to a journal Special Issue on the topic of Machine Learning for Hearing Aid Processing that will be announced next year.\\n\\n## Further information\\n\\nIf you are interested in participating and wish to receive further information, please sign up.\\n\\nIf you have questions, contact us directly at contact@claritychallenge.org\\n\\n## Organisers\\n\\n- Prof. Jon P. Barker, Department of Computer Science, University of Sheffield\\n- Prof. Michael A. Akeroyd, Hearing Sciences, School of Medicine, University of Nottingham\\n- Prof. Trevor J. Cox, Acoustics Research Centre, University of Salford\\n- Prof. John F. Culling, School of Psychology, Cardiff University\\n- Prof. Graham Naylor, Hearing Sciences, School of Medicine, University of Nottingham\\n- Dr Simone Graetzer, Acoustics Research Centre, University of Salford\\n- Dr Rhoddy Viveros Mu\xf1oz, School of Psychology, Cardiff University\\n- Eszter Porter, Hearing Sciences, School of Medicine, University of Nottingham\\n\\nFunded by the Engineering and Physical Sciences Research Council (EPSRC), UK.\\n\\nSupported by RNID (formerly Action on Hearing Loss), Hearing Industry Research Consortium, Amazon TTS Research, Honda Research Institute Europe.\\n\\n## Acknowledgement\\n\\nThe image copyright is owned by the University of Nottingham."},{"id":"One approach to our enhancement challenge","metadata":{"permalink":"/blog/One approach to our enhancement challenge","source":"@site/blog/2020-07-29-one-approach-to-our-enhancement-challenge.mdx","title":"One approach to our enhancement challenge","description":"Improving hearing aid processing using DNNs blog. A suggested approach to overcome the non-differentiable loss function.","date":"2020-07-29T00:00:00.000Z","formattedDate":"July 29, 2020","tags":[{"label":"DNN","permalink":"/blog/tags/dnn"},{"label":"enhancement","permalink":"/blog/tags/enhancement"},{"label":"evaluation","permalink":"/blog/tags/evaluation"},{"label":"GAN","permalink":"/blog/tags/gan"},{"label":"hearing aid","permalink":"/blog/tags/hearing-aid"},{"label":"knowledge distillation","permalink":"/blog/tags/knowledge-distillation"}],"readingTime":3.59,"hasTruncateMarker":true,"authors":[{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","image_url":"https://avatars.githubusercontent.com/trevorjcox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"}],"frontMatter":{"slug":"One approach to our enhancement challenge","title":"One approach to our enhancement challenge","authors":[{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","image_url":"https://avatars.githubusercontent.com/trevorjcox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"}],"tags":["DNN","enhancement","evaluation","GAN","hearing aid","knowledge distillation"]},"unlisted":false,"prevItem":{"title":"Clarity Challenge pre-announcement","permalink":"/blog/Clarity Challenge pre-announcement"},"nextItem":{"title":"The speech-in-noise problem part two","permalink":"/blog/The speech-in-noise problem part two"}},"content":"Improving hearing aid processing using DNNs blog. A suggested approach to overcome the non-differentiable loss function.\\n\\n\\nThe aim of our Enhancement Challenge is to get people producing new algorithms for processing speech signals through hearing aids. We expect most entries to replace the classic hearing aid processing of Dynamic Range Compressors (DRCs) with [deep neural networks (DNN)](https://en.wikipedia.org/wiki/Deep_learning) (although all approaches are welcome!). The first round of the challenge is going to be all about improving speech intelligibility.\\n\\nSetting up a DNN structure and training regime for the task is not as straightforward as it might first appear. Figure 1 shows an example of a naive training regime. An audio example of Speech in Noise (SPIN) is randomly created (*audio sample generation*, bottom left), and a listener is randomly selected with particular hearing loss characteristics (*random artificial listener generation*, top left). The DNN Enhancement model (represented by the bright yellow box) then produces improved speech in noise. (Audio signals in pink are two-channel, left and right because this is for binaural hearing aids.)\\n\\n![schematic](/img/clarity_schematic_for_blog-09.png)\\n\\nFigure 1\\n\\nNext the improved speech in noise is passed to the Prediction Model in the lime green box, and this gives an estimation of the Speech Intelligibility (SI). Our baseline system will include algorithms for this. We\u2019ve already blogged about the Hearing Loss Simulation. Our current thinking is that the intelligibility model will be using a binaural form of the Short-Time Objective Intelligibility Index (STOI) [1]. The dashed line going back to the enhancement model shows that the DNN will be updated based on the reciprocal of the Speech Intelligibility (SI) score. By minimising (1/SI), the enhancement model will be maximising intelligibility.\\n\\n\x3c!--truncate--\x3e\\n\\nThe difficulty here is that updating the Enhancement Model DNN during training requires the error to be known at the DNN\u2019s output (the point labelled \u201cimproved SPIN\u201d). But we don\u2019t know this, we only know the error on the output of the prediction model at the far right of the diagram. This wouldn\u2019t be a problem if the prediction model could be inverted, because we could then run the 1/SI error backwards through the inverse model.\\n\\nAs the inverse of the prediction model isn\u2019t available, one solution is to train another DNN to mimic its behaviour (Figure 2). As this new Prediction Model is a DNN, the 1/SI error can be passed backwards through it using standard neural network training formulations.\\n\\n![schematic](/img/clarity_schematic_for_blog-10.png)\\n\\nThis DNN prediction model could be trained first using knowledge distillation ([this is something I\u2019ve previous done for a speech intelligibility model](http://usir.salford.ac.uk/id/eprint/56234/)), and then the weights frozen while the Enhancement Model is trained. But there is a \u2018chicken and egg\u2019 problem here. The difficulty is generating all the training data for the prediction model. Until you train the enhancement model, you won\u2019t have a representative examples of \u201cimproved SPIN\u201d to train the prediction model. But without the prediction model, you can\u2019t train the enhancement model.\\n\\nOne solution is to train the two DNNs in tandem, with an approach analogous to how pairs of networks are trained in a [Generative Adversarial Network](https://en.wikipedia.org/wiki/Generative_adversarial_network) (GAN). iMetricGan developed by Li et al. [2] is an example of this being done for speech enhancement, although the authors weren\u2019t trying to include hearing loss simulation. They aren\u2019t the only ones looking at trying to solve problems where a non-differentiable or black-box evaluation function is in the way of DNN training [3][4].\\n\\nWe hope our entrants will come up with lots of other ways of overcoming this problem. How would you tackle it?\\n\\n## References\\n\\n- [1] Andersen, A.H., Haan, J.M.D., Tan, Z.H. and Jensen, J., 2015. A binaural short time objective intelligibility measure for noisy and enhanced speech. In the *Sixteenth Annual Conference of the International Speech Communication Association*.\\n- [2] Li, H., Fu, S.W., Tsao, Y. and Yamagishi, J., 2020. iMetricGAN: Intelligibility Enhancement for Speech-in-Noise using Generative Adversarial Network-based Metric Learning. *arXiv preprint arXiv:2004.00932*.\\n- [3] Gillhofer, M., Ramsauer, H., Brandstetter, J., Sch\xe4fl, B. and Hochreiter, S., 2019. A GAN based solver of black-box inverse problems. Proceedings of the *NeurIPS 2019 Workshop*.\\n- [4] Kawanaka, M., Koizumi, Y., Miyazaki, R. and Yatabe, K., 2020, May. Stable training of DNN for speech enhancement based on perceptually-motivated black-box cost function. In ICASSP 2020-2020 *IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)* (pp. 7524-7528). IEEE."},{"id":"The speech-in-noise problem part two","metadata":{"permalink":"/blog/The speech-in-noise problem part two","source":"@site/blog/2020-07-06-the-speech-in-noise-problem-part-2.mdx","title":"The speech-in-noise problem part two","description":"How hearing aids address the problem of speech-in-noise in noisy and quieter places. We\u2019ll also discuss what machine learning techniques are often used for noise reduction, and some promising strategies for hearing aids.","date":"2020-07-06T00:00:00.000Z","formattedDate":"July 6, 2020","tags":[{"label":"machine learning","permalink":"/blog/tags/machine-learning"},{"label":"speech-in-noise","permalink":"/blog/tags/speech-in-noise"}],"readingTime":4.24,"hasTruncateMarker":true,"authors":[{"name":"Simone Graetzer","title":"Clarity Team Member","url":"https://www.salford.ac.uk/our-staff/simone-graetzer","image_url":"https://avatars.githubusercontent.com/sgraetzer","imageURL":"https://avatars.githubusercontent.com/sgraetzer"},{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","image_url":"https://avatars.githubusercontent.com/trevorjcox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"}],"frontMatter":{"slug":"The speech-in-noise problem part two","title":"The speech-in-noise problem part two","authors":[{"name":"Simone Graetzer","title":"Clarity Team Member","url":"https://www.salford.ac.uk/our-staff/simone-graetzer","image_url":"https://avatars.githubusercontent.com/sgraetzer","imageURL":"https://avatars.githubusercontent.com/sgraetzer"},{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","image_url":"https://avatars.githubusercontent.com/trevorjcox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"}],"tags":["machine learning","speech-in-noise"]},"unlisted":false,"prevItem":{"title":"One approach to our enhancement challenge","permalink":"/blog/One approach to our enhancement challenge"},"nextItem":{"title":"Hearing loss simulation","permalink":"/blog/Hearing loss simulation"}},"content":"How hearing aids address the problem of speech-in-noise in noisy and quieter places. We\u2019ll also discuss what machine learning techniques are often used for noise reduction, and some promising strategies for hearing aids.\\n\\n![Tablet user](/img/UoN_HS-08207-1536x1024.jpeg)\\n\\nIn a previous blog, we set out the problem of using hearing aids to pick out speech in noisy places. When the [signal-to-noise ratio (SNR)](https://en.wikipedia.org/wiki/Signal-to-noise_ratio) is low, hearing aids can only do so much to improve the intelligibility of the speech.\\n\\nA solitary hearing aid has various ways of addressing everyday constant noises such as cars, vacuum cleaners and fans. The aids work best when the noise is not too intrusive and SNR is relatively high. Problems arise when the noise is high (low SNRs), because then the hearing aid processing can distort the sound too much. While the hearing aid might have limited success in improving intelligibility in certain cases, they can still make the noise less annoying (e.g., Brons et al., 2014).\\n\\nUsing multiple microphones on each hearing aid can help in noisy conditions. The sound from the microphones is combined in a way that boosts the speech relative to the noise. This technology can be put into larger hearing aids, when there is enough spacing between the front and rear microphones.\\n\\nOne of the reasons why our brains are really good at picking out speech from the hubbub of a restaurant, is that it compares and contrasts the sounds from both ears. Our hearing is [binaural](https://en.wikipedia.org/wiki/Binaural). Similarly, if you have a hearing aids in both ears, they work better if they collaborate on reducing the noise.\\n\\nCrucial to how our brains locate sound and pick out speech in noise are timing and level cues that come from comparing the sound at both ears. When sound comes from the side:\\n\\n- interaural time differences occur because the sound arrives at one ear earlier than the other.\\n- interaural level differences occur because the sound has to bend around the head to reach the furthest ear.\\n\\nBinaural hearing aids communicate wirelessly and use noise reduction strategies that preserve these interaural time and level difference cues (e.g., Van den Bogaert et al., 2009). This allows the listener\u2019s brain to better locate the speech and boost this compared to the noise.\\n\\n\x3c!--truncate--\x3e\\n\\n## Machine learning\\n\\nIn recent years, there has been increasing interest in what [machine learning](https://en.wikipedia.org/wiki/Machine_learning) methods can do for hearing aids. Machine learning is a branch of artificial intelligence where computers learn directly from example data. One machine learning method is the neural network. This is an algorithm formed from layers of simple computational units connected to each other in a way that is inspired by connections between neurons in the brain. Deep (3+ layer) neural networks are able to learn complex, non-linear mapping functions, which makes them ideal candidates for noise reduction tasks.\\n\\nWe anticipate that machine learning can help tackle the challenge of speech in noise for hearing aids, providing a tailored solution for each individual and listening situation. For example, one thing machine learning could do is to sense the acoustic environment the listener is in, and choose the most suitable processing settings.\\n\\n![Electronic brain](/img/neural_brain.jpeg)\\n\\nImage via www.vpnsrus.com\\n\\nIn recent years, a machine learning approach for noise reduction has become popular. Neural networks are used to estimate time-frequency masks (a set of gains for each time-frequency unit that, when multiplied by the signal, produce less noisy speech; see, e.g., Zhao et al., 2018).\\n\\nMachine learning systems for noise reduction are trained on artificially mixed speech and noise. Some operate on a single channel, i.e., using spectral cues, and some work with multiple channels using spatial cues. We expect that future hearing aids built on machine learning will perform best if they combine the left and right microphones to work binaurally.\\n\\nMost of these noise reduction systems have been designed and evaluated in an off-line mode where they process pre-recorded signals. This isn\u2019t much use for hearing aids that need to work in real-time with low latency (i.e., short delays). One challenge for hearing aids is to redesign off-line approaches to work quickly enough without too much loss of performance.\\n\\nThe potential for machine learning to produce better approaches to hearing aid processing is what motivated the Clarity Project. If you\u2019re interested in hearing more as the challenges develop, please sign up.\\n\\n## References\\n\\n- Brons, I., Houben, R., and Dreschler, W. A. (2014). Effects of noise reduction on speech intelligibility, perceived listening effort, and personal preference in hearing-impaired listeners. *Trends in hearing*, 18, 1-10.\\n- Van den Bogaert, T., Doclo, S., Wouters, J., and Moonen, M. (2009). Speech enhancement with multichannel Wiener filter techniques in multimicrophone binaural hearing aids. *The Journal of the Acoustical Society of America*, 125(1), 360-371.\\n- Zhao, Y., Wang, D., Johnson, E. M., and Healy, E. W. (2018). A deep learning based segregation algorithm to increase speech intelligibility for hearing-impaired listeners in reverberant-noisy conditions. *The Journal of the Acoustical Society of America*, 144(3), 1627-1637.\\n\\n## Credits\\n\\nPhotograph of hearing aid wearer, copyright University of Nottingham.\\n\\nImage of brain with overlaid circuity made available by www.vpnsrus.com."},{"id":"Hearing loss simulation","metadata":{"permalink":"/blog/Hearing loss simulation","source":"@site/blog/2020-06-23-hearing-loss-simulation.mdx","title":"Hearing loss simulation","description":"What our hearing loss algorithms simulate, with audio examples to illustrate hearing loss.","date":"2020-06-23T00:00:00.000Z","formattedDate":"June 23, 2020","tags":[{"label":"baseline","permalink":"/blog/tags/baseline"},{"label":"hearing loss simulation","permalink":"/blog/tags/hearing-loss-simulation"}],"readingTime":3.75,"hasTruncateMarker":true,"authors":[{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","image_url":"https://avatars.githubusercontent.com/trevorjcox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"},{"name":"Simone Graetzer","title":"Clarity Team Member","url":"https://www.salford.ac.uk/our-staff/simone-graetzer","image_url":"https://avatars.githubusercontent.com/sgraetzer","imageURL":"https://avatars.githubusercontent.com/sgraetzer"}],"frontMatter":{"slug":"Hearing loss simulation","title":"Hearing loss simulation","authors":[{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","image_url":"https://avatars.githubusercontent.com/trevorjcox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"},{"name":"Simone Graetzer","title":"Clarity Team Member","url":"https://www.salford.ac.uk/our-staff/simone-graetzer","image_url":"https://avatars.githubusercontent.com/sgraetzer","imageURL":"https://avatars.githubusercontent.com/sgraetzer"}],"tags":["baseline","hearing loss simulation"]},"unlisted":false,"prevItem":{"title":"The speech-in-noise problem part two","permalink":"/blog/The speech-in-noise problem part two"},"nextItem":{"title":"Sounds for round one","permalink":"/blog/Sounds"}},"content":"What our hearing loss algorithms simulate, with audio examples to illustrate hearing loss.\\n\\nOur challenge entrants are going to use machine learning to develop better processing of speech in noise (SPIN) for hearing aids. For a machine learning algorithm to learn new ways of processing audio for the hearing impaired, it needs to estimate how the sound will be degraded by any hearing loss. Hence, we need an algorithm to simulate hearing loss for each of our listeners. The diagram belows shows our draft baseline system that was detailed in a previous blog. The hearing loss simulation is part of the prediction model. The Enhancement Model to the left is effectively the hearing aid and the Prediction Model to the right is estimating how someone will perceive the intelligibility of the speech in noise.\\n\\n![baseline](/img/baseline-1536x684.png)\\n\\nThe draft baseline system (where SPIN is speech in noise, DRC is Dynamic Range Compression, HL is Hearing Loss, SI is Speech Intelligibility and L & R are Left and Right).\\n\\nThere are different causes of hearing loss, but we\u2019re concentrating on the most common type that happens when you age ([presbycusis](https://www.nhs.uk/conditions/hearing-loss/)). [RNID](https://rnid.org.uk/) (formerly Action on Hearing Loss) estimate that more than 40% of people over the age of 50 have a hearing loss, and this rises to 70% of people who are older than 70.\\n\\nThe aspects of hearing loss we\u2019ve decided to simulate are\\n\\n1. The loss of ability to sense the quietest sounds (increase in absolute threshold).\\n2. How as an audible sound increases in level, the perceived increase in loudness is greater than normal (loudness recruitment) (Moore et al. 1996).\\n3. How the ear has a poorer ability to discriminate the frequency of sounds (impaired frequency selectivity).\\n\\n\x3c!--truncate--\x3e\\n\\n## Audio examples of hearing loss\\n\\nHere are two samples of speech in noise processed through the simulator. In each audio example there are three versions of the same sentence:\\n\\n1. Unimpaired hearing\\n2. Mild hearing impairment\\n3. Moderate to severe hearing impairment\\n\\n0 dB signal to noise ratio\\n\\n<audio controls>\\n<source src=\\"/audio/spin_example.wav\\" type=\\"audio/wav\\" />\\nYour browser does not support the audio element.\\n</audio>\\n\\nAnd here is an example where the noise is louder:\\n\\n<audio controls>\\n<source src=\\"/audio/spin_example-10SNR.wav\\" type=\\"audio/wav\\" />\\nYour browser does not support the audio element.\\n</audio>\\n\\nNoisier: -10dB signal to noise ratio\\n\\n## Acknowledgements\\n\\nThe hearing loss model we\u2019re using was generously supplied by [Michael Stone at the University of Manchester](https://www.research.manchester.ac.uk/portal/michael.stone.html) as MATLAB code and translated by us into Python. The original code was written by members of the Auditory Perception Group at the University of Cambridge, ca. 1991-2013, including Michael Stone, Brian Moore, Brian Glasberg and Thomas Baer. Information about the model can be found primarily in Nejime and Moore (1997), but also in Nejime and Moore (1998), Baer and Moore (1993 and 1994), and Moore and Glasberg (1993).\\n\\nThe original speech recordings come from the ARU corpus, University of Liverpool (Hopkins et al. 2019). This corpus is freely available at the link in the reference below.\\n\\n## References\\n\\n- Baer, T., & Moore, B. C. (1993). Effects of spectral smearing on the intelligibility of sentences in noise. *The Journal of the Acoustical Society of America*, 94(3), 1229-1241.\\n- Baer, T., & Moore, B. C. (1994). Effects of spectral smearing on the intelligibility of sentences in the presence of interfering speech. *The Journal of the Acoustical Society of America*, 95(4), 2277-2280.\\n- Hopkins, C., Graetzer, S., & Seiffert, G. (2019). ARU adult British English speaker corpus of IEEE sentences (ARU speech corpus) version 1.0 [data collection]. Acoustics Research Unit, School of Architecture, University of Liverpool, United Kingdom. DOI: 10.17638/datacat.liverpool.ac.uk/681. Retrieved from http://datacat.liverpool.ac.uk/681/.\\n- Moore, B. C., & Glasberg, B. R. (1993). Simulation of the effects of loudness recruitment and threshold elevation on the intelligibility of speech in quiet and in a background of speech. *The Journal of the Acoustical Society of America*, 94(4), 2050-2062.\\n- Moore, B. C., Glasberg, B. R., & Vickers, D. A. (1996). Factors influencing loudness perception in people with cochlear hearing loss. B. Kollmeier, *World Scientific*, Singapore, 7-18.\\n- Nejime, Y., & Moore, B. C. (1997). Simulation of the effect of threshold elevation and loudness recruitment combined with reduced frequency selectivity on the intelligibility of speech in noise. *The Journal of the Acoustical Society of America*, 102(1), 603-615.\\n- Nejime, Y., & Moore, B. C. (1998). Evaluation of the effect of speech-rate slowing on speech intelligibility in noise using a simulation of cochlear hearing loss. *The Journal of the Acoustical Society of America*, 103(1), 572-576."},{"id":"Sounds","metadata":{"permalink":"/blog/Sounds","source":"@site/blog/2020-06-18-sounds-for-round-one.mdx","title":"Sounds for round one","description":"We\u2019ll be challenging our contestants to find innovative ways of making speech more audible for hearing impaired listeners when there is noise getting in the way. But what noises should we consider? To aid us in choosing sounds and situations that are relevant to people with hearing aids, we held a focus group.","date":"2020-06-18T00:00:00.000Z","formattedDate":"June 18, 2020","tags":[{"label":"CEC1","permalink":"/blog/tags/cec-1"},{"label":"focus group","permalink":"/blog/tags/focus-group"},{"label":"noise","permalink":"/blog/tags/noise"},{"label":"sounds","permalink":"/blog/tags/sounds"}],"readingTime":3.425,"hasTruncateMarker":true,"authors":[{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"}],"frontMatter":{"slug":"Sounds","title":"Sounds for round one","author":"Trevor Cox","author_title":"Clarity Team Member","author_url":"http://trevorcox.me/trevor-cox","author_image_url":"https://avatars.githubusercontent.com/trevorjcox","tags":["CEC1","focus group","noise","sounds"]},"unlisted":false,"prevItem":{"title":"Hearing loss simulation","permalink":"/blog/Hearing loss simulation"},"nextItem":{"title":"The speech-in-noise problem","permalink":"/blog/The speech-in-noise problem"}},"content":"We\u2019ll be challenging our contestants to find innovative ways of making speech more audible for hearing impaired listeners when there is noise getting in the way. But what noises should we consider? To aid us in choosing sounds and situations that are relevant to people with hearing aids, we held a focus group.\\n\\nWe wanted to know about\\n\\n- Everyday background noises that make having a conversation difficult.\\n- The characteristics of speech after it has been processed by a hearing-aid that hearing aid listeners would value.\\n\\nA total of eight patients (four males, four females) attended the meeting, six of whom were recruited from the [Nottingham Biomedical Research Centre\u2019s](https://nottinghambrc.nihr.ac.uk/) patient and public involvement contact list. Two attendees were recruited from a local lip reading class organised by the [Nottinghamshire Deaf Society](https://www.nottsdeaf.org.uk/). The range of hearing loss within the group is from mild to severe. They all regularly use bilateral hearing aids.\\n\\nOur focus was on the living room because that is the scenario for round one of the challenges.\\n\\n![People Listening](/img/photo-of-people-sitting-on-sofa-3890171.jpeg)\\n\\nPhoto by Gustavo Fring from Pexels\\n\\n\x3c!--truncate--\x3e\\n\\n## Everyday background noises that interfere with understanding of speech\\n\\nA long and varied list of sounds cause problems. These lists are in no particular order.\\n\\n- Living room or space\\n- Clocks ticking\\n- Crisp packets rustling\\n- Taps running\\n- Kettles boiling\\n- Dishwasher\\n- Microwave\\n- Washing machine\\n- TV, music, radio\\n- Phone ringing (or receiving texts \u2013 unknown beeps/tones)\\n- Newspapers rustling\\n- Air-conditioning and oven extractor fans\\n- Vacuum cleaner\\n- Doorbell ringing\\n- Dog barking\\n- Rain on window\\n   \\n## Family and friends\\n\\n- Cutlery/crockery banging/clanging\\n- Doors opening/closing (to rooms and cupboards)\\n- Music\\n- People walking around the room\\n- Children playing with toys\\n- Laughing\\n- People talking from another room\\n- Speakers from a different conversation in close proximity (i.e. beside you) when you are trying to converse\\n- Traffic outside\\n- Chewing/chomping\\n- Steam pipes/ coffee machines\\n- Chairs being moved\\n\\n## Outside\\n\\n- Church bells\\n- Market noise\\n- Footsteps on different types of ground, i.e. heels on hard floors but also wellingtons in mud\\n- Clothes rustling (such as waterproof coats or hat on hearing aid)\\n- Wind (even with HA on \u2018wind setting\u2019)\\n- Pigeons/birds\\n- Sirens\\n- Traffic noise (especially at junctions)\\n- Music\\n- Laughter\\n- Phones ringing\\n- Tills\\n- Children playing outside or running around (in shops, on the street and at parks)\\n- Beeping signal at crossings\\n- Garden centres \u2013 high glass ceilings, open plan, trolleys\\n- Road/ tyre and traffic noise when in a car or on the bus\\n- Also mentioned how people you speak to in the car may be in front or behind you\\n- Trains and the tube\\n- Aeroplanes and airports (suitcases rolling)\\n- Tannoys\\n\\n## Characteristics of processed speech to consider\\n\\n- Clarity (clearness) or quality\\n- Rhythm of speech\\n- \u2018Inflection\u2019 (intonation)\\n- Similarity to original speaker\\n- Agreed that in situations where the voice would not be processed clearly, i.e. outside with many noise sources, not sounding like the original - speaker is fine.\\n\\n## Other comments\\n\\n- Speed of speech; it was suggested that we have sentences read at different speeds as faster talkers are often harder to understand.\\n- Stated that emphasis on key words is useful for following conversation; perhaps key words in the sentence when marked should be given higher value.\\n- Lots of comments on room acoustics, i.e., ceiling heights, furnishings, floorings, windows etc., which has a big impact on how difficult it is to have a conversation with background noise.\\n- Different accents of talkers can make conversation more difficult; including speakers with different accents in the background.\\n- We\u2019re now working out what sounds to use. But are there other sounds we should consider?\\n\\n## Credits\\n\\n- Thank you to the patient and public involvement representatives who participated.\\n- Clarity Organiser: Eszter Porter .\\n- Facilitators: Adele Horobin, Erin Dawe-Lane.\\n- This discussion group was supported by the [National Institute for Health Research Nottingham Biomedical Research Centre](https://nottinghambrc.nihr.ac.uk/).\\n![NIHR logo](/img/nihr-nbrc.png)"},{"id":"The speech-in-noise problem","metadata":{"permalink":"/blog/The speech-in-noise problem","source":"@site/blog/2020-06-18-the-speech-in-noise-problem.mdx","title":"The speech-in-noise problem","description":"People often have problems understanding speech in noise, and this is one of the main deficits of hearing aids that our machine learning challenges will address.","date":"2020-06-18T00:00:00.000Z","formattedDate":"June 18, 2020","tags":[{"label":"cocktail party","permalink":"/blog/tags/cocktail-party"},{"label":"hearing","permalink":"/blog/tags/hearing"},{"label":"hearing aid","permalink":"/blog/tags/hearing-aid"},{"label":"noise","permalink":"/blog/tags/noise"},{"label":"speech","permalink":"/blog/tags/speech"}],"readingTime":3.725,"hasTruncateMarker":true,"authors":[{"name":"Simone Graetzer","title":"Clarity Team Member","url":"https://www.salford.ac.uk/our-staff/simone-graetzer","image_url":"https://avatars.githubusercontent.com/sgraetzer","imageURL":"https://avatars.githubusercontent.com/sgraetzer"},{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","image_url":"https://avatars.githubusercontent.com/trevorjcox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"}],"frontMatter":{"slug":"The speech-in-noise problem","title":"The speech-in-noise problem","authors":[{"name":"Simone Graetzer","title":"Clarity Team Member","url":"https://www.salford.ac.uk/our-staff/simone-graetzer","image_url":"https://avatars.githubusercontent.com/sgraetzer","imageURL":"https://avatars.githubusercontent.com/sgraetzer"},{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","image_url":"https://avatars.githubusercontent.com/trevorjcox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"}],"image":"https://i.imgur.com/mErPwqL.png","tags":["cocktail party","hearing","hearing aid","noise","speech"]},"unlisted":false,"prevItem":{"title":"Sounds for round one","permalink":"/blog/Sounds"},"nextItem":{"title":"Why use machine learning challenges for hearing aids?","permalink":"/blog/Why use machine learning challenges for hearing aids"}},"content":"People often have problems understanding speech in noise, and this is one of the main deficits of hearing aids that our machine learning challenges will address.\\n\\n\\n![cocktail party](/img/cocktail_party.jpeg)\\n\\n\\nIt\u2019s common for us to hear sounds coming simultaneously from different sources. Our brains then need to separate out what we want to hear (the target speaker) from the other sounds. This is especially difficult when the competing sounds are speech. This has the quaint name, The Cocktail Party Problem (Cherry, 1953). We don\u2019t go to many cocktail parties, but we encounter lots of times where the The Cocktail Party Problem is important. Hearing a conversation in a busy restaurant, trying to understand a loved one while the television is on or hearing the radio in the kitchen when the kettle is boiling, are just a few examples.\\n\\nDifficulty in picking out speech in noise is really common if you have a hearing loss. Indeed, it\u2019s often when people have problems doing this that they realise they have a hearing loss.\\n\\n```\\n\u201cHearing aids don\u2019t work when there is a lot of background noise. This is when you need them to work.\u201d\\n\\n-- Statement from a hearing aid wearer (Kochkin, 2000)\\n```\\n\\nHearing aids are the the most common form of treatment for hearing loss. However, surveys indicate that at least 40% of hearing aids are never or rarely used (Knudsen et al., 2010). A major reason for this is dissatisfaction with performance. Even the best hearing aids perform poorly for speech in noise. This is particularly the case when there are many people talking at the same time, and when the amount of noise is relatively high (i.e., the signal-to-noise ratio (SNR) is low). As hearing ability worsen with age, the ability to understand speech in background noise also reduces (e.g., Akeroyd, 2008).\\n\\n\x3c!--truncate--\x3e\\n\\nWhen an audiologist assesses hearing loss, one thing they measure is the pure tone audiogram. This assesses the quietest sound someone can hear over a range of frequencies. However, an audiogram only partly explains your experience with speech in background noise (Heinrich et al. 2015), because it only measures the quietest sound you can hear. For example, picking out speech from noise is a complex task for the brain to perform, and this cognitive ability isn\u2019t assessed by an audiogram. In addition, there are other factors that are important such as personality, motivation, attitude toward hearing aids and prior hearing aid experience.\\n\\n![Audiogram](/img/audiogram.jpg)\\n\\nAn audiogram displaying a \u201cski slope\u201d pattern that is a sign of age-related hearing loss (source: Ronan and Barrett, BMJ, 2014).\\nSpeech-in-noise tests get closer to the real-life problem a hearing aid is trying to solve. Listeners listen to speech in the presence of noise and write down what words they hear. More words correct show an increase in the ability to understand speech in specific noisy situations when listeners are wearing their hearing aid (aided) relative to when they are not (unaided). Of course, listening conditions in the clinic differ from real-life conditions.\\n\\nCurrently, while speech-in-noise test scores can be useful when fine-tuning a hearing aid, even then many users are disappointed about the performance of their hearing aids. Through our challenges, we hope to improve this situation, whether you go to cocktail parties or not.\\n\\nWhat\u2019s your experience with speech in noise? Please comment below.\\n\\n## References\\n\\n- Akeroyd, M. A. (2008). Are individual differences in speech reception related to individual differences in cognitive ability? A survey of twenty experimental studies with normal and hearing-impaired adults. *International Journal of Audiology*, 47(sup2), S53-S71.\\n- Cherry, E. C. (1953). Some experiments on the recognition of speech, with one and with two ears. *The Journal of the Acoustical Society of America*, 25(5), 975-979.\\n- Heinrich, A., Henshaw, H., and Ferguson, M. A. (2015). The relationship of speech intelligibility with hearing sensitivity, cognition, and perceived hearing difficulties varies for different speech perception tests. *Frontiers in Psychology*, 6, 782.\\n- Vestergaard Knudsen, L., \xd6berg, M., Nielsen, C., Naylor, G., and Kramer, S. E. (2010). Factors influencing help seeking, hearing aid uptake, hearing aid use and satisfaction with hearing aids: A review of the literature. *Trends in Amplification*, 14(3), 127-154.\\n- Kochkin, S. (2000). MarkeTrak V: \u201cWhy my hearing aids are in the drawer\u201d The consumers\u2019 perspective. *The Hearing Journal*, 53(2), 34-36.\\n\\n## Credits\\n\\n- Photo of Cocktail party by Ross CC BY-NC-SA 2.0\\n- Ronan, N., & Barrett, G. (2014). A 68 year old woman with deteriorating hearing. BMJ, 348, g2984. https://www.bmj.com/content/348/bmj.g2984"},{"id":"Why use machine learning challenges for hearing aids","metadata":{"permalink":"/blog/Why use machine learning challenges for hearing aids","source":"@site/blog/2020-05-06-why-use-machine-learning-challenges.mdx","title":"Why use machine learning challenges for hearing aids?","description":"The Clarity Project is based around the idea that machine learning challenges could improve hearing aid signal processing. After all this has happened in other areas, such as automatic speech recognition (ASR) in the presence of noise. The improvements in ASR have happened because of:","date":"2020-05-06T00:00:00.000Z","formattedDate":"May 6, 2020","tags":[{"label":"challenges","permalink":"/blog/tags/challenges"},{"label":"CHiME","permalink":"/blog/tags/c-hi-me"},{"label":"enhancement","permalink":"/blog/tags/enhancement"},{"label":"machine learning","permalink":"/blog/tags/machine-learning"},{"label":"prediction","permalink":"/blog/tags/prediction"}],"readingTime":2.21,"hasTruncateMarker":false,"authors":[{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","image_url":"https://avatars.githubusercontent.com/trevorjcox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"}],"frontMatter":{"slug":"Why use machine learning challenges for hearing aids","title":"Why use machine learning challenges for hearing aids?","authors":[{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","image_url":"https://avatars.githubusercontent.com/trevorjcox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"}],"tags":["challenges","CHiME","enhancement","machine learning","prediction"]},"unlisted":false,"prevItem":{"title":"The speech-in-noise problem","permalink":"/blog/The speech-in-noise problem"},"nextItem":{"title":"The baseline","permalink":"/blog/The baseline"}},"content":"The Clarity Project is based around the idea that machine learning challenges could improve hearing aid signal processing. After all this has happened in other areas, such as automatic speech recognition (ASR) in the presence of noise. The improvements in ASR have happened because of:\\n\\n- Machine learning (ML) at scale \u2013 big data and raw [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit) power.\\n- Benchmarking \u2013 research has developed around community-organised evaluations or challenges.\\n- Collaboration has been enabled by these challenges, allowing working across communities such as signal processing, acoustic modelling, language modelling and machine learning\\n\\nWe\u2019re hoping that these three mechanisms can drive improvements in hearing aids.\\n\\n## Components of a challenge\\n\\nThere needs to be a common task based on a target application scenario to allow communities to gain from benchmarking and collaboration. Clarity project\u2019s first enhancement challenge will be about hearing speech from a single talker in a typical living room, where there is one source of noise and a little reverberation.\\n\\nWe\u2019re currently working on developing simulation tools to allow us to generate our living room data. The room acoustic will be simulated using [RAVEN](https://www.semanticscholar.org/paper/RAVEN%3A-A-real-time-framework-for-the-auralization-Schr%C3%B6der-Vorl%C3%A4nder/6977f2c2c1fb4cac2305e7965ee0da8192ced72d?p2df) and [the Hearing Device Head-related Transfer Functions will come from Denk\u2019s work](https://uol.de/mediphysik/downloads/hearingdevicehrtfs). We\u2019re working on getting better, more ecologically valid speech than is often used in speech intelligibility work.\\n\\n![baseline](/img/baseline-1536x684.png)\\n\\nEntrants are then given training data and development (dev) test data along with a baseline system that represents the current state-of-the-art. You can find a post and video on the current thinking on the baseline here. We\u2019re still working on the rules stipulating what is and what is not allowed (for example, will entrants be allowed to use data from outside the challenge).\\n\\nClarity\u2019s first enhancement challenge is focussed on maximising the speech intelligibility (SI) score. We will evaluate this first through a prediciton model that is based on a hearing loss simulation and an objective metric for speech intellibility. Simulation has been hugely important for generating training data in the [CHIME challenges](https://chimechallenge.github.io/chime6/) and so we intend to use that approach in Clarity. But results from simulated test sets cannot be trusted and hence a second evaluation will come through perceptual tests on hearing impaired subjects. However, one of our current problems is that we can\u2019t bring listeners into our labs because of COVID-19.\\n\\nWe\u2019ll actually be running two challenges in roughly parallel, because we\u2019re also going to task the community to improve our prediction model for speech intelligibility.\\n\\nWe\u2019re running a series of challenges over five years. What other scenarios should we consider? What speech? What noise? What environment? Please comment below.\\n\\n## Acknowledgements\\n\\nMuch of this text is based on [Jon Barker\u2019s 2020 SPIN keynote](http://2020.speech-in-noise.eu/?p=3)"},{"id":"The baseline","metadata":{"permalink":"/blog/The baseline","source":"@site/blog/2020-04-29-the-baseline.mdx","title":"The baseline","description":"An overview of the current state of the baseline we\u2019re developing for the machine learning challenges","date":"2020-04-29T00:00:00.000Z","formattedDate":"April 29, 2020","tags":[{"label":"baseline","permalink":"/blog/tags/baseline"},{"label":"enhancement","permalink":"/blog/tags/enhancement"},{"label":"processing","permalink":"/blog/tags/processing"}],"readingTime":0.82,"hasTruncateMarker":false,"authors":[{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","image_url":"https://avatars.githubusercontent.com/trevorjcox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"}],"frontMatter":{"slug":"The baseline","title":"The baseline","authors":[{"name":"Trevor Cox","title":"Clarity Team Member","url":"http://trevorcox.me/trevor-cox","image_url":"https://avatars.githubusercontent.com/trevorjcox","imageURL":"https://avatars.githubusercontent.com/trevorjcox"}],"tags":["baseline","enhancement","processing"]},"unlisted":false,"prevItem":{"title":"Why use machine learning challenges for hearing aids?","permalink":"/blog/Why use machine learning challenges for hearing aids"},"nextItem":{"title":"Welcome","permalink":"/blog/welcome"}},"content":"\x3c!-- import { TwitterTimelineEmbed } from \\"react-twitter-embed\\"; --\x3e\\n\\nAn overview of the current state of the baseline we\u2019re developing for the machine learning challenges\\n\\n![The baseline](/img/baseline-1536x684.png)\\n\\nWe\u2019re currently developing the baseline processing that challenge entrants will need. This takes a random listener and a random audio sample of speech in noise (SPIN) and passes that through a simulated hearing aid (the Enhancement Model). This improves the speech in noise. We then have an algorithm (the Prediction Model) to estimate the Speech Intelligibility that the listener would perceive (SI score). This score can then be used to drive machine learning to improve the hearing aid.\\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/I1v8_TmXkeA\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowfullscreen></iframe>\\n\\n*A talk through the baseline model we\u2019re developing.*\\n\\nThe first machine learning challenge is to improve the enhancement model, in other words, to produce a better processing algorithm for the hearing aid. The second challenge is to improve the prediction model using perceptual data we\u2019ll provide."},{"id":"welcome","metadata":{"permalink":"/blog/welcome","source":"@site/blog/2020-04-28-welcome.mdx","title":"Welcome","description":"Welcome to the new Clarity blog. We will be using this blog to post regular updates about our Challenges and Workshop, as well as posts discussing the tools and techniques that we are using in our baseline systems.","date":"2020-04-28T00:00:00.000Z","formattedDate":"April 28, 2020","tags":[{"label":"clarity","permalink":"/blog/tags/clarity"},{"label":"hello","permalink":"/blog/tags/hello"},{"label":"CEC1","permalink":"/blog/tags/cec-1"}],"readingTime":0.19,"hasTruncateMarker":false,"authors":[{"name":"Jon Barker","title":"Clarity Team Member","url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","imageURL":"https://avatars.githubusercontent.com/jonbarker68"}],"frontMatter":{"slug":"welcome","title":"Welcome","author":"Jon Barker","author_title":"Clarity Team Member","author_url":"http://staffwww.dcs.shef.ac.uk/people/J.Barker/","author_image_url":"https://avatars.githubusercontent.com/jonbarker68","tags":["clarity","hello","CEC1"]},"unlisted":false,"prevItem":{"title":"The baseline","permalink":"/blog/The baseline"}},"content":"Welcome to the new Clarity blog. We will be using this blog to post regular updates about our Challenges and Workshop, as well as posts discussing the tools and techniques that we are using in our baseline systems."}]}')}}]);