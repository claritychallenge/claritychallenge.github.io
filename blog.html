<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-list-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Latest News | The Clarity Project</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://claritychallenge.github.io/blog"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="Latest News | The Clarity Project"><meta data-rh="true" name="description" content="News and updates about our Challenges and Workshops"><meta data-rh="true" property="og:description" content="News and updates about our Challenges and Workshops"><meta data-rh="true" name="docusaurus_tag" content="blog_posts_list"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_posts_list"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://claritychallenge.github.io/blog"><link data-rh="true" rel="alternate" href="https://claritychallenge.github.io/blog" hreflang="en"><link data-rh="true" rel="alternate" href="https://claritychallenge.github.io/blog" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="The Clarity Project RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="The Clarity Project Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-198878187-1","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>






<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons"><link rel="stylesheet" href="/assets/css/styles.b39ccc75.css">
<link rel="preload" href="/assets/js/runtime~main.4d80ea3e.js" as="script">
<link rel="preload" href="/assets/js/main.97869bb1.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.png" alt="Clarity Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.png" alt="Clarity Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Clarity</b></a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Shortcuts</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/cpc2/cpc2_intro">I want to learn more about the CPC2 challenge...</a></li><li><a class="dropdown__link" href="/docs/cpc2/taking_part/cpc2_download">I want to download the data...</a></li><li><a href="https://github.com/claritychallenge/clarity" target="_blank" rel="noopener noreferrer" class="dropdown__link">I want to see the code on GitHub...<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a class="dropdown__link" href="/docs/cpc2/taking_part/cpc2_registration">I want to register a team...</a></li><li><a class="dropdown__link" href="/docs/cpc2/taking_part/cpc2_submission">I want to submit results...</a></li></ul></div></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Challenges</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/cpc2/cpc2_intro">CPC2</a></li><li><a class="dropdown__link" href="/docs/icassp2023/icassp2023_intro">ICASSP 2023 Grand Challenge</a></li><li><a class="dropdown__link" href="/docs/cec2/cec2_intro">CEC2</a></li><li><a class="dropdown__link" href="/docs/cpc1/cpc1_intro">CPC1</a></li><li><a class="dropdown__link" href="/docs/cec1/cec1_intro">CEC1</a></li><li><a class="dropdown__link" href="/timeline">Future Challenges</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Workshops</a><ul class="dropdown__menu"><li><a href="https://claritychallenge.github.io/clarity2023-workshop/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Clarity 2023<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://claritychallenge.github.io/clarity2022-CEC2-workshop/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Clarity CEC2 2022, Dec<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://claritychallenge.github.io/clarity2022-workshop/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Clarity 2022, Jun<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://claritychallenge.github.io/clarity2021-workshop/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Clarity 2021<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Software</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/tutorials">Tutorials</a></li><li><a href="https://github.com/claritychallenge/clarity" target="_blank" rel="noopener noreferrer" class="dropdown__link">GitHub<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><a class="navbar__item navbar__link" href="/docs/learning/learning_intro">Learning</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">About Us</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/about">About Us</a></li><li><a class="dropdown__link" href="/contact">Contact Us</a></li><li><a class="dropdown__link" href="/timeline">Project timeline</a></li></ul></div><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Latest</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Search" aria-label="Search" class="navbar__search-input search-bar"></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/CPC2 eval data released">CPC2 eval data released</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Clarity-2023 Workshop @ Interspeech, Dublin">Clarity-2023 Workshop @ Interspeech, Dublin</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Announcing CPC2">Announcing the 2nd Clarity Prediction Challenge (CPC2)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/ICASSP 2023 evaluation data released">ICASSP 2023 evaluation data released</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Announcement of ICASSP 2023 Grand Challenge">Announcement of ICASSP 2023 Grand Challenge</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/CPC1 results and prizes">CPC1 results and prizes</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/CEC2 registration open">CEC2 registration open</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/release of CEC2 baseline">Release of CEC2 baseline</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/launch of CEC2">Launch of CEC2</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Jan-2-live-events">Live events in January</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/CPC2 eval data released">CPC2 eval data released</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-07-03T00:00:00.000Z" itemprop="datePublished">July 3, 2023</time> · <!-- -->One min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/jonbarker68" alt="Jon Barker"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jon Barker</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>The CPC2 evaluation data has now been released.</p><p>The data is available for download as a single 478 MB file, <a href="https://mab.to/138typ3cpVBmK" target="_blank" rel="noopener noreferrer">clarity_CPC2_data.test.v1_0.tgz</a>. The evaluation data should be untarred into the same root as the training data. Further details can be found on the <a href="https://claritychallenge.org/docs/cpc2/cpc2_intro" target="_blank" rel="noopener noreferrer">challenge website</a>.</p><p>The data consists of the hearing aid algorithm output signals, clean reference signals, listener audiograms, and head rotation information. Listener responses are not provided for the evaluation data but will be made available after the submission window has closed.</p><p>For details on how to prepare your submission <a href="https://claritychallenge.org/docs/cpc2/taking_part/cpc2_submission" target="_blank" rel="noopener noreferrer">please see the instructions on the website</a>.</p><p>If you have any questions please feel free to post them on this forum.</p><p>The submission window will close on the 31st of July.</p><p>Good luck!</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/clarity">clarity</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/announcement">announcement</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/cpc-2">CPC2</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/Clarity-2023 Workshop @ Interspeech, Dublin">Clarity-2023 Workshop @ Interspeech, Dublin</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-06-21T00:00:00.000Z" itemprop="datePublished">June 21, 2023</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/jonbarker68" alt="Jon Barker"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jon Barker</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>We are pleased to announce the <strong>4th ISCA Clarity Workshop on Machine Learning Challenges for Hearing Aids (Clarity-2023)</strong>.</p><p>The event will be a one-day workshop held as an ISCA satellite event to Interspeech 2023 in Dublin, Ireland.</p><p>For registration and programme details please visit the workshop website</p><p><a href="https://claritychallenge.github.io/clarity2023-workshop/" target="_blank" rel="noopener noreferrer">https://claritychallenge.github.io/clarity2023-workshop/</a></p><p><strong>IMPORTANT DATES</strong></p><ul><li>2nd June 2023 - Workshop Submission Deadline (Regular Papers)</li><li>31st July 2023 - Workshop Submission Deadline (Clarity Challenge Papers)</li><li>5th August 2023 - Registration closes</li><li>19th August - Workshop / Clarity Challenge results announced</li></ul><p><strong>About</strong></p><p>One of the biggest challenges for hearing-impaired listeners is understanding speech in the presence of background noise. Everyday social noise levels can have a devastating impact on speech intelligibility. The inability to communicate effectively can lead to social withdrawal and isolation. Disabling hearing impairment affects 360 million people worldwide, with that number increasing because of the ageing population. Unfortunately, current hearing aid technology is often ineffective in noisy situations. Although amplification can restore audibility, it does not compensate fully for the effects of hearing loss.</p><p>The Clarity workshops are designed to stimulate a two-way conversation between the speech research community and hearing aid developers. Hearing aid developers, who are not typically represented at Interspeech, will have an opportunity to present the challenges of their industry to the speech community; the speech community will be able to present and discuss potentially transformative approaches to speech in noise processing in the presence of hearing researchers and industry experts.</p><p><strong>Topics</strong></p><p>Any work related to the challenges of hearing aid signal processing will be considered relevant topics include,</p><ul><li>Binaural technology for speech enhancement and source separation</li><li>Multi-microphone processing technology</li><li>Real-time approaches to speech enhancement</li><li>Statistical model-driven approaches to hearing aid processing</li><li>Audio quality &amp; intelligibility assessment hearing aid and cochlear implant users</li><li>Efficient and effective integration of psychoacoustic testing in machine learning</li><li>Machine learning for diverse target listeners</li><li>Machine learning models of hearing impairment</li></ul><p><strong>The 2nd Clarity Prediction Challenge</strong></p><p>The Clarity-2023 will also host the 2nd Clarity Prediction Challenge, that is addressing the problem of developing new intrusive and non-intrusive approaches to hearing-aid speech intelligibility prediction. The Challenge will be launching on 1st March, is you may be interested in participating please sign up to our Google group for further announcements.</p><p><strong>Keynote Talks</strong></p><ul><li>Prof Fei Chen, SUSTech, China,</li><li>Prof DeLiang Wang, Ohio State University, US</li></ul><p><strong>Organisers</strong></p><ul><li>Michael Akeroyd, University of Nottingham</li><li>Jon Barker,  University of Sheffield</li><li>Trevor Cox, University of Salford</li><li>Fei Chen, Southern University of Science and Technology, China</li><li>John Culling,  University of Cardiff</li><li>Simone Graetzer, University of Salford</li><li>Andrew Hines, University College Dublin</li></ul><p><strong>For further information</strong></p><p>To be kept up to date please join our <a href="https://groups.google.com/g/clarity-challenge" target="_blank" rel="noopener noreferrer">Clarity Challenge Google group</a>. If you have questions, please contact us directly using the <a href="https://claritychallenge.org/contact" target="_blank" rel="noopener noreferrer">contact details found here</a>.</p><p><strong>Funded by</strong> the Engineering and Physical Sciences Research Council (EPSRC), UK</p><p><strong>Supported by</strong> RNID (formerly Action on Hearing Loss), Hearing Industry Research Consortium, Amazon TTS Research</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/clarity">clarity</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/announcement">announcement</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/cpc-2">CPC2</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/Announcing CPC2">Announcing the 2nd Clarity Prediction Challenge (CPC2)</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-03-17T00:00:00.000Z" itemprop="datePublished">March 17, 2023</time> · <!-- -->2 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/jonbarker68" alt="Jon Barker"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jon Barker</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/trevorjcox" alt="Trevor Cox"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Trevor Cox</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>The 2nd Clarity Prediction Challenge - <a href="https://claritychallenge.org/docs/cpc2/taking_part/cpc2_registration" target="_blank" rel="noopener noreferrer">Register Now</a></p><p>To allow the development of better hearing aids, we need ways to evaluate the speech intelligibility of audio signals automatically. We need a prediction model that takes the audio produced by a hearing aid and the listener&#x27;s characteristics (e.g. audiogram) and estimates the speech intelligibility score that the listener would achieve in a listening test.</p><p>Last year we ran the <a href="https://claritychallenge.org/docs/cpc1/cpc1_intro" target="_blank" rel="noopener noreferrer">CPC1 Challenge</a> to develop such models. The challenge was presented at an online workshop and a special session of Interspeech 2022. We are now running the 2nd round of this challenge (CPC2), which builds on the first by using more complex signals and a larger set of listening test data for training and evaluating the prediction systems.</p><p>The outputs of the new challenge will be presented at an <a href="https://claritychallenge.org/clarity2023-workshop/" target="_blank" rel="noopener noreferrer">ISCA workshop</a> that is being run as a satellite event to Interspeech 2023 in Dublin on 19th August 2023.</p><p>Full details can be found on the Challenge Website.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="register-now-to-take-part">Register now to take part<a href="#register-now-to-take-part" class="hash-link" aria-label="Direct link to Register now to take part" title="Direct link to Register now to take part">​</a></h3><p>If you are interested in participating please register now via the <a href="https://claritychallenge.org/docs/cpc2/taking_part/cpc2_registration" target="_blank" rel="noopener noreferrer">online registration form</a>.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="important-dates">Important Dates<a href="#important-dates" class="hash-link" aria-label="Direct link to Important Dates" title="Direct link to Important Dates">​</a></h3><ul><li>March - Launch of challenge, release of training data + baseline system.</li><li>1st July - Release of evaluation data and opening of submission window.</li><li>31st July - Submission deadline.</li><li>19th August - ISCA Clarity 2023 workshop @ Interspeech</li><li>19th September - Deadline for submission of finalised Workshop papers</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-will-be-provided">What will be provided<a href="#what-will-be-provided" class="hash-link" aria-label="Direct link to What will be provided" title="Direct link to What will be provided">​</a></h3><ul><li>Audio produced by a variety of (simulated) hearing aids for speech-in-noise;</li><li>The corresponding clean reference signals (the original speech);</li><li>Characteristics of the listeners (pure tone audiograms, etc);</li><li>The measured speech intelligibility scores from listening tests, where hearing-impaired listeners were asked to say what they heard after listening to the hearing aid processed signals.</li><li>Software tools including a baseline system based on HASPI scores.</li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="for-further-information">For further information<a href="#for-further-information" class="hash-link" aria-label="Direct link to For further information" title="Direct link to For further information">​</a></h3><p>To be kept up to date please join our <a href="https://groups.google.com/g/clarity-challenge" target="_blank" rel="noopener noreferrer">Clarity Challenge Google group</a>. If you have questions, please contact us directly using the contact details found <a href="https://claritychallenge.org/contact" target="_blank" rel="noopener noreferrer">here</a>.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/clarity">clarity</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/announcement">announcement</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/cpc-2">CPC2</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/ICASSP 2023 evaluation data released">ICASSP 2023 evaluation data released</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-02-02T00:00:00.000Z" itemprop="datePublished">February 2, 2023</time> · <!-- -->One min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/jonbarker68" alt="Jon Barker"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jon Barker</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/trevorjcox" alt="Trevor Cox"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Trevor Cox</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>We are pleased to announce that the evaluation dataset for the ICASSP Clarity Challenge is now available for download.</p><p><a href="https://www.myairbridge.com/en/#!/folder/EkthOZZeBW33aaDBWSDadTgpOkbgaFxO" target="_blank" rel="noopener noreferrer">https://www.myairbridge.com/en/#!/folder/EkthOZZeBW33aaDBWSDadTgpOkbgaFxO</a></p><p>For instructions on preparing your submission please visit:</p><p><a href="https://claritychallenge.org/docs/icassp2023/taking_part/icassp2023_submission" target="_blank" rel="noopener noreferrer">https://claritychallenge.org/docs/icassp2023/taking_part/icassp2023_submission</a></p><p><strong>If you have not yet registered</strong> it is not too late to do so. Please use the form at the link below and we will then send you a Team ID and a personalised upload link for your submission.</p><p><a href="https://claritychallenge.org/docs/icassp2023/taking_part/icassp2023_registration" target="_blank" rel="noopener noreferrer">https://claritychallenge.org/docs/icassp2023/taking_part/icassp2023_registration</a></p><p>Note, we have extended the deadline for submission until Friday 10th February so that teams have a full week to process the signals.</p><p>The remaining schedule is as follows,</p><ul><li><strong>2nd Feb 2023</strong>: Release of evaluation data.</li><li><strong>10th Feb 2023</strong>: Teams submit processed signals and technical reports.</li><li><strong>14th Feb 2023</strong>: Results released. Top 5 ranked teams invited to submit papers to ICASSP-2023</li><li><strong>20th Feb 2023</strong>: Invited papers submitted to ICASSP-2023</li><li><strong>4-9th June 2023</strong>: Overview paper and invited papers presented at dedicated ICASSP session</li></ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/clarity">clarity</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/announcement">announcement</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/cec-2">CEC2</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/icassp-2023">ICASSP2023</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/Announcement of ICASSP 2023 Grand Challenge">Announcement of ICASSP 2023 Grand Challenge</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-11-15T00:00:00.000Z" itemprop="datePublished">November 15, 2022</time> · <!-- -->One min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.sheffield.ac.uk/dcs/people/research-staff/will-bailey" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Will Bailey</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>We are pleased to announce that registration for the <strong>ICASSP 2023 Clarity Grand Challenge</strong> is now open.</p><p>To register please complete the simple Google form found on the <a href="/docs/icassp2023/taking_part/icassp2023_registration">registration page</a>.</p><p>The remaining important dates for the challenge are as follows:</p><ul><li><strong>28th Nov 2022</strong>: Challenge launch: Release training/dev data; tools; baseline; rules &amp; documentation.</li><li><strong>2nd Feb 2023</strong>: Release of evaluation data.</li><li><strong>10th Feb 2023</strong>: Teams submit processed signals and technical reports.</li><li><strong>14th Feb 2023</strong>: Results released. Top 5 ranked teams invited to submit papers to ICASSP-2023</li><li><strong>20th Feb 2023</strong>: Invited papers submitted to ICASSP-2023</li><li><strong>4-9th</strong> June 2023: Overview paper and invited papers presented at dedicated ICASSP session</li></ul><p>The challenge training, dev data and initial tools are now fully from the <a href="https://github.com/claritychallenge/clarity" target="_blank" rel="noopener noreferrer">Github repository</a>.</p><p>If you have any questions please do not hesitate to contact us at <a href="mailto:claritychallengecontact@gmail.com" target="_blank" rel="noopener noreferrer">claritychallengecontact@gmail.com</a>.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/clarity">clarity</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/announcement">announcement</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/cec-2">CEC2</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/CPC1 results and prizes">CPC1 results and prizes</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-07-10T00:00:00.000Z" itemprop="datePublished">July 10, 2022</time> · <!-- -->One min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/jonbarker68" alt="Jon Barker"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jon Barker</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>The 1st Clarity Prediction Challenge is now complete. Thank you to all who took part!</p><p>The full results can be found on the <a href="https://claritychallenge.org/clarity2022-workshop/" target="_blank" rel="noopener noreferrer">Clarity-2022 workshop website</a> where you will also find links to system papers and the overview presentation.</p><p>Many of the systems have led to successful Interspeech 2022 papers and will be contributing to the Interspeech 2022 special session on <a href="https://claritychallenge.org/interspeech2022_siphil/" target="_blank" rel="noopener noreferrer">Speech Intelligibility Prediction for Hearing-Impaired Listeners</a>. We hope to see many of you in Korea!</p><p>In the meantime, please be sure to check out the onging <a href="https://claritychallenge.org/docs/cec2/cec2_intro" target="_blank" rel="noopener noreferrer">2nd Clarity Enhancement Challenge</a>. The deadline for submitting enhanced signals is 1st September 2022, so there is still time to participate. To register a team please use the form <a href="https://claritychallenge.org/docs/cec2/taking_part/cec2_registration" target="_blank" rel="noopener noreferrer">here</a>.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/clarity">clarity</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/registration">registration</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/cec-2">CEC2</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/CEC2 registration open">CEC2 registration open</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-05-03T00:00:00.000Z" itemprop="datePublished">May 3, 2022</time> · <!-- -->One min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/jonbarker68" alt="Jon Barker"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jon Barker</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>We are pleased to announce that registration for the <strong>2nd Clarity Enhancement Challenge (CEC2)</strong> is now open.</p><p>To register please complete the simple Google form found on the <a href="/docs/cec2/taking_part/cec2_registration">registration page</a>.</p><p>The remaining important dates for the challenge are as follows:</p><ul><li><strong>25th July 2022</strong>: Evaluation data released</li><li><strong>1st Sept 2022</strong>: 1st round <a href="/docs/cec2/taking_part/cec2_submission">submission</a> deadline for evaluation by objective measure</li><li><strong>15th Sept 2022</strong>: 2nd round <a href="/docs/cec2/taking_part/cec2_submission">submission</a> deadline for listening tests</li><li><strong>Sept-Nov 2022</strong>: Listening test evaluation period.</li><li><strong>2nd Dec 2022</strong>:  Results announced at a Clarity Challenge Workshop; prizes awarded.</li></ul><p>The challenge training, dev data and initial tools are now fully from the <a href="https://github.com/claritychallenge/clarity" target="_blank" rel="noopener noreferrer">Github repository</a>.</p><p>If you have any questions please do not hesitate to contact us at <a href="mailto:claritychallengecontact@gmail.com" target="_blank" rel="noopener noreferrer">claritychallengecontact@gmail.com</a>.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/clarity">clarity</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/registration">registration</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/cec-2">CEC2</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/release of CEC2 baseline">Release of CEC2 baseline</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-05-02T00:00:00.000Z" itemprop="datePublished">May 2, 2022</time> · <!-- -->One min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/jonbarker68" alt="Jon Barker"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jon Barker</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>We are pleased to announce the release of the <strong>2nd Clarity Enhancement Challenge (CEC2)</strong> baseline system code.</p><p>The baseline code has been released in the latest commit to the <a href="https://github.com/claritychallenge/clarity" target="_blank" rel="noopener noreferrer">Clarity GitHub repository</a>. </p><p>The baseline system perform NAL-R amplification according to the audiogram of the target listener, followed by a simple gain control and output of the signals to 16-bit stereo wav format. The system has been kept deliberately simple with no microphone array processing or attempt at noise cancellation.</p><p>HASPI scores for the dev set have been measured. The scores are as follows.</p><table><thead><tr><th>System</th><th>HASPI</th></tr></thead><tbody><tr><td>Unprocessed</td><td>0.1615</td></tr><tr><td>NAL-R baseline</td><td>0.2493</td></tr></tbody></table><p>See <a href="/docs/cec2/software/cec2_baseline">here</a> for further details.</p><p>If you have any problems using the baseline code please do not hesitate to contact us at <a href="mailto:claritychallengecontact@gmail.com" target="_blank" rel="noopener noreferrer">claritychallengecontact@gmail.com</a>, or post questions on the <a href="https://groups.google.com/g/clarity-challenge" target="_blank" rel="noopener noreferrer">Google group</a>.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/clarity">clarity</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/baseline">baseline</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/haspi">HASPI</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/cec-2">CEC2</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/launch of CEC2">Launch of CEC2</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-03-30T00:00:00.000Z" itemprop="datePublished">March 30, 2022</time> · <!-- -->One min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/jonbarker68" alt="Jon Barker"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jon Barker</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>We are pleased to announce the launch of the <strong>2nd Clarity Enhancement Challenge (CEC2)</strong>.</p><p>The website has been fully updated to provide you with all the information you will need to participate in the challenge.</p><p>The schedule for the challenge is as follows:</p><ul><li><strong>13th April 2022</strong>: <a href="/docs/cec2/cec2_download">Release</a> of training and development data; initial tools.</li><li><strong>30th April 2022</strong>: <a href="/docs/cec2/cec2_download">Release</a> of full toolset and baseline system.</li><li><strong>1st May 2022</strong>: <a href="/docs/cec2/taking_part/cec2_registration">Registration</a> for challenge entrants opens.</li><li><strong>25th July 2022</strong>: Evaluation data released</li><li><strong>1st Sept 2022</strong>: 1st round <a href="/docs/cec2/taking_part/cec2_submission">submission</a> deadline for evaluation by objective measure</li><li><strong>15th Sept 2022</strong>: 2nd round <a href="/docs/cec2/taking_part/cec2_submission">submission</a> deadline for listening tests</li><li><strong>Sept-Nov 2022</strong>: Listening test evaluation period.</li><li><strong>2nd Dec 2022</strong>:  Results announced at a Clarity Challenge Workshop; prizes awarded.</li></ul><p>The challenge training, dev data and initial tools will be available from 13th April. In the meantime, please visit the <a href="/docs/cec2/cec2_intro">CEC2 Intro page</a> to learn more about the task.</p><p>If you have any questions please do not hesitate to contact us at <a href="mailto:claritychallengecontact@gmail.com" target="_blank" rel="noopener noreferrer">claritychallengecontact@gmail.com</a>.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/clarity">clarity</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/launch">launch</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/cec-2">CEC2</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/Jan-2-live-events">Live events in January</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-01-07T00:00:00.000Z" itemprop="datePublished">January 7, 2022</time> · <!-- -->2 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://acoustictesting.salford.ac.uk/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/LaraHarris-Salford" alt="Lara Harris"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://acoustictesting.salford.ac.uk/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Lara Harris</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>The Clarity team are hosting two live sessions this month related to the Prediction Challenge. Everyone is welcome to attend, whether or not you have registered to participate in the challenge or are still considering signing up.</p><p><strong>The presentations will be very similar to the webinar in November.</strong> These events are intended as a chance for people in different time zones to attend live and ask the team questions. </p><p>Hosting is via Microsoft Teams. You can join from your browser without needing to install Teams, but if you join from a mobile device you may need to install the Teams app.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="webinar---challenge-overview">Webinar - Challenge Overview<a href="#webinar---challenge-overview" class="hash-link" aria-label="Direct link to Webinar - Challenge Overview" title="Direct link to Webinar - Challenge Overview">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="friday-14th-january">Friday 14th January<a href="#friday-14th-january" class="hash-link" aria-label="Direct link to Friday 14th January" title="Direct link to Friday 14th January">​</a></h3><p><strong>9:00 GMT | 17:00 CST (GMT+8)</strong></p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="click-here-to-join-the-webinar"><a href="https://teams.microsoft.com/l/meetup-join/19%3ameeting_ZjFhNjJhMzYtOWEyMC00NjNiLThjOTEtYTIwMTk2YTczZGRh%40thread.v2/0?context=%7b%22Tid%22%3a%2265b52940-f4b6-41bd-833d-3033ecbcf6e1%22%2c%22Oid%22%3a%223f282402-9a2d-4e68-b2d4-3c1f86585a45%22%7d" target="_blank" rel="noopener noreferrer">Click here to join the webinar</a><a href="#click-here-to-join-the-webinar" class="hash-link" aria-label="Direct link to click-here-to-join-the-webinar" title="Direct link to click-here-to-join-the-webinar">​</a></h3><p>An introduction to the aims of the challenge and some background to the problem of speech intelligibility prediction for hearing aids:</p><ul><li>Welcome, introduction to Clarity.</li><li>Speech intelligibility models: Overview and why are they needed.</li><li>Hearing impairment speech intelligibility prediction.</li><li>The prediction challenge - details and how you can sign up to participate.</li><li>Audience questions / discussion.</li></ul><p>The presentations will be recorded and made available online shortly after the event. The Q&amp;A discussion will not be recorded.</p><p>You are welcome to join slightly later if you are only interested in joining for the Q&amp;A section (presentations should finish around 9:40 GMT).</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="live-qa-session">Live Q&amp;A session<a href="#live-qa-session" class="hash-link" aria-label="Direct link to Live Q&amp;A session" title="Direct link to Live Q&amp;A session">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="monday-17th-january">Monday 17th January<a href="#monday-17th-january" class="hash-link" aria-label="Direct link to Monday 17th January" title="Direct link to Monday 17th January">​</a></h3><p><strong>17:00 GMT | 12:00 EST (GMT-5) | 9:00 PST (GMT-8)</strong></p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="click-here-to-join-the-qa"><a href="https://teams.microsoft.com/l/meetup-join/19%3ameeting_YTJhOGRmZGMtMjRiZS00MGY0LTliNjctZmZhMzhmNDI5M2I0%40thread.v2/0?context=%7b%22Tid%22%3a%2265b52940-f4b6-41bd-833d-3033ecbcf6e1%22%2c%22Oid%22%3a%223f282402-9a2d-4e68-b2d4-3c1f86585a45%22%7d" target="_blank" rel="noopener noreferrer">Click here to join the Q&amp;A</a><a href="#click-here-to-join-the-qa" class="hash-link" aria-label="Direct link to click-here-to-join-the-qa" title="Direct link to click-here-to-join-the-qa">​</a></h3><p>A chance to ask the team questions about the Clarity Prediction Challenge - for anyone that could not attend the webinar on Friday 14th due to time zone differences.</p><p><strong>Please note there will be no presentations in this session.</strong> The talks from Friday’s webinar will be uploaded to the <a href="https://www.youtube.com/channel/UCIc8FCHUA3Il9PUPt-sW1qw/videos" target="_blank" rel="noopener noreferrer">Clarity project YouTube channel</a> later in the day so you are invited to watch those before joining this live Q&amp;A.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/clarity">clarity</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/webinar">webinar</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/cpc-1">CPC1</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/webinar-1-link">Introduction Webinar - Recording Available</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2021-12-13T00:00:00.000Z" itemprop="datePublished">December 13, 2021</time> · <!-- -->One min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://acoustictesting.salford.ac.uk/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/LaraHarris-Salford" alt="Lara Harris"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://acoustictesting.salford.ac.uk/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Lara Harris</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>The Clarity team recently hosted a webinar to introduce the Prediction Challenge. The recording is now available to view online: </p><iframe width="560" height="315" src="https://www.youtube.com/embed/G_9KczaoZY4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"></iframe><h1>Slides</h1><p>The slides are available to download:</p><p><a href="https://drive.google.com/file/d/1pycRrMv5OF9R6948Cda0QsJ4jh_Bek22/view?usp=sharing" target="_blank" rel="noopener noreferrer">1 Welcome and Overview</a></p><p><a href="https://drive.google.com/file/d/1rScmDEUrtjBHG14VhWrKQu66-PiesU85/view?usp=sharing" target="_blank" rel="noopener noreferrer">2 Speech Intelligibility Models</a></p><p><a href="https://drive.google.com/file/d/1CQptm9sSIC8o2qHf_mtzQstLVhBnBzM_/view?usp=sharing" target="_blank" rel="noopener noreferrer">3 Hearing Impariment and SI Prediction</a></p><p><a href="https://drive.google.com/file/d/1BVeqMbygIWyiIo61HEMjwjyEALxHipzP/view?usp=sharing" target="_blank" rel="noopener noreferrer">4 Clarity Prediction Challenge Details</a></p><p>Note that we did not record the Q&amp;A session at the end, but if you have questions about taking part in the challenge you can contact us at <a href="mailto:claritychallengecontact@gmail.com" target="_blank" rel="noopener noreferrer">claritychallengecontact@gmail.com</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/clarity">clarity</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/webinar">webinar</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/cpc-1">CPC1</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/welcome to CPC1">Welcome to CPC1</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2021-07-08T00:00:00.000Z" itemprop="datePublished">July 8, 2021</time> · <!-- -->One min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/trevorjcox" alt="Trevor Cox"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Trevor Cox</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>Welcome to the new Clarity CPC1 site for the first prediction challenge launching in autumn 2021. Feel free to look around. At the moment we&#x27;re still doing listening tests and preparing the data, so the download links don&#x27;t work. If anything is unclear or you&#x27;ve got questions, please contact us through the Google group.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/clarity">clarity</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/hello">hello</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/cpc-1">CPC1</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/CEC1 submissions received">CEC1 submissions received</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2021-06-16T00:00:00.000Z" itemprop="datePublished">June 16, 2021</time> · <!-- -->One min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/jonbarker68" alt="Jon Barker"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jon Barker</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>The CEC1 submission deadline has now passed. Thank you to all the teams who sent us signals. </p><p>Please remember to submit your finalised system descriptions by June 22nd to the Clarity workshop following the <a href="https://claritychallenge.github.io/clarity2021-workshop/submissions.html" target="_blank" rel="noopener noreferrer">instructions provided on the workshop website</a>.</p><p>We are currently busy evaluating the submissions using the MBSTOI metric. We will be contacting teams on the 22nd with details of how to prepare signals for the listening panel evaluation.</p><p><strong>If you have been working on the challenge but missed the submission deadline</strong> then <a href="mailto:claritychallengecontact@gmail.com" target="_blank" rel="noopener noreferrer">please do get in contact</a>. We will still be happy to receive your signals and system descriptions. Although late entries will not be eligible for the official challenge ranking, we will be happy to compute the eval set MBSTOI score for you and may even be able to arrange listening test evaluation through our panel.</p><p>For any questions please contact us at <a href="mailto:claritychallengecontact@gmail.com" target="_blank" rel="noopener noreferrer">claritychallengecontact@gmail.com </a> or by posting to the <a href="https://groups.google.com/g/clarity-challenge?pli=1" target="_blank" rel="noopener noreferrer">Clarity challenge google group</a>.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/clarity">clarity</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/submission">submission</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/cec-1">CEC1</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/CEC1 eval data released">CEC1 eval data released</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2021-06-02T00:00:00.000Z" itemprop="datePublished">June 2, 2021</time> · <!-- -->2 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/jonbarker68" alt="Jon Barker"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jon Barker</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>The evaluation dataset is now available to download from the myairbridge <a href="https://mab.to/I9mkGx4wsiiaX" target="_blank" rel="noopener noreferrer">download site</a>. The evaluation data filename is <code>clarity_CEC1_data.scenes_eval.v1_1.tgz</code>.</p><p>Full details of how to prepare your submission are now available on this site. Please read them carefully. </p><p><strong>Registration</strong>: Teams must register via the Google form on the <a href="/docs/cec1/cec1_submission">How To Submit</a> page of this site. (Please complete this even if you have already completed a pre-registration form). Only one person from each team should register. Only those who have registered will be eligible to proceed to the evaluation. Once you have registered you will receive a confirmation email, a team ID and a link to a Google Drive to which you can upload your signals.</p><p><strong>Submission deadline</strong>: The deadline for submission is the <strong>15th June</strong>. </p><p>The submission consists of two components:</p><p>i) a technical document of up to 2 pages describing the system/model and any external data and pre-existing tools, software and models used. This should be prepared as a Clarity-2021 workshop abstract and <a href="https://claritychallenge.github.io/clarity2021-workshop/" target="_blank" rel="noopener noreferrer">submitted to the workshop</a>.</p><p>ii) the set of processed signals that we will evaluate using the MBSTOI metric. Details of how to name and package your signals for upload can be found on the <a href="/docs/cec1/cec1_submission">How To Submit</a> page.</p><p><strong>Listening Tests:</strong> Teams that do well in the MBSTOI evaluation will be notified on <strong>22nd June</strong> and invited to submit further signals for the second stage Listening Test evaluation.</p><p>For any questions please contact us at <a href="mailto:claritychallengecontact@gmail.com" target="_blank" rel="noopener noreferrer">claritychallengecontact@gmail.com </a> or by posting to the <a href="https://groups.google.com/g/clarity-challenge?pli=1" target="_blank" rel="noopener noreferrer">Clarity challenge google group</a>.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/clarity">clarity</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/evaluation">evaluation</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/cec-1">CEC1</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/baseline">Baseline speech intelligibility model in round one</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2021-04-22T00:00:00.000Z" itemprop="datePublished">April 22, 2021</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.salford.ac.uk/our-staff/simone-graetzer" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/sgraetzer" alt="Simone Graetzer"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.salford.ac.uk/our-staff/simone-graetzer" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Simone Graetzer</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="some-comments-on-signal-alignment-and-level-insensitivity">Some comments on signal alignment and level-insensitivity<a href="#some-comments-on-signal-alignment-and-level-insensitivity" class="hash-link" aria-label="Direct link to Some comments on signal alignment and level-insensitivity" title="Direct link to Some comments on signal alignment and level-insensitivity">​</a></h3><p>Our baseline binaural speech intelligibility measure in round one is the Modified Binaural Short-Time Objective Intelligibility measure, or MBSTOI. This short post outlines the importance of correcting for delays that your hearing aid processing algorithm introduces into the audio signals to allow MBSTOI to estimate the speech intelligibility accurately. It also discusses the importance of considering the audibility of signals before evaluation with MBSTOI.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="evaluation">Evaluation<a href="#evaluation" class="hash-link" aria-label="Direct link to Evaluation" title="Direct link to Evaluation">​</a></h2><p>In stage one, entries will be ranked according to the average MBSTOI score across all samples in the evaluation test set. In the second stage, entries will be evaluated by the listening panel. There will be prizes for both stages. See this <a href="https://claritychallenge.github.io/clarity_CEC1_doc/docs/cec1_rules" target="_blank" rel="noopener noreferrer">page</a> for more information.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/audibility">audibility</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/intelligibility">intelligibility</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/mbstoi">MBSTOI</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/baseline">baseline</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/cec-1">CEC1</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Baseline speech intelligibility model in round one" href="/blog/baseline"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/Latency, computation time and real-time operation">Latency, computation time and real-time operation</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2021-03-05T00:00:00.000Z" itemprop="datePublished">March 5, 2021</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/trevorjcox" alt="Trevor Cox"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Trevor Cox</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>An explanation of the time and computational limits for the first round of the enhancement challenge.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-1st-clarity-enhancement-challenge">The 1st Clarity Enhancement Challenge<a href="#the-1st-clarity-enhancement-challenge" class="hash-link" aria-label="Direct link to The 1st Clarity Enhancement Challenge" title="Direct link to The 1st Clarity Enhancement Challenge">​</a></h2><p>For a hearing aid to work well for users, the processing needs to be quick. The output of the hearing aid should be produced with a delay of less than about 10 ms. Many audio processing techniques are non-causal, i.e., the output of the system depends on samples from the future. Such processing is useless for hearing aids and therefore our rules include a restriction on the use of future samples.</p><p>The rules state the following:</p><ul><li>Systems must be causal; the output at time t must not use any information from input samples more than 5 ms into the future (i.e., no information from input samples &gt;t+5ms).</li><li>There is no limit on computational cost.</li></ul></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/challenge">challenge</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/computation">computation</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/enhancement">enhancement</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/latency">latency</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/real-time">real-time</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Latency, computation time and real-time operation" href="/blog/Latency, computation time and real-time operation"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/Clarity Challenge pre-announcement">Clarity Challenge pre-announcement</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2020-11-24T00:00:00.000Z" itemprop="datePublished">November 24, 2020</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/trevorjcox" alt="Trevor Cox"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Trevor Cox</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>Although age-related hearing loss affects 40% of 55 to 74 year-olds, the majority of adults who would benefit from hearing aids don’t use them. A key reason is simply that hearing aids don’t provide enough benefit.</p><p>Picking out speech from background noise is a critical problem even for the most sophisticated devices. The purpose of the Clarity Challenges is to catalyse new work to radically improve the speech intelligibility provided by hearing aids.</p><p>The series of challenges will consider increasingly complex listening scenarios. The first round, launching in January 2021, will focus on speech in indoor environments in the presence of a single interferer. It will begin with a challenge involving improving hearing aid processing. Future challenges on how to model speech-in-noise perception will be launched at a later date.</p><p><img loading="lazy" alt="Person using tablet" src="/assets/images/UoN_HS-08207-1536x1024-c3cb2429eb9f80e07fc15a97b1ede0c1.jpeg" width="1536" height="1024" class="img_ev3q"></p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/cec-1">CEC1</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Clarity Challenge pre-announcement" href="/blog/Clarity Challenge pre-announcement"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/One approach to our enhancement challenge">One approach to our enhancement challenge</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2020-07-29T00:00:00.000Z" itemprop="datePublished">July 29, 2020</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/trevorjcox" alt="Trevor Cox"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Trevor Cox</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>Improving hearing aid processing using DNNs blog. A suggested approach to overcome the non-differentiable loss function.</p><p>The aim of our Enhancement Challenge is to get people producing new algorithms for processing speech signals through hearing aids. We expect most entries to replace the classic hearing aid processing of Dynamic Range Compressors (DRCs) with <a href="https://en.wikipedia.org/wiki/Deep_learning" target="_blank" rel="noopener noreferrer">deep neural networks (DNN)</a> (although all approaches are welcome!). The first round of the challenge is going to be all about improving speech intelligibility.</p><p>Setting up a DNN structure and training regime for the task is not as straightforward as it might first appear. Figure 1 shows an example of a naive training regime. An audio example of Speech in Noise (SPIN) is randomly created (<em>audio sample generation</em>, bottom left), and a listener is randomly selected with particular hearing loss characteristics (<em>random artificial listener generation</em>, top left). The DNN Enhancement model (represented by the bright yellow box) then produces improved speech in noise. (Audio signals in pink are two-channel, left and right because this is for binaural hearing aids.)</p><p><img loading="lazy" alt="schematic" src="/assets/images/clarity_schematic_for_blog-09-6f015d990f75f14a4068ca7ab8295f69.png" width="1149" height="446" class="img_ev3q"></p><p>Figure 1</p><p>Next the improved speech in noise is passed to the Prediction Model in the lime green box, and this gives an estimation of the Speech Intelligibility (SI). Our baseline system will include algorithms for this. We’ve already blogged about the Hearing Loss Simulation. Our current thinking is that the intelligibility model will be using a binaural form of the Short-Time Objective Intelligibility Index (STOI) <!-- -->[1]<!-- -->. The dashed line going back to the enhancement model shows that the DNN will be updated based on the reciprocal of the Speech Intelligibility (SI) score. By minimising (1/SI), the enhancement model will be maximising intelligibility.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/dnn">DNN</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/enhancement">enhancement</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/evaluation">evaluation</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/gan">GAN</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/hearing-aid">hearing aid</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/knowledge-distillation">knowledge distillation</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about One approach to our enhancement challenge" href="/blog/One approach to our enhancement challenge"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/The speech-in-noise problem part two">The speech-in-noise problem part two</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2020-07-06T00:00:00.000Z" itemprop="datePublished">July 6, 2020</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.salford.ac.uk/our-staff/simone-graetzer" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/sgraetzer" alt="Simone Graetzer"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.salford.ac.uk/our-staff/simone-graetzer" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Simone Graetzer</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/trevorjcox" alt="Trevor Cox"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Trevor Cox</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>How hearing aids address the problem of speech-in-noise in noisy and quieter places. We’ll also discuss what machine learning techniques are often used for noise reduction, and some promising strategies for hearing aids.</p><p><img loading="lazy" alt="Tablet user" src="/assets/images/UoN_HS-08207-1536x1024-c3cb2429eb9f80e07fc15a97b1ede0c1.jpeg" width="1536" height="1024" class="img_ev3q"></p><p>In a previous blog, we set out the problem of using hearing aids to pick out speech in noisy places. When the <a href="https://en.wikipedia.org/wiki/Signal-to-noise_ratio" target="_blank" rel="noopener noreferrer">signal-to-noise ratio (SNR)</a> is low, hearing aids can only do so much to improve the intelligibility of the speech.</p><p>A solitary hearing aid has various ways of addressing everyday constant noises such as cars, vacuum cleaners and fans. The aids work best when the noise is not too intrusive and SNR is relatively high. Problems arise when the noise is high (low SNRs), because then the hearing aid processing can distort the sound too much. While the hearing aid might have limited success in improving intelligibility in certain cases, they can still make the noise less annoying (e.g., Brons et al., 2014).</p><p>Using multiple microphones on each hearing aid can help in noisy conditions. The sound from the microphones is combined in a way that boosts the speech relative to the noise. This technology can be put into larger hearing aids, when there is enough spacing between the front and rear microphones.</p><p>One of the reasons why our brains are really good at picking out speech from the hubbub of a restaurant, is that it compares and contrasts the sounds from both ears. Our hearing is <a href="https://en.wikipedia.org/wiki/Binaural" target="_blank" rel="noopener noreferrer">binaural</a>. Similarly, if you have a hearing aids in both ears, they work better if they collaborate on reducing the noise.</p><p>Crucial to how our brains locate sound and pick out speech in noise are timing and level cues that come from comparing the sound at both ears. When sound comes from the side:</p><ul><li>interaural time differences occur because the sound arrives at one ear earlier than the other.</li><li>interaural level differences occur because the sound has to bend around the head to reach the furthest ear.</li></ul><p>Binaural hearing aids communicate wirelessly and use noise reduction strategies that preserve these interaural time and level difference cues (e.g., Van den Bogaert et al., 2009). This allows the listener’s brain to better locate the speech and boost this compared to the noise.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/machine-learning">machine learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/speech-in-noise">speech-in-noise</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about The speech-in-noise problem part two" href="/blog/The speech-in-noise problem part two"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/Hearing loss simulation">Hearing loss simulation</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2020-06-23T00:00:00.000Z" itemprop="datePublished">June 23, 2020</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/trevorjcox" alt="Trevor Cox"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Trevor Cox</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.salford.ac.uk/our-staff/simone-graetzer" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/sgraetzer" alt="Simone Graetzer"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.salford.ac.uk/our-staff/simone-graetzer" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Simone Graetzer</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>What our hearing loss algorithms simulate, with audio examples to illustrate hearing loss.</p><p>Our challenge entrants are going to use machine learning to develop better processing of speech in noise (SPIN) for hearing aids. For a machine learning algorithm to learn new ways of processing audio for the hearing impaired, it needs to estimate how the sound will be degraded by any hearing loss. Hence, we need an algorithm to simulate hearing loss for each of our listeners. The diagram belows shows our draft baseline system that was detailed in a previous blog. The hearing loss simulation is part of the prediction model. The Enhancement Model to the left is effectively the hearing aid and the Prediction Model to the right is estimating how someone will perceive the intelligibility of the speech in noise.</p><p><img loading="lazy" alt="baseline" src="/assets/images/baseline-1536x684-b7b4b9fdbaffe5d680f9dc1e0b073d7f.png" width="1536" height="684" class="img_ev3q"></p><p>The draft baseline system (where SPIN is speech in noise, DRC is Dynamic Range Compression, HL is Hearing Loss, SI is Speech Intelligibility and L &amp; R are Left and Right).</p><p>There are different causes of hearing loss, but we’re concentrating on the most common type that happens when you age (<a href="https://www.nhs.uk/conditions/hearing-loss/" target="_blank" rel="noopener noreferrer">presbycusis</a>). <a href="https://rnid.org.uk/" target="_blank" rel="noopener noreferrer">RNID</a> (formerly Action on Hearing Loss) estimate that more than 40% of people over the age of 50 have a hearing loss, and this rises to 70% of people who are older than 70.</p><p>The aspects of hearing loss we’ve decided to simulate are</p><ol><li>The loss of ability to sense the quietest sounds (increase in absolute threshold).</li><li>How as an audible sound increases in level, the perceived increase in loudness is greater than normal (loudness recruitment) (Moore et al. 1996).</li><li>How the ear has a poorer ability to discriminate the frequency of sounds (impaired frequency selectivity).</li></ol></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/baseline">baseline</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/hearing-loss-simulation">hearing loss simulation</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Hearing loss simulation" href="/blog/Hearing loss simulation"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/Sounds">Sounds for round one</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2020-06-18T00:00:00.000Z" itemprop="datePublished">June 18, 2020</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/trevorjcox" alt="Trevor Cox"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Trevor Cox</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>We’ll be challenging our contestants to find innovative ways of making speech more audible for hearing impaired listeners when there is noise getting in the way. But what noises should we consider? To aid us in choosing sounds and situations that are relevant to people with hearing aids, we held a focus group.</p><p>We wanted to know about</p><ul><li>Everyday background noises that make having a conversation difficult.</li><li>The characteristics of speech after it has been processed by a hearing-aid that hearing aid listeners would value.</li></ul><p>A total of eight patients (four males, four females) attended the meeting, six of whom were recruited from the <a href="https://nottinghambrc.nihr.ac.uk/" target="_blank" rel="noopener noreferrer">Nottingham Biomedical Research Centre’s</a> patient and public involvement contact list. Two attendees were recruited from a local lip reading class organised by the <a href="https://www.nottsdeaf.org.uk/" target="_blank" rel="noopener noreferrer">Nottinghamshire Deaf Society</a>. The range of hearing loss within the group is from mild to severe. They all regularly use bilateral hearing aids.</p><p>Our focus was on the living room because that is the scenario for round one of the challenges.</p><p><img loading="lazy" alt="People Listening" src="/assets/images/photo-of-people-sitting-on-sofa-3890171-d5866af4bceb29983f924efe7600d1ec.jpeg" width="1280" height="853" class="img_ev3q"></p><p>Photo by Gustavo Fring from Pexels</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/cec-1">CEC1</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/focus-group">focus group</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/noise">noise</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/sounds">sounds</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Sounds for round one" href="/blog/Sounds"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="image" content="https://i.imgur.com/mErPwqL.png"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/The speech-in-noise problem">The speech-in-noise problem</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2020-06-18T00:00:00.000Z" itemprop="datePublished">June 18, 2020</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://www.salford.ac.uk/our-staff/simone-graetzer" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/sgraetzer" alt="Simone Graetzer"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://www.salford.ac.uk/our-staff/simone-graetzer" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Simone Graetzer</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/trevorjcox" alt="Trevor Cox"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Trevor Cox</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>People often have problems understanding speech in noise, and this is one of the main deficits of hearing aids that our machine learning challenges will address.</p><p><img loading="lazy" alt="cocktail party" src="/assets/images/cocktail_party-bffe47988f231d9d0811d899c5e6022e.jpeg" width="800" height="600" class="img_ev3q"></p><p>It’s common for us to hear sounds coming simultaneously from different sources. Our brains then need to separate out what we want to hear (the target speaker) from the other sounds. This is especially difficult when the competing sounds are speech. This has the quaint name, The Cocktail Party Problem (Cherry, 1953). We don’t go to many cocktail parties, but we encounter lots of times where the The Cocktail Party Problem is important. Hearing a conversation in a busy restaurant, trying to understand a loved one while the television is on or hearing the radio in the kitchen when the kettle is boiling, are just a few examples.</p><p>Difficulty in picking out speech in noise is really common if you have a hearing loss. Indeed, it’s often when people have problems doing this that they realise they have a hearing loss.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">“Hearing aids don’t work when there is a lot of background noise. This is when you need them to work.”</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">-- Statement from a hearing aid wearer (Kochkin, 2000)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Hearing aids are the the most common form of treatment for hearing loss. However, surveys indicate that at least 40% of hearing aids are never or rarely used (Knudsen et al., 2010). A major reason for this is dissatisfaction with performance. Even the best hearing aids perform poorly for speech in noise. This is particularly the case when there are many people talking at the same time, and when the amount of noise is relatively high (i.e., the signal-to-noise ratio (SNR) is low). As hearing ability worsen with age, the ability to understand speech in background noise also reduces (e.g., Akeroyd, 2008).</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/cocktail-party">cocktail party</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/hearing">hearing</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/hearing-aid">hearing aid</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/noise">noise</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/speech">speech</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about The speech-in-noise problem" href="/blog/The speech-in-noise problem"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/Why use machine learning challenges for hearing aids">Why use machine learning challenges for hearing aids?</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2020-05-06T00:00:00.000Z" itemprop="datePublished">May 6, 2020</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/trevorjcox" alt="Trevor Cox"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Trevor Cox</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>The Clarity Project is based around the idea that machine learning challenges could improve hearing aid signal processing. After all this has happened in other areas, such as automatic speech recognition (ASR) in the presence of noise. The improvements in ASR have happened because of:</p><ul><li>Machine learning (ML) at scale – big data and raw <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit" target="_blank" rel="noopener noreferrer">GPU</a> power.</li><li>Benchmarking – research has developed around community-organised evaluations or challenges.</li><li>Collaboration has been enabled by these challenges, allowing working across communities such as signal processing, acoustic modelling, language modelling and machine learning</li></ul><p>We’re hoping that these three mechanisms can drive improvements in hearing aids.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="components-of-a-challenge">Components of a challenge<a href="#components-of-a-challenge" class="hash-link" aria-label="Direct link to Components of a challenge" title="Direct link to Components of a challenge">​</a></h2><p>There needs to be a common task based on a target application scenario to allow communities to gain from benchmarking and collaboration. Clarity project’s first enhancement challenge will be about hearing speech from a single talker in a typical living room, where there is one source of noise and a little reverberation.</p><p>We’re currently working on developing simulation tools to allow us to generate our living room data. The room acoustic will be simulated using <a href="https://www.semanticscholar.org/paper/RAVEN%3A-A-real-time-framework-for-the-auralization-Schr%C3%B6der-Vorl%C3%A4nder/6977f2c2c1fb4cac2305e7965ee0da8192ced72d?p2df" target="_blank" rel="noopener noreferrer">RAVEN</a> and <a href="https://uol.de/mediphysik/downloads/hearingdevicehrtfs" target="_blank" rel="noopener noreferrer">the Hearing Device Head-related Transfer Functions will come from Denk’s work</a>. We’re working on getting better, more ecologically valid speech than is often used in speech intelligibility work.</p><p><img loading="lazy" alt="baseline" src="/assets/images/baseline-1536x684-b7b4b9fdbaffe5d680f9dc1e0b073d7f.png" width="1536" height="684" class="img_ev3q"></p><p>Entrants are then given training data and development (dev) test data along with a baseline system that represents the current state-of-the-art. You can find a post and video on the current thinking on the baseline here. We’re still working on the rules stipulating what is and what is not allowed (for example, will entrants be allowed to use data from outside the challenge).</p><p>Clarity’s first enhancement challenge is focussed on maximising the speech intelligibility (SI) score. We will evaluate this first through a prediciton model that is based on a hearing loss simulation and an objective metric for speech intellibility. Simulation has been hugely important for generating training data in the <a href="https://chimechallenge.github.io/chime6/" target="_blank" rel="noopener noreferrer">CHIME challenges</a> and so we intend to use that approach in Clarity. But results from simulated test sets cannot be trusted and hence a second evaluation will come through perceptual tests on hearing impaired subjects. However, one of our current problems is that we can’t bring listeners into our labs because of COVID-19.</p><p>We’ll actually be running two challenges in roughly parallel, because we’re also going to task the community to improve our prediction model for speech intelligibility.</p><p>We’re running a series of challenges over five years. What other scenarios should we consider? What speech? What noise? What environment? Please comment below.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="acknowledgements">Acknowledgements<a href="#acknowledgements" class="hash-link" aria-label="Direct link to Acknowledgements" title="Direct link to Acknowledgements">​</a></h2><p>Much of this text is based on <a href="http://2020.speech-in-noise.eu/?p=3" target="_blank" rel="noopener noreferrer">Jon Barker’s 2020 SPIN keynote</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/challenges">challenges</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/c-hi-me">CHiME</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/enhancement">enhancement</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/machine-learning">machine learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/prediction">prediction</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/The baseline">The baseline</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2020-04-29T00:00:00.000Z" itemprop="datePublished">April 29, 2020</time> · <!-- -->One min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/trevorjcox" alt="Trevor Cox"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://trevorcox.me/trevor-cox" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Trevor Cox</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>An overview of the current state of the baseline we’re developing for the machine learning challenges</p><p><img loading="lazy" alt="The baseline" src="/assets/images/baseline-1536x684-b7b4b9fdbaffe5d680f9dc1e0b073d7f.png" width="1536" height="684" class="img_ev3q"></p><p>We’re currently developing the baseline processing that challenge entrants will need. This takes a random listener and a random audio sample of speech in noise (SPIN) and passes that through a simulated hearing aid (the Enhancement Model). This improves the speech in noise. We then have an algorithm (the Prediction Model) to estimate the Speech Intelligibility that the listener would perceive (SI score). This score can then be used to drive machine learning to improve the hearing aid.</p><iframe width="560" height="315" src="https://www.youtube.com/embed/I1v8_TmXkeA" title="YouTube video player" frameborder="0" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture"></iframe><p><em>A talk through the baseline model we’re developing.</em></p><p>The first machine learning challenge is to improve the enhancement model, in other words, to produce a better processing algorithm for the hearing aid. The second challenge is to improve the prediction model using perceptual data we’ll provide.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/baseline">baseline</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/enhancement">enhancement</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/processing">processing</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/welcome">Welcome</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2020-04-28T00:00:00.000Z" itemprop="datePublished">April 28, 2020</time> · <!-- -->One min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/jonbarker68" alt="Jon Barker"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="http://staffwww.dcs.shef.ac.uk/people/J.Barker/" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jon Barker</span></a></div><small class="avatar__subtitle" itemprop="description">Clarity Team Member</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>Welcome to the new Clarity blog. We will be using this blog to post regular updates about our Challenges and Workshop, as well as posts discussing the tools and techniques that we are using in our baseline systems.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/clarity">clarity</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/hello">hello</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/cec-1">CEC1</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/cpc2/cpc2_intro">CPC2 Documentation</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/icassp2023/icassp2023_intro">ICASSP 2023 Documentation</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/cec2/cec2_intro">CEC2 Documentation</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/cpc1/cpc1_intro">CPC1 Documentation</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/cec1/cec1_intro">CEC1 Documentation</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="http://claritychallenge.org" target="_blank" rel="noopener noreferrer" class="footer__link-item">The Clarity Project<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://groups.google.com/g/clarity-challenge" target="_blank" rel="noopener noreferrer" class="footer__link-item">Clarity Google Group<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="mailto:claritychallengecontact@gmail.com" target="_blank" rel="noopener noreferrer" class="footer__link-item">Email Us<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Challenge Updates</a></li><li class="footer__item"><a href="https://github.com/claritychallenge/clarity" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 The Clarity Team. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.4d80ea3e.js"></script>
<script src="/assets/js/main.97869bb1.js"></script>
</body>
</html>